[
  {
    "id": "1.1",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Text File Inspector & Reporter",
    "description": "Write a Python script using os and chardet that: 1. Reads all .txt files from a specified directory. 2. Automatically detects each file's encoding using chardet, defaulting to UTF-8 if detection fails or is uncertain. 3. Prints the first five lines of each file to the console. 4. Skips empty files and logs any unreadable files to an errors.log file. 5. Generates a summary report in JSON format file_inspection_report.json containing for each file: filename, encoding_used, file_size_bytes, and lines_read. 6. Provides a final console summary showing the total number of files processed and skipped.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.2",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Multi-Keyword File Searcher",
    "description": "Write a Python develop a Python tool using argparse and csv that: 1. Accepts multiple keywords from the user via command- line arguments. 2. Searches through all text files in a directory and its subdirectories for these keywords. 3. For each match, records the filename, line_number, and a context snippet the matching line Â±2 lines in search_results.csv. 4. Skips files larger than50 MB to maintain performance. 5. Handles files that cannot be opened permission errors, corruption by logging them to a separate file. 6. Generates a final summary showing the total number of matches found per keyword.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.3",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "CSV Merger & Deduplicator",
    "description": "Write a Python create a Python script using pandas that: 1. Reads all CSV files from a specified directory. 2. Merges them into a single combined.csv file, aligning columns by name and filling missing values with NULL. 3. Removes duplicate rows based on all columns. 4. Logs the number of rows discarded from each source file to a merge_log.json. 5. Handles encoding issues and inconsistent delimiters gracefully. 6. Generates a statistical summary showing: total_rows_combined, unique_rows_final, total_columns_final.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "file processing"
    ]
  },
  {
    "id": "1.4",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Directory Monitor & Processor",
    "description": "Write a Python build a Python program using watchdog that: 1. Monitors a specified directory for new files .csv, .txt. 2. For new .csv files, appends their content to a master .csv file. 3. For new .txt files, scan them for a set of predefined keywords and log matches to keyword_matches.log. 4. Runs continuously, checking for changes every30 seconds. 5. Maintains a JSON log processing_log.json with metadata, timestamp, filename, and action taken for each processed file. 6. Provides a summary upon keyboard interrupt showing: files_processed, broken down by type.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "1.5",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Directory Backup Monitor",
    "description": "Write a Python build a Python tool using watchdog and shutil that: 1. Monitors a directory for new or modified files. 2. Creates a timestamped backup of each changed file in a backups/ directory. 3. Maintains a JSON index backup_index.json mapping original files to their backup paths. 4. Enforces a configurable maximum number of backups per file, deleting the oldest when exceeded. 5. Generates a summary showing: files_backed_up, total_backup_size_gb, backup_date_range.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "1.6",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Bulk File Renamer",
    "description": "Write a Python develop a Python script using os and argparse that: 1. Renames files in a directory based on a user-defined pattern, prefix_<timestamp>_<sequence>.<ext>. 2. Ensures no filename conflicts occur. 3. Provides a dry-run option to preview changes without renaming. 4. Generates a summary report listing: files_renamed, files_skipped, rename_conflicts_avoided.",
    "tags": [
      "python",
      "file processing"
    ]
  },
  {
    "id": "1.7",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Secure ZIP Archiver",
    "description": "Write a Python create a Python utility using zipfile and json that: 1. Compresses a set of files/directories into a password- protected ZIP archive. 2. Allows splitting large archives into parts. 3. Maintains a JSON log compression_log.json of original and compressed sizes. 4. Generates a summary showing: files_compressed, archive_size, compression_ratio_percentage.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.8",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Duplicate File Finder & Remover",
    "description": "Write a Python tool using hashlib and os that: 1. Identifies duplicate files in a directory by comparing SHA-256 hashes. 2. Generates a report duplicates_report.json listing all duplicate groups. 3. Provides options to automatically delete duplicates or move them to a duplicates/ folder. 4. Logs all actions taken. 5. Generates a summary showing: duplicates_found, disk_space_saved, unique_files_remaining.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.9",
    "category": "File & Text Processing",
    "subcategory": "File Operations",
    "title": "Directory Snapshot & Comparator",
    "description": "Write a Python build a Python program using json and os that: 1. Creates a timestamped JSON snapshot of a directory's structure and file metadata. 2. Allows comparison of two snapshots to identify added, deleted, and modified files. 3. Generates a comparison report snapshot_comparison.json detailing the changes. 4. Provides a summary showing: files_added, files_deleted, files_modified. PDF Tools",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "1.10",
    "category": "File & Text Processing",
    "subcategory": "PDF Tools",
    "title": "PDF Keyword Search & Reporter",
    "description": "Write a Python build a Python utility using pypdf2 or pdfplumber that: 1. Searches for a user-provided keyword across all pages of all PDFs in a directory. 2. For each match, records the filename, page_number, and a text snippet surrounding the keyword. 3. Saves all results to pdf_matches.json. 4. Handles encrypted PDFs by skipping them and logging a warning. 5. Generates a summary showing: files_scanned, pages_processed, total_matches_found.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.11",
    "category": "File & Text Processing",
    "subcategory": "PDF Tools",
    "title": "Batch PDF Encryption Tool",
    "description": "Write a Python design a Python tool using pikepdf that: 1. Encrypts PDFs with a user-provided password. 2. Validates that the input PDF is not already encrypted. 3. Supports batch processing of multiple files. 4. Generates a log entry per file in encryption_log.json with filename, status, encryption_algorithm. 5. Provides a final summary report: encrypted_successfully, skipped already encrypted, failed.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.12",
    "category": "File & Text Processing",
    "subcategory": "PDF Tools",
    "title": "Automated Report Generation",
    "description": "Write a Python develop a Python script using reportlab that: 1. Generates a multi-page PDF report with a title page, table of contents, sample chapters, and page numbers. 2. Allows customization of font, font_size, and chapter_style via a configuration dictionary. 3. Saves the report to report.pdf. 4. Logs the details of each generated section to report_generation.log. 5. Generates a summary showing: total_pages, chapters_created.",
    "tags": [
      "python",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "1.13",
    "category": "File & Text Processing",
    "subcategory": "PDF Tools",
    "title": "Table Extractor to CSV",
    "description": "Write a Python design a Python automation tool using camelot or tabula-py that: 1. Extracts tables from every page of PDF documents in a directory. 2. Exports each detected table to a separate CSV file in an output folder. 3. Handles pages with no tables by logging a warning. 4. Generates a summary report table_extraction_report.json listing: tables_extracted_per_pdf, total_rows_extracted.",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "1.14",
    "category": "File & Text Processing",
    "subcategory": "PDF Tools",
    "title": "OCR Text Extractor from Scanned PDFs",
    "description": "Write a Python develop a Python script using pytesseract and pdf2image that: 1. Performs OCR on scanned PDFs to extract text. 2. Saves the extracted text to a .txt file for each PDF. 3. Logs pages with low OCR confidence <80% for manual review. 4. Generates a summary showing: pages_scanned, words_extracted, low_confidence_pages. Watermarking & Image Tools",
    "tags": [
      "python",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.15",
    "category": "File & Text Processing",
    "subcategory": "Watermarking & Image Tools",
    "title": "Text Watermark Applicator",
    "description": "Write a Python develop a Python utility using pypdf2 and reportlab that: 1. Adds a diagonal text watermark, \"CONFIDENTIAL - \" to every page of a PDF. 2. Provides options to control watermark opacity, font_size, and color via command-line arguments. 3. Processes multiple PDFs in batch, saving watermarked versions with _watermarked suffix. 4. Generates a summary report listing: files_processed, processing_time, failures, encrypted PDFs.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "1.16",
    "category": "File & Text Processing",
    "subcategory": "Watermarking & Image Tools",
    "title": "Image Watermark Applicator",
    "description": "Write a Python create a Python script using pypdf2 and PIL that: 1. Applies an image watermark PNG to the center of every page of a PDF. 2. Allows the user to adjust the opacity and scaling of the watermark image. 3. Processes multiple PDFs, saving output to a specified folder with _watermarked filenames. 4. Handles missing watermark images and incompatible PDFs gracefully. 5. Generates a summary showing: pdfs_processed, total_output_size_mb. Text Analysis & Automation",
    "tags": [
      "python",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "1.17",
    "category": "File & Text Processing",
    "subcategory": "Text Analysis & Automation",
    "title": "Console Pattern Generator",
    "description": "Write a Python script using argparse that: 1. Generates text patterns, horizontal/vertical lines, squares, and pyramids from a user-provided character. 2. Validates that the input is a single character. 3. Allows the user to specify the pattern size width/height. 4. Provides an output option to save the pattern to a text file. 5. Displays the pattern in the console. 6. Generates a summary log showing: pattern_type, size, output_file if used.",
    "tags": [
      "python",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.18",
    "category": "File & Text Processing",
    "subcategory": "Text Analysis & Automation",
    "title": "Config-Driven Pattern Generator",
    "description": "Write a Python develop a Python program using json and argparse that: 1. Loads pattern generation configuration character, style, size, uppercase, and reverse from a JSON file. 2. Allows overriding any config option via command-line arguments. 3. Generates the specified pattern and exports it to a text file.7 4. Logs the final configuration used to a separate JSON file, config_used.json, for reproducibility. 5. Generates a summary report showing which configuration options were applied.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.19",
    "category": "File & Text Processing",
    "subcategory": "Text Analysis & Automation",
    "title": "Data: Multi-Format Financial Report Generator",
    "description": "Write a Python create a Python script using Jinja2 and pandas that: 1. Formats numerical financial data, revenue, profit, and margins into multiple output formats: JSON, CSV, and plain text. 2. Uses a template system for consistent structuring of fields and currency symbols. 3. Allows users to select the output format via a command- line option format. 4. Calculates and includes summary statistics: total revenue, total profit, average margin. 5. Saves all output files with a unique timestamp in the filename. 6. Prints a summary of the generated reports and their locations.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "1.20",
    "category": "File & Text Processing",
    "subcategory": "Text Analysis & Automation",
    "title": "Log File Filter & Consolidator",
    "description": "Write a Python program using datetime and re that: 1. Reads multiple log files and filters entries based on a user- provided date and severity level: INFO, WARNING, and ERROR. 2. Writes the filtered entries to a consolidated filtered.log file. 3. Handles various date formats and missing fields. 4. Logs processing metrics to log_processing_summary.json: lines_processed, lines_retained, lines_skipped per file. 5. Prints a final summary of totals across all files.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "1.21",
    "category": "File & Text Processing",
    "subcategory": "Text Analysis & Automation",
    "title": "Word Frequency Analyzer",
    "description": "Write a Python script using nltk and collections that: 1. Analyzes a directory of text files, calculating word frequency. 2. Normalizes words lowercase, removes stopwords and counts occurrences. 3. Saves the top50 words per file to per_file_frequency.json. 4. Generates a consolidated chart, top_words.png, of the most common words across all files. 5. Handles large files efficiently. 6. Generates a final summary showing: total_words_processed, unique_words, top_5_common_words.",
    "tags": [
      "python",
      "json",
      "file processing"
    ]
  },
  {
    "id": "2.1",
    "category": "Command-Line Utilities",
    "subcategory": "Math & Conversions",
    "title": "Arithmetic Calculator with History",
    "description": "Write a Python CLI tool using argparse that: 1. Accepts two numbers and an operation add, subtract, multiply, or divide as positional arguments. 2. Validates numeric inputs and handles division by zero with a clear error message. 3. Includes an optional format argument to output the result as JSON or plain text. 4. Supports a save flag to append the operation with a timestamp to a calc_history.json file. 5. Prints the result clearly and provides a final summary showing the total number of operations performed in the session.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.2",
    "category": "Command-Line Utilities",
    "subcategory": "Math & Conversions",
    "title": "Advanced Scientific Calculator",
    "description": "Write a Python create a Python CLI calculator that: 1. Supports basic arithmetic and advanced functions sqrt, pow, and factorial. 2. Logs all calculations with timestamps to a calc_history.json file when history is used. 3. Includes a clear-history flag to reset the log file. 4. Handles invalid inputs and operations gracefully. 5. Displays the result and a summary of the total operations stored in history.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "2.3",
    "category": "Command-Line Utilities",
    "subcategory": "Math & Conversions",
    "title": "Multi-Base Number Converter",
    "description": "Write a Python build a Python CLI utility that: 1. Converts a number between binary, octal, decimal, and hexadecimal bases. 2. Validates input digits for the given source base. 3. Includes an all flag to display conversions to all supported bases at once. 4. Supports an output option to save results to a JSON file. 5. Provides a summary showing the original input, target base, and result.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.4",
    "category": "Command-Line Utilities",
    "subcategory": "Math & Conversions",
    "title": "Temperature Converter & Table Generator",
    "description": "Write a Python develop a Python CLI program that: 1. Converts temperatures between Celsius, Fahrenheit, and Kelvin. 2. Provides a table option to print a conversion table across all scales. 3. Handles invalid inputs and scales. 4. Supports saving results to a CSV file. 5. Displays a summary of the original value, converted value, and session total.",
    "tags": [
      "python",
      "csv",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.5",
    "category": "Command-Line Utilities",
    "subcategory": "Math & Conversions",
    "title": "Live Currency Converter",
    "description": "Write a Python develop a Python CLI tool that: 1. Performs currency conversion using real-time exchange rates from an API. 2. Accepts amount, source currency, and target currency as arguments. 3. Handles API failures and unsupported currencies. 4. Saves conversion history to a CSV file with timestamps. 5. Provides a summary showing original amount, result, exchange rate, and total conversions. Generators",
    "tags": [
      "python",
      "api",
      "csv",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "2.6",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Text Transformation Pipeline",
    "description": "Write a Python create a Python CLI utility that: 1. Accepts a string input and flags for transformations: uppercase, lowercase, reverse, title. 2. Applies selected transformations in sequence, printing each result on a new line. 3. Includes an output option to save the final transformed string to a .txt file. 4. Logs the applied operations and provides a summary showing the number of transformations performed.",
    "tags": [
      "python",
      "cli",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "2.7",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Configurable ASCII Art Generator",
    "description": "Write a Python build a Python CLI program that: 1. Accepts arguments for shape pyramid, square, diamond, character, and size1-50. 2. Validates the size is within the safe range; errors on invalid inputs. 3. Includes a save option to write the pattern to a file and a color argument for ANSI-colored terminal output. 4. Prints a confirmation message with the output destination.",
    "tags": [
      "python",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.8",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Multi-Format Multiplication Table Generator",
    "description": "Write a Python CLI tool that: 1. Generates multiplication tables for a given number and range. 2. Provides a format option for a formatted grid layout or CSV output. 3. Supports a reverse flag to display the table in descending order. 4. Can process multiple numbers via a list argument, combining results. 5. Provides a summary of the total rows and columns generated.",
    "tags": [
      "python",
      "csv",
      "cli"
    ]
  },
  {
    "id": "2.9",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "YAML-Driven Pattern Generator",
    "description": "Write a Python develop a Python CLI program that: 1. Reads a YAML config file specifying pattern type, size, character, and orientation. 2. Parses the config, generates the pattern, and saves it to a specified .txt output file. 3. Handles invalid configurations with clear errors and logs warnings for unsupported options. 4. Ends with a confirmation message showing the config source, pattern generated, and output path.",
    "tags": [
      "python",
      "machine learning",
      "cli",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "2.10",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Multi-Format Text Converter",
    "description": "Write a Python create a Python CLI utility that: 1. Converts input text from argument or file to formats: uppercase, reversed, base64. 2. Allows selecting multiple transformations to apply sequentially. 3. Saves the final result to a .txt file with the output option. 4. Provides error handling for missing inputs. 5. Displays a summary of transformations applied and the output file.",
    "tags": [
      "python",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.11",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Text Scrambler",
    "description": "Write a Python build a Python CLI utility that: 1. Scrambles words in input text, preserving the first and last letters. 2. Can process entire files and supports a seed for reproducible results. 3. Saves output to a .txt file if requested. 4. Provides a summary of words scrambled, average length, and characters processed.",
    "tags": [
      "python",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.12",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Test Data Generator",
    "description": "Write a Python develop a Python CLI program that: 1. Generates random test data names, emails, phones, and addresses. 2. Allows specifying the number of records and output format CSV/JSON. 3. Provides flags to include/exclude specific fields, no- phone. 4. Ensures generated data follows valid formats. 5. Provides a summary of records created, format, and output location. ID & Content Generators",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "2.13",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Pattern-Based Name/ID Generator",
    "description": "Write a Python create a Python CLI program that: 1. Generates random names/IDs from patterns like ADJ- NOUN or VERB-NOUN. 2. Allows control over the quantity and optional prefix/suffix. 3. Includes a unique flag to ensure no duplicates across runs. 4. Supports saving results to a CSV file with output. 5. Provides a summary showing the total names created and the pattern used.",
    "tags": [
      "python",
      "csv",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.14",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Digital Dice Roller with Statistics",
    "description": "Write a Python develop a Python CLI tool that: 1. Simulates rolling dice, accepting dice and sides arguments. 2. Displays individual roll results and their sum. 3. Includes an option to save rolls with timestamps to a JSON file. 4. Provides a stats flag to display averages and outcome frequencies. 5. Ends with a summary of total rolls performed and aggregate results.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.15",
    "category": "Command-Line Utilities",
    "subcategory": "Generators",
    "title": "Categorized Quote Generator",
    "description": "Write a Python build a Python CLI utility that: 1. Generates random quotes from a JSON data file. 2. Allows requesting multiple quotes count and filtering by category. 3. Supports saving quotes to a .txt file. 4. Handles invalid categories gracefully. 5. Provides a summary of quotes generated, categories, and the output file. Visual Generators",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.16",
    "category": "Command-Line Utilities",
    "subcategory": "Visual Generators",
    "title": "Calendar Generator with Format Options",
    "description": "Write a Python CLI application that: 1. Generates calendars for a given year and optional month. 2. Provides output options: plain text, colored ASCII, or save to. txt. 3. Includes a week-start option to set the first day of the week Sun/Mon. 4. For yearly view, generates all months in sequence. 5. Provides a summary of the generated calendar and output format.",
    "tags": [
      "python",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "2.17",
    "category": "Command-Line Utilities",
    "subcategory": "Visual Generators",
    "title": "QR Code Generator & Viewer",
    "description": "Write a Python develop a Python CLI tool that: 1. Generates a PNG QR code from a provided text or URL. 2. Includes options for size, error correction level, and output directory. 3. Provides an open flag to automatically open the generated image. 4. Ends with a summary showing the encoded content, file path, and settings.",
    "tags": [
      "python",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.18",
    "category": "Command-Line Utilities",
    "subcategory": "Visual Generators",
    "title": "Batch Barcode Generator",
    "description": "Write a Python CLI program that: 1. Generates barcode PNG images from numeric input strings. 2. Provides options to control image size and scaling. 3. Supports a batch flag to process multiple inputs from a file. 4. Logs failed attempts to a JSON file. 5. Provides a summary of barcodes created, skipped inputs, and output location.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.19",
    "category": "Command-Line Utilities",
    "subcategory": "Visual Generators",
    "title": "Colored Banner Generator",
    "description": "Write a Python CLI tool that: 1. Generates colored ASCII-art banners from text input. 2. Allows specifying font style and color via arguments. 3. Supports printing to the terminal with ANSI colors or saving to a .txt file. 4. Handles invalid fonts/colors with warnings. 5. Provides a summary of the style used, text length, and output status. System Utilities",
    "tags": [
      "python",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.20",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "File Compression Manager",
    "description": "Write a Python create a Python CLI utility that: 1. Compresses text files using Gzip; includes a decompress mode. 2. Supports a batch flag to process all.txt files in a directory. 3. Handles missing/corrupted files gracefully. 4. Generates a JSON log of original/compressed sizes and savings. 5. Displays a summary of files processed and total space saved.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "2.21",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "Simple Task Scheduler",
    "description": "Write a Python build a Python CLI tool that: 1. Schedules a command to run after a time delay in seconds. 2. Supports recurring commands with a repeat flag. 3. Handles invalid commands gracefully. 4. Maintains a JSON log of executed commands with timestamps and status. 5. Prints a summary of total commands executed, errors, and average runtime.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "logging"
    ]
  },
  {
    "id": "2.22",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "System Monitor & Exporter",
    "description": "Write a Python develop a Python CLI program that: 1. Displays system info OS, CPU, memory, disk with flags like cpu, memory. 2. Provides a watch flag for real-time refreshing of data. 3. Supports saving results to a JSON file. 4. Handles unsupported platforms gracefully. 5. Ends with a summary of the sections retrieved and the output method.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.23",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "Persistent To-Do List Manager",
    "description": "Write a Python create a Python CLI application that: 1. Manages a to-do list: add, complete, list, and remove tasks stored in a JSON file. 2. Includes flags for priority and due dates. 3. Provides a summary command showing total, completed, and overdue tasks. 4. Automatically saves changes and logs the number of operations per session.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "2.24",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "File Tail Monitor with Highlighting",
    "description": "Write a Python CLI program that: 1. Monitors a text file and prints new lines as they are appended tail -f. 2. Allows specifying the number of initial lines to display. 3. Includes a keyword flag to highlight matching lines. 4. Handles file rotation/deletion gracefully. 5. Provides a summary of lines printed, matches found, and runtime.",
    "tags": [
      "python",
      "machine learning",
      "cli",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "2.25",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "Markdown to HTML Converter with Preview",
    "description": "Write a Python create a Python CLI utility that: 1. Converts Markdown files to HTML documents. 2. Includes an option to embed CSS styles. 3. Provides a preview flag to open the output in a web browser. 4. Handles Markdown syntax errors. 5. Displays a summary of files converted, lines processed, and output location.",
    "tags": [
      "python",
      "web",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.27",
    "category": "Command-Line Utilities",
    "subcategory": "System Utilities",
    "title": "System Usage Logger",
    "description": "Write a Python create a Python CLI tool that: 1. Captures CPU, memory, and disk stats to a CSV file with timestamps. 2. Provides interval and duration options for repeated logging. 3. Handles unavailable metrics gracefully. 4. Generates a summary of samples collected, average CPU, peak memory, and log size. Miscellaneous Tools",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "2.28",
    "category": "Command-Line Utilities",
    "subcategory": "Miscellaneous Tools",
    "title": "Secure Password Generator",
    "description": "Write a Python CLI tool that: 1. Generates random passwords with customizable length, and flags for numbers and symbols. 2. Uses a count flag to generate multiple passwords in one run. 3. Provides an option to copy the password to the clipboard automatically. 4. Can save results to a secure log file if requested. 5. Ends with a summary of the number of passwords generated and their average length.",
    "tags": [
      "python",
      "cli",
      "file processing",
      "logging",
      "security"
    ]
  },
  {
    "id": "2.29",
    "category": "Command-Line Utilities",
    "subcategory": "Miscellaneous Tools",
    "title": "Stopwatch and Timer Manager",
    "description": "Write a Python build a Python CLI program that: 1. Acts as a stopwatch stopwatch or countdown timer timer seconds. 2. Plays a sound or prints a message upon timer completion. 3. Supports a log flag to store timing results in a JSON file. 4. Allows running multiple timers sequentially. 5. Prints a summary of completed timers and total elapsed time.",
    "tags": [
      "python",
      "json",
      "cli",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.1",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "Structured JSON Logger with Rotation",
    "description": "Write a Python script using the logging and logging.handlers modules that: 1. Configures a logger to output messages in structured JSON format, including keys for timestamp, level, module, and message. 2. Supports all standard log levels DEBUG, INFO, WARNING, ERROR, CRITICAL selectable via a command-line flag, log-level INFO. 3. Stores logs in a rotating file using RotatingFileHandler, which creates a new file after the current log file reaches a specified size limit,10 MB. 4. At the end of execution, it generates and prints a summary report of the count of messages logged per severity level.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.2",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "Multi-Module Logger with File Separation",
    "description": "Write python script that: 1. Configures separate loggers for different application modules database, network, and app, each writing to its own dedicated log file: db.log, network.log, and errors.log. 2. Accepts a command-line argument, global-level INFO, to set the global logging level for all loggers. 3. Provides an optional merge flag that consolidates all log entries from the separate files into a single, combined log file with clearly labeled sections. 4. Write a Python program that: 5. Configures separate loggers for different application modules database, network, and app, each writing to its own dedicated log file: db.log, network.log, and errors.log. 6. Accepts a command-line argument, global-level INFO to set the global logging level for all loggers. 7. Provides an optional merge flag that consolidates all log entries from the separate files into a single, combined log file with clearly labeled sections. 8. At the end of execution, it displays a summary of the number of log entries written to each log file.",
    "tags": [
      "python",
      "sql",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.3",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "Automated Log Rotation & Archival",
    "description": "Write a Python design a Python script using logging.handlers. TimedRotatingFileHandler and zipfile that: 1. Monitors a running application's log file and automatically rotates it every24 hours. 2. Archives the rotated log file by compressing it into a ZIP archive, naming it with the date app_2023-10-27.log.zip. 3. Provides configurable retention policies via a retention- days N argument, keeping the last N days of archives and deleting older ones. 4. Includes error handling for failed compression or deletion operations, logging any skipped files as errors. 5. Generates a report at the end on how many log files were rotated, compressed, and deleted.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "3.4",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "Database Query Profiler & Auditor",
    "description": "Write a Python script that: 1. Integrates with a database driver psycopg2, sqlite3 to log all queries executed by an application. 2. Each log entry in the db_queries.log file includes the query text, execution time, number of affected rows, and any errors encountered. 3. Uses a RotatingFileHandler to manage the log file size. 4. Provides an optional summarize argument that generates a summary of queries, grouped by type SELECT, INSERT, etc., with counts and average execution times. 5. Includes error handling for invalid queries and failed connections. 6. Generates a final summary report of the slowest query, average execution time, and total number of queries logged.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.5",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "CLI Application Activity Auditor",
    "description": "Write a Python design a Python monitoring tool that: 1. Logs user activity within a custom command-line application by wrapping the command processing loop. 2. For each command executed, records the command string, timestamp, execution time, and success/failure result to an activity.log file. 3. Provides an option anonymize to hash or anonymize usernames in the logs for privacy. 4. Includes error handling to log invalid commands as WARNING-level entries. 5. Generates a report of the most frequently executed commands, average command execution time, and total number of commands processed.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "3.6",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "File I/O Performance Profiler",
    "description": "Write a Python script that: 1. Profiles the performance of file read and write operations by intercepting calls to open, read, and write. 2. For each operation, logs the filename, operation type read/write, size of data transferred, and duration to a CSV file with a timestamp. 3. Provides a verbose option to print progress of ongoing operations to the console. 4. Handles and logs errors permission denied, file not found as WARNING entries. 5. Generates a summary report of average read/write speeds MB/s, total files processed, and details of the slowest operation encountered.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.7",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "Email Transaction Logger",
    "description": "Write a Python tool using smtplib and email that: 1. Logs all email sending operations performed by an application. 2. For each email, record the recipients, subject, timestamp, and delivery status success/failure to a file named emails.log. 3. Provides options to also log the message size and names of any attachments. 4. Handles SMTP-specific errors invalid addresses, authentication failure, connection refused and logs them as WARNING entries with the error detail. 5. Generates a summary of the number of emails sent successfully, the number of failures, and the most common error message encountered.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.8",
    "category": "Logging & Debugging Systems",
    "subcategory": "Logging",
    "title": "Unit Test Results Aggregator",
    "description": "Write a Python script that: 1. Executes a project's unit tests using unittest or pytest modules and logs the results. 2. For each test, record the test name, status pass, fail, error, skipped, execution time, and any error or failure messages to a structured JSON file. 3. Provides options to generate a human-readable summary report in plain text for quick viewing in the console. 4. At the end of the test run, it displays statistics on the number of tests passed, failed, skipped, and average execution time across all tests. Debugging & Profiling",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.9",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "Function Execution Timer & Profiler",
    "description": "Write a Python create a Python debugging tool that: 1. Uses a decorator to wrap function calls, automatically logging the function name, start time, end time, and duration in milliseconds for each invocation. 2. Supports optional CLI flags save-format csv/json for saving the detailed timing results to a file. 3. Includes error handling to catch and log any exceptions raised inside decorated functions, including the full stack trace. 4. Provides a final summary report listing the top five slowest functions executed during the session and their average runtimes across all calls.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "3.10",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "Global Exception Handler & Logger",
    "description": "Write a Python create a Python debugging utility using sys.excepthook that: 1. Captures all uncaught exceptions in a running application and logs them into a structured JSON file. 2. Each log entry includes the exception type, error message, full stack trace, and timestamp of occurrence. 3. Provides a CLI option verbose to enable verbose mode, which prints the detailed error message to the console in addition to logging it. 4. At the end of execution, it generates a summary report of the total number of exceptions captured, grouped by exception type, and identifies the most frequently occurring error.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "3.11",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "Variable State Watcher",
    "description": "Write a Python build a Python debugging script that: 1. Allows a user to specify a list of variable names to watch via a configuration file or decorator. 2. Logs the value of each watched variable to a JSON file every time it changes, including the timestamp, function name, and line number where the change occurred. 3. Provides an optional replay mode that reads the log file and reconstructs the state of variables step-by-step for post-execution analysis. 4. Handles exceptions related to undefined variables or unsupported types gracefully. 5. Prints a summary of how many times each watched variable was modified during the program's execution.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.12",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "Function I/O Logger with Decorators",
    "description": "Write a Python build a Python debugging utility that: 1. Uses decorators to automatically wrap functions and log their input arguments, outputs, return values, and timestamps to a structured JSON file. 2. Provides a configuration option to mask sensitive data within the logged arguments/return values passwords, API keys. 3. Includes error handling to log any exceptions raised during the function's execution instead of returning a value. 4. At the end of a run, it generates a summary of the most frequently called functions, their average execution time, and the percentage of function calls that resulted in an error.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "3.13",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "Function Call Graph Generator",
    "description": "Write a Python develop a Python script that: 1. Tracks function calls during program execution to build a call graph, recording parent-child relationships between functions. 2. Saves the results as a directed graph structure in a JSON file, including metadata on the number of times each function was called. 3. Provides an optional visualize flag that uses graphviz or matplotlib to generate and save a PNG image of the call graph. 4. Handles recursion by detecting cycles in the graph and logging a warning if the recursion depth exceeds a safe limit. 5. Prints a summary of the total number of functions tracked, the most frequently called function, and the deepest recursion level observed.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.14",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "Object Lifecycle & Memory Tracker",
    "description": "Write a Python build a Python debugging script that: 1. Uses weak references weakref and a custom wrapper to track the creation and garbage collection of specific objects. 2. Logs object lifecycle events creation, deletion to a JSON file, including the object's type, memory size via sys.getsizeof, and timestamps. 3. Provides an optional plot flag that uses matplotlib to generate a timeline graph of memory usage for the tracked objects. 4. Handles errors for objects that cannot be sized or tracked. 5. Prints a summary of the peak memory usage per object type and the total number of objects tracked throughout the execution.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "3.15",
    "category": "Logging & Debugging Systems",
    "subcategory": "Debugging & Profiling",
    "title": "CPU Usage Profiler",
    "description": "Write a Python create a Python debugging utility using cProfile or pyinstrument that: 1. Profiles a target program's execution and records the percentage of CPU time spent in each function. 2. Logs the profiling results function name, total time, cumulative time, and call count into a JSON file. 3. Provides a top N flag to display the top N most CPU- intensive functions directly in the console during the summary. 4. Includes error handling for platforms where profiling is not supported. 5. Generates a summary report of the total runtime, overall CPU utilization, and identifies the functions responsible for the majority of CPU consumption.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.1",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "New File Detection Notifier",
    "description": "Write a Python develop a Python program using watchdog and smtplib that: 1. Monitors a specified project folder for new file creation events. 2. Captures the new file's name, size, and creation time. 3. Sends an email notification to a configured recipient with these details. 4. Provides CLI options to set the recipient address and email subject. 5. Handles network interruptions and invalid email addresses gracefully. 6. Generates a summary showing the number of notifications sent and details of the last file added.",
    "tags": [
      "python",
      "machine learning",
      "cli",
      "email",
      "file processing"
    ]
  },
  {
    "id": "4.2",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Disk Space SMS Alert",
    "description": "Write a Python develop a Python program using psutil and Twilio's API that: 1. Periodically checks system disk space usage for all drives. 2. Sends an SMS alert if usage exceeds a configured threshold,90% for any drive. 3. The SMS includes drive name, usage %, and a cleanup suggestion. 4. Implements retry logic for failed SMS deliveries. 5. Provides configuration for phone numbers and thresholds. 6. Generates a summary showing checks performed, alerts triggered, and average disk usage.",
    "tags": [
      "python",
      "api",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "4.3",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "High CPU Usage Alert",
    "description": "Write a Python script using psutil and smtplib that: 1. Monitors CPU usage at fixed intervals. 2. Sends an email alert if usage stays >90% for5 consecutive minutes. 3. The alert includes the name of the top CPU-consuming process. 4. Configurable monitoring interval and recipient list. 5. Handles system permission issues. 6. Generates a summary showing avg CPU usage, alerts triggered, and the top process.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "monitoring"
    ]
  },
  {
    "id": "4.4",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Log File Keyword Alert",
    "description": "Write a Python create a Python utility using watchdog and smtplib that: 1. Monitors a log file in real-time for keywords like \"error\", \"failure\", \"critical\". 2. Sends an email alert upon detection, including timestamp, keyword, and log context. 3. Configurable keywords and recipients. 4. Handles file permission issues. 5. Generates a summary showing alerts triggered, the most frequent keyword, and log entries processed.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.5",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Stock Price Alert",
    "description": "Write a Python develop a Python program using a financial API, Alpha Vantage and smtplib that: 1. Monitors a list of stock symbols for user-configured price thresholds. 2. Sends an immediate email alert when a threshold is crossed: symbol, price, % change. 3. Handles API rate limits and connection errors. 4. Generates a summary showing stocks monitored, alerts triggered, and top mover.",
    "tags": [
      "python",
      "api",
      "machine learning",
      "email",
      "monitoring"
    ]
  },
  {
    "id": "4.6",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Daily Server Performance Report",
    "description": "Write a Python scheduler using psutil, pandas, and smtplib that: 1. Monitors server metrics, CPU, memory, and disk usage throughout the day. 2. Compiles a daily report and emails it to system admins at midnight. 3. Includes retry logic for failed email delivery. 4. Handles missing metric data. 5. Generates a summary showing average usage, critical alerts, and email status.",
    "tags": [
      "python",
      "pandas",
      "machine learning",
      "email",
      "logging"
    ]
  },
  {
    "id": "4.7",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Slack File Upload Notifier",
    "description": "Write a Python create a Python script using Slack API and watchdog that: 1. Monitors a shared folder for new file uploads. 2. Sends a Slack message to a configured channel for each new file name, uploader, and timestamp. 3. Handles API errors and permission issues. 4. Generates a summary showing notifications sent, the most common file type, and the total size processed.",
    "tags": [
      "python",
      "api",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "4.8",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "GitHub New Issue Notifier",
    "description": "Write a Python develop a Python utility using GitHub API and smtplib that: 1. Monitors configured GitHub repositories for new issues. 2. Sends an email notification for each new issue title, author, date, and link. 3. Handles API rate limits and auth errors. 4. Generates a summary showing issues detected, repos monitored, and the most active issue author.",
    "tags": [
      "python",
      "api",
      "machine learning",
      "email",
      "monitoring"
    ]
  },
  {
    "id": "4.9",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Calendar Meeting Reminder",
    "description": "Write a Python create a Python script using Google Calendar API and smtplib that: 1. Fetches upcoming meetings from a calendar. 2. Sends email reminders1 hour before each meeting. 3. The email includes title, participants, time, and link. 4. Handles API auth failures. 5. Generates a summary showing reminders sent, busiest weekday, and total meetings.",
    "tags": [
      "python",
      "api",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.10",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "High Memory Usage Alert",
    "description": "Write a Python develop a Python program using psutil and smtplib that: 1. Monitors system memory usage. 2. Sends an email alert if usage exceeds a configured threshold. 3. The alert includes the top memory-consuming process and available memory. 4. Configurable thresholds and recipients. 5. Handles permission issues. 6. Generates a summary showing checks performed, alerts triggered, and the top process.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "monitoring"
    ]
  },
  {
    "id": "4.11",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Brand Mention Slack Alert",
    "description": "Write a Python develop a Python program using Twitter API and Slack API that: 1. Monitors Twitter for mentions of a configured brand name. 2. Sends a real-time Slack notification for each mention text, author, time, and link. 3. Handles API authentication errors. 4. Generates a summary showing mentions detected, the most frequent keyword, and the top author.",
    "tags": [
      "python",
      "api",
      "monitoring"
    ]
  },
  {
    "id": "4.12",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Notifications & Alerts",
    "title": "Daily Cron Job Summary",
    "description": "Write a Python create a Python script that: 1. Integrates with system cron to track job execution start/end time, status. 2. Sends a daily email summary of all cron jobs, highlighting failures. 3. Configurable recipient address. 4. Handles permission errors and missing job data. 5. Generates a summary showing total jobs, successful runs, and failures. Email Automation",
    "tags": [
      "python",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.13",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Daily Motivational Quote Sender",
    "description": "Write a Python script using smtplib and csv that: 1. Reads a list of recipients' names, email from a recipients.csv file. 2. Fetches a \"Quote of the Day\" from a public API, quotable.io. 3. Sends a personalized email to each recipient, including their name, the quote, and the current date. 4. Implements retry logic for3 attempts for SMTP connection failures. 5. Logs each successful and failed delivery to email_log.json. 6. Generates a summary showing total emails sent, failed attempts, and average time per message.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "4.14",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Automated Weekly Sales Report",
    "description": "Write a Python create a Python utility using pandas, matplotlib, and smtplib that: 1. Reads sales data from a weekly_sales.csv file. 2. Calculates weekly totals, averages, and generates a bar chart image sales_chart.png. 3. Composes an email to management with a text summary and the chart attached. 4. Allows configuration of delivery schedule, every Friday5 PM via a config file. 5. Handles missing data points in the CSV. 6. Logs a summary showing records processed, total revenue, and email delivery status.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.15",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Automated Nightly Backup",
    "description": "Write a Python create a Python scheduler using schedule, zipfile, and smtplib that: 1. Creates a zip archive of a target folder every night at midnight. 2. Sends an email notification post-backup with archive name, size, and duration. 3. Handles permission errors and missing source files gracefully. 4. Accepts the backup source path as a command-line argument. 5. Generates a summary report showing total backup size, time taken, and email status.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "4.16",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Unread Email Desktop Notifier",
    "description": "Write a Python script using imaplib and plyer that: 1. Checks a Gmail inbox via IMAP for new unread emails. 2. Sends a desktop notification for each new message, showing sender, subject, and time. 3. Configurable polling frequency and subject keyword filters. 4. Handles login failures and connection timeouts. 5. Generates a summary showing new messages detected, notifications sent, and the most common sender.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "logging"
    ]
  },
  {
    "id": "4.17",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Daily Weather Update Sender",
    "description": "Write a Python develop a Python program using smtplib and a weather API, OpenWeatherMap, that: 1. Fetches daily weather data, temp, humidity, forecast, sunrise/sunset for a configured city. 2. Sends a formatted email with this information to a list of recipients. 3. Configurable delivery time and recipient list. 4. Handles API errors and invalid city names. 5. Generates a daily summary showing emails sent, average forecasted temp, and the most frequent city.",
    "tags": [
      "python",
      "api",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.18",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Bill Payment Reminder",
    "description": "Write a Python create a Python scheduler using schedule and smtplib that: 1. Reads bill details due date, amount, payee, and recipient email from bills.json. 2. Sends reminder emails3 days before the due date and on the due date if unpaid. 3. Handles invalid email addresses and missing bill entries. 4. Configurable reminder frequency. 5. Generates a summary showing bills processed, reminders sent, and total outstanding balance.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.19",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Daily Inspirational Message Sender",
    "description": "Write a Python develop a Python program using Twilio's API that: 1. Sends a daily SMS with a random quote from quotes.txt to a list of recipients. 2. Configurable delivery time and recipient phone numbers. 3. Retries failed SMS deliveries up to3 times. 4. Generates a summary showing total messages sent, failures, and the most sent quote.",
    "tags": [
      "python",
      "api",
      "machine learning"
    ]
  },
  {
    "id": "4.20",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Daily Expense Report",
    "description": "Write a Python scheduler using pandas, matplotlib, and smtplib that: 1. Reads daily expenses from expenses.csv. 2. Categorizes expenses, calculates totals, and generates a bar chart. 3. Emails a CSV summary and the chart image to finance managers. 4. Configurable reporting time. 5. Handles invalid entries. 6. Generates a summary showing total expenses, the highest spending category, and email status.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.21",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Software Update Notifier",
    "description": "Write a Python develop a Python script using subprocess and plyer that: 1. Checks for available system software updates, via apt list upgradable. 2. Sends a desktop notification with package names, versions, and sizes if updates exist. 3. Configurable check frequency. 4. Handles permission errors. 5. Generates a summary showing packages checked, updates available, and total download size.",
    "tags": [
      "python",
      "machine learning"
    ]
  },
  {
    "id": "4.22",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Automated Birthday Greetings",
    "description": "Write a Python create a Python utility using csv and smtplib that: 1. Reads employee data name, email, and birthdate from employees.csv. 2. Sends a personalized birthday greeting email on each employee's birthday. 3. Handles invalid emails and missing birthdates. 4. Configurable email template. 5. Generates a summary showing birthdays detected, greetings sent, and failed attempts.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.23",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Weekly Project Status Update",
    "description": "Write a Python scheduler using JSON, matplotlib, and smtplib that: 1. Reads project task status from projects.json. 2. Calculates completed, pending, overdue tasks; generates a Gantt chart image. 3. Emails a formatted summary and chart to the team. 4. Handles invalid task data. 5. Generates a summary showing projects processed, tasks completed, and overall progress %.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.25",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Student Task Reminder",
    "description": "Write a Python develop a Python program using JSON and smtplib that: 1. Reads student tasks subject, description, deadline, and email from tasks.json. 2. Sends daily email reminders for tasks due soon. 3. Configurable recipient list. 4. Handles invalid entries. 5. Generates a summary showing tasks processed, reminders sent, and the most common subject.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.26",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Subscription Renewal Reminder",
    "description": "Write a Python scheduler using openpyxl and smtplib that: 1. Reads customer subscription data from subscriptions.xlsx. 2. Sends a personalized renewal reminder email one week before expiry. 3. Configurable email template. 4. Handles invalid data. 5. Generates a summary showing reminders sent, % due for renewal, and the most expiring plan.",
    "tags": [
      "python",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "4.27",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Medication Reminder",
    "description": "Write a Python scheduler using the Twilio API that: 1. Reads patient medication schedules from patients.json. 2. Sends timely SMS reminders: name, medication, dosage, and time. 3. Configurable message template. 4. Handles invalid entries. 5. Generates a summary showing reminders sent, the most common medication, and patients processed.",
    "tags": [
      "python",
      "api",
      "json"
    ]
  },
  {
    "id": "4.28",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Automated Newsletter Delivery",
    "description": "Write a Python create a Python utility using markdown, csv, and smtplib that: 1. Reads newsletter content from a Markdown file and converts it to HTML. 2. Reads recipient list from a CSV file. 3. Sends the HTML newsletter to all recipients at a scheduled time. 4. Handles invalid emails and retries failures. 5. Generates a summary showing successful sends, failures, and the most common recipient domain.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "4.29",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Email Automation",
    "title": "Library Book Return Reminder",
    "description": "Write a Python scheduler using openpyxl and smtplib that: 1. Reads library data user, book, due date, and email from library.xlsx. 2. Sends reminder emails3 days before a book is due. 3. Configurable email template. 4. Handles missing data. 5. Generates a summary showing reminders sent, overdue books, and the most borrowed title. Monitoring",
    "tags": [
      "python",
      "machine learning",
      "email",
      "monitoring"
    ]
  },
  {
    "id": "4.30",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Real-Time System Resource Logger",
    "description": "Write a Python develop a Python utility using psutil that: 1. Monitors system resource usage CPU percentage, memory usage, disk usage in real-time at a fixed, user-configurable interval, interval5 seconds. 2. Allows the user to configure the output format for the collected data via a format csv/json argument. 3. Saves the collected data into a timestamped file metrics.csv or metrics.json. 4. Includes configurable thresholds, cpu-warn80, that trigger a WARNING log entry when 5. exceeded. 6. Provides a summary report at the end: peak usage values, average CPU load, and total number of samples collected.",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.31",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Website Uptime & Performance Checker",
    "description": "Write a Python build a Python monitoring tool using the requests library that: 1. Tracks the availability of a list of websites provided in a config file, sending an HTTP GET request at a user- configurable interval. 2. For each check, logs the URL, HTTP status code, response time in milliseconds, and availability status to a CSV file with a timestamp. 3. If a website is unavailable, non-200 status or timeout, records an ERROR log entry and sends an alert email to a configured administrator address. 4. Provides options to configure the monitoring frequency interval60 and email recipients via a config file. 5. Prints a summary at the end of the uptime percentage for each monitored website.",
    "tags": [
      "python",
      "requests",
      "csv",
      "web",
      "machine learning"
    ]
  },
  {
    "id": "4.32",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Filesystem Event Watcher",
    "description": "Write a Python develop a Python monitoring tool using the watchdog library that: 1. Tracks filesystem changes create, modify, delete, move in a specified directory and logs each event to a JSON file with a timestamp. 2. Provides command-line options to filter events by file extension filter.txt, .log and to monitor subdirectories recursively recursive. 3. Implements a configurable alert threshold alert- threshold10 that triggers a WARNING log if more than N events occur within a60-second window. 4. At the end of monitoring, it displays a summary report of the total number of events captured and a breakdown of counts by event type.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "4.33",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "API Request/Response Interceptor",
    "description": "Write a Python create a Python script that: 1. Acts as a middleware or uses library hooks from requests or http.client to log API request and response data for debugging. 2. For each API call, records the endpoint URL, HTTP method, request headers, request body, response status code, response headers, and duration into a JSON log file. 3. Provides a configuration option to mask sensitive fields in the logs, Authorization headers, and password fields. 4. Includes a filter flag to log only failed requests status >=400 or requests exceeding a specified response time threshold. 5. Generates a report summarizing the total requests logged, error rate %, and average response time.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "4.34",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Scheduled Task Execution Tracker",
    "description": "Write a Python tool that: 1. Monitors the execution of a list of scheduled tasks, functions or shell commands. 2. For each task, logs the start time, end time, duration, and status success/failure to a structured CSV file. 3. Provides an option retry-failed to automatically retry failed tasks a specified number of times, logging each attempt. 4. Includes error handling to capture and log detailed exception messages for tasks that fail. 5. Generates a summary report of the total number of tasks executed, success rate, average runtime, and details of the longest-running task.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.35",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Network Socket Connection Logger",
    "description": "Write a Python create a Python utility using the socket library and perhaps psutil that: 1. Logs network socket activity, recording for each connection: source IP, destination IP, source port, destination port, and connection status/duration. 2. Provides an option log-traffic to record packet counts and total bytes transferred for each connection. 3. Saves logs into a rotating file named network.log. 4. Includes a summary flag to display a real-time summary of active connections, failed connection attempts, and total bandwidth usage. 5. Handles permission errors gracefully, requiring sudo on Linux. 6. Prints a final summary highlighting peak concurrent connections and total traffic captured during the session.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.36",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "System Log Error Scraper & Alert",
    "description": "Write a Python develop a Python tool that: 1. Parses system log files /var/log/syslog on Linux to extract lines containing error keywords like \"ERROR\", \"FAIL\", or \"CRITICAL\". 2. Saves all matching lines into a new, consolidated log file named errors.log. 3. Provides an option alert-on-critical to send an alert email when a line containing a \"CRITICAL\" keyword is detected. 4. Includes error handling for missing or unreadable log files, logging these events itself. 5. Generates a summary report of the total number of errors found, grouped by keyword, and the time range of the parsed logs.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.37",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Hardware Metrics API Poller",
    "description": "Write a Python create a Python script that: 1. Integrates with a third-party monitoring API HWInfo's API, or pysmart for S.M.A.R. T.data, to fetch hardware health metrics. 2. Logs metrics CPU temperature, fan speeds in RPM, battery health/percentage into a JSON file with timestamps. 3. Accepts command-line arguments to specify the polling interval poll-interval30 and the API endpoint/configuration. 4. Handles API errors, timeouts, and invalid responses gracefully, logging them as WARNING entries. 5. Generates a report summarizing collected metrics, including max/min values, averages, and the total number of successful samples.",
    "tags": [
      "python",
      "api",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.38",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Message Queue Consumer Auditor",
    "description": "Write a Python design a Python script that: 1. Monitors a message queue system RabbitMQ via pika, Kafka via kafka-python, consuming messages from a specified queue. 2. Logs details of each message processed, including timestamp, queue name, message size, and processing status success/failure to a CSV file. 3. Provides an option to track and log the number of retries for failed messages and the specific error that caused the failure. 4. Includes error handling for connection timeouts and channel errors. 5. Generates a summary report of the total messages processed, success rate %, and average size of messages handled.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.39",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "System Process Snapshot Logger",
    "description": "Write a Python create a Python monitoring script using psutil that: 1. Takes snapshots of running processes on the system at a fixed interval, logging the process ID PID, name, CPU usage %, memory usage MB, and start time to a JSON file. 2. Provides a top N flag to only log the top N most resource- intensive processes in each snapshot. 3. Handles AccessDenied exceptions gracefully by skipping restricted processes and logging a warning. 4. Generates a summary of the total number of process snapshots taken, peak CPU usage observed, and average memory usage across all processes.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "4.40",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "System Reboot & Uptime Historian",
    "description": "Write a Python utility that: 1. Monitors system uptime by periodically checking the system boot time using psutil.boot_time or reading /proc/uptime on Linux. 2. Logs each reboot event, recording the timestamp of the reboot and the duration of the previous uptime period to a CSV file. 3. Provides a summary flag to display statistics including average uptime duration, total number of reboots detected in the log, and time since the last reboot. 4. Handles OSError exceptions on unsupported platforms gracefully. 5. Generates a final report summarizing historical uptime patterns and stability.",
    "tags": [
      "python",
      "csv",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "4.41",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "User Authentication Audit Logger",
    "description": "Write a Python design a Python tool that: 1. Parses system authentication logs /var/log/auth.log on Linux, Security Event Log on Windows to track user login attempts. 2. For each attempt, record the username, timestamp, source IP address, and login status success/failure to a CSV file for auditing. 3. Provides an option alert-failures5 to trigger an alert if more than N failed login attempts are detected from the same IP address within a10-minute window. 4. Handles errors related to missing log files or parse errors gracefully. 5. Generates a report summarizing total login attempts, success rate %, and the most common source IPs of failed attempts.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "4.42",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Service Health Checker",
    "description": "Write a Python create a Python monitoring utility using subprocess or psutil that: 1. Checks the status active, inactive, failed of a list of system services nginx, PostgreSQL. 2. Logs the service name, status, timestamp, and any systemd/initd status codes to a JSON file. 3. Provides options to retry checks for services that report a transient failure state. 4. Implements alert escalation for services marked as \"critical\" in a configuration file. 5. Handles unsupported services by logging a warning and skipping them. 6. Generates a summary of the total services checked, percentage active, percentage failed, and a list of critical services that are down.",
    "tags": [
      "python",
      "json",
      "sql",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "4.43",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Network Latency & Packet Loss Pinger",
    "description": "Write a Python develop a Python tool using the subprocess module and the ping command that: 1. Monitors network latency to a list of servers by sending ICMP ping packets at a regular interval. 2. For each server, record the average round-trip time RTT, packet loss percentage, and jitter to a CSV file with timestamps. 3. Provides an option continuous to run the monitoring continuously until stopped by the user. 4. Handles unreachable hosts gracefully by logging them as warnings and recording100% packet loss. 5. Generates a summary of the fastest and slowest servers by RTT, overall average latency, and percentage of servers with high packet loss>5%.",
    "tags": [
      "python",
      "csv",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "4.44",
    "category": "Monitoring, Alerts, & Notifications",
    "subcategory": "Monitoring",
    "title": "Application Heartbeat Listener",
    "description": "Write a Python build a Python monitoring script that: 1. Listens for periodic heartbeat signals UDP packets, HTTP requests, and writes to a file from a monitored application. 2. Logs the timestamp of each received heartbeat to a structured JSON file. 3. If a heartbeat is not received within a specified interval timeout30, logs a WARNING and can trigger an alert email. 4. At the end of the monitoring session, it generates a summary report of the total heartbeats received, the number of missed signals, and the calculated percentage uptime of the monitored application.",
    "tags": [
      "python",
      "requests",
      "json",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "5.1",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "Monthly Sales Report Generator",
    "description": "Write a Python script using pandas that: 1. Reads a CSV file of sales transactions date, customer_id, and amount. 2. Calculates total revenue, average transaction value, and unique customer count. 3. Saves the summary to sales_summary.json and a formatted sales_summary.txt. 4. Handles missing files and corrupt data gracefully. 5. Includes an optional chart flag to generate a matplotlib bar chart of monthly sales. 6. Prints a console summary comparing current revenue to the previous month.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "file processing"
    ]
  },
  {
    "id": "5.2",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "Employee Attendance Analyzer",
    "description": "Write a Python develop a Python utility using pandas that: 1. Imports Excel files with employee attendance data, date, and status. 2. Calculates total working days, absences, and lates per employee. 3. Saves results to a new Excel file with Summary and Detailed Logs sheets. 4. Validates data for missing dates or invalid status codes. 5. Provides an email flag to send the report to HR via smtplib. 6. Displays a summary of employees processed and key absenteeism rates.",
    "tags": [
      "python",
      "pandas",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "5.3",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "Financial Data Consolidation",
    "description": "Write a Python build a Python utility that: 1. Merges multiple financial CSV files: date, amount, category. 2. Removes duplicates, calculates category totals. 3. Saves cleaned data to master_finances.xlsx. 4. Provides a summary flag to generate a PDF report with charts. 5. Handles corrupted files gracefully. 6. Prints summary: files merged, date range, net profit/loss.",
    "tags": [
      "python",
      "csv",
      "file processing"
    ]
  },
  {
    "id": "5.4",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "E-commerce Sales Dashboard",
    "description": "Write a Python create a Python script that: 1. Processes order data CSV category, revenue, and refund_status. 2. Generates category report: orders, revenue, AOV, refund rate. 3. Saves report to CSV and an interactive HTML dashboard. 4. Handles missing categories. 5. Displays summary: best-selling category, revenue leader, and refund rate.",
    "tags": [
      "python",
      "csv",
      "machine learning"
    ]
  },
  {
    "id": "5.5",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "Product Return Analysis",
    "description": "Write a Python script that: 1. Analyzes return data from Excel product_id, refund_amount, reason. 2. Calculates return rate, avg refund, most common reason per product. 3. Saves results to return_analysis.json. 4. Optional pdf flag for a chart-based report. 5. Handles incomplete records. 6. Displays summary: highest return rate product, average refund, overall return rate.",
    "tags": [
      "python",
      "json"
    ]
  },
  {
    "id": "5.6",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "HR Performance Review Consolidator",
    "description": "Write a Python utility that: 1. Consolidates HR Excel files employee_id, scores, comments. 2. Calculates avg scores, promotion readiness, common improvement areas, and NLP. 3. Saves results to hr_summary.json. 4. Optional individual-reports flag for employee PDFs. 5. Handles inconsistent scales. 6. Displays summary: top3 employees, most common improvement area, avg score.",
    "tags": [
      "python",
      "json",
      "file processing"
    ]
  },
  {
    "id": "5.7",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "Sales Lead Conversion Reporter",
    "description": "Write a Python build a Python script that: 1. Analyzes leads CSV data source, created_date, converted_date, deal_amount. 2. Calculates conversion rate by source, avg deal size, and avg time-to-close. 3. Saves results to lead_conversion.json. 4. highlight flag marks high-probability leads. 5. Handles open leads. 6. Displays summary: top3 lead sources, overall conversion rate, and leads analyzed.",
    "tags": [
      "python",
      "csv",
      "json"
    ]
  },
  {
    "id": "5.8",
    "category": "Data Analysis & Reporting",
    "subcategory": "Business & Finance",
    "title": "Financial Transaction Anomaly Detector",
    "description": "Write a Python develop a Python utility that: 1. Reads transaction CSV data: timestamp, amount, merchant. 2. Uses statistical methods to identify anomalous transactions. 3. Saves flagged transactions to anomalies.csv. 4. Optional report flag generates a PDF summary. 5. Handles corrupt data. 6. Displays summary: anomalies detected, largest transaction, and average transaction size. Web & User Analytics",
    "tags": [
      "python",
      "csv",
      "web"
    ]
  },
  {
    "id": "5.9",
    "category": "Data Analysis & Reporting",
    "subcategory": "Web & User Analytics",
    "title": "Website User Behavior Profiler",
    "description": "Write a Python create a Python program that: 1. Reads JSON logs of website visits user_id, session_id, timestamp, page_url. 2. Calculates per-user metrics: sessions, avg session duration, and most visited page. 3. Saves results to user_behavior.csv. 4. Offers an optional visualize flag to create pie charts of top pages. 5. Handles incomplete sessions. 6. Prints a summary of the top5 users by session count.",
    "tags": [
      "python",
      "csv",
      "json",
      "web",
      "logging"
    ]
  },
  {
    "id": "5.10",
    "category": "Data Analysis & Reporting",
    "subcategory": "Web & User Analytics",
    "title": "Survey Results Analyzer",
    "description": "Write a Python script using pandas that: 1. Processes survey CSV data: age, region, satisfaction_score, feedback_text. 2. Calculates avg score, response distribution, and positive feedback percentage. 3. Generates a PDF report with charts, bar charts, and histograms. 4. Allows filtering by demographics age-group, region. 5. Handles missing values and logs them. 6. Displays summary: overall rating, most common feedback, total responses.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "logging"
    ]
  },
  {
    "id": "5.11",
    "category": "Data Analysis & Reporting",
    "subcategory": "Web & User Analytics",
    "title": "Web Server Traffic Report",
    "description": "Write a Python develop a Python utility that: 1. Parses server log files to extract visits, IPs, URLs, and response times. 2. Calculates daily totals, unique visitors, avg response time, and top pages. 3. Saves results to traffic_report.json. 4. Optional chart flag for traffic trend line graphs. 5. Handles malformed log entries. 6. Displays summary: peak traffic day, slowest response, top pages.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "5.12",
    "category": "Data Analysis & Reporting",
    "subcategory": "Web & User Analytics",
    "title": "Social Media Engagement Analyzer",
    "description": "Write a Python develop a Python script that: 1. Analyzes social media JSON data likes, shares, comments, hashtags. 2. Calculates avg engagement per post and per day. 3. Saves results to engagement_metrics.csv. 4. trending flag identifies top posts and hashtags. 5. Handles missing metrics. 6. Displays summary: top3 posts, avg engagement, posts analyzed. Education & Academia",
    "tags": [
      "python",
      "csv",
      "json"
    ]
  },
  {
    "id": "5.13",
    "category": "Data Analysis & Reporting",
    "subcategory": "Education & Academia",
    "title": "Student Exam Performance Analyzer",
    "description": "Write a Python design a Python program that: 1. Analyzes exam scores from Excel student_id, subject, score. 2. Calculates per-subject averages, highs, lows, and standard deviation. 3. Generates individual PDF reports for each student. 4. Includes a rank flag to create a leaderboard CSV. 5. Handles missing scores. 6. Displays summary: students processed, top3 performers.",
    "tags": [
      "python",
      "csv"
    ]
  },
  {
    "id": "5.14",
    "category": "Data Analysis & Reporting",
    "subcategory": "Education & Academia",
    "title": "Academic Publication Metrics",
    "description": "Write a Python develop a Python script that: 1. Processes research CSV data researcher_id, citation_count. 2. Calculates publications, avg citations, and h-index per researcher. 3. Saves ranked results to research_metrics.xlsx. 4. visualize flag creates bar charts of top researchers. 5. Handles missing citations. 6. Displays summary: top3 by h-index, avg h-index, publications processed.",
    "tags": [
      "python",
      "csv"
    ]
  },
  {
    "id": "5.15",
    "category": "Data Analysis & Reporting",
    "subcategory": "Education & Academia",
    "title": "University Course Enrollment Analyzer",
    "description": "Write a Python utility that: 1. Analyzes enrollment CSV data course_id, department, grade. 2. Calculates enrollment numbers, avg grades, and pass rates per course. 3. Saves results to course_enrollment.xlsx. 4. visualize flag generates enrollment and grade charts. 5. Handles missing grades. 6. Displays summary: course with the highest enrollment, department with the highest pass rate, and total students.",
    "tags": [
      "python",
      "csv"
    ]
  },
  {
    "id": "5.16",
    "category": "Data Analysis & Reporting",
    "subcategory": "Education & Academia",
    "title": "Academic Course Evaluation Summarizer",
    "description": "Write a Python script that: 1. Consolidates course evaluation Excel files instructor_id, rating_teaching, rating_content. 2. Calculates avg ratings for instructors and course content. 3. Saves results to evaluation_summary.json. 4. Option to generate bar charts comparing departments. 5. Handles incomplete surveys. 6. Displays summary: highest-rated instructor, lowest-rated course, overall satisfaction score. Science, Environment & IoT",
    "tags": [
      "python",
      "json",
      "file processing"
    ]
  },
  {
    "id": "5.17",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Weather Data Consolidation",
    "description": "Write a Python script that: 1. Consolidates weather CSV files: date, max_temp, rainfall. 2. Calculates daily averages, weekly totals, and extremes. 3. Saves data to weather_consolidated.xlsx with charts. 4. forecast flag generates a7-day temp forecast using moving averages. 5. Handles missing readings. 6. Displays summary: hottest day, highest rainfall, days analyzed.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing",
      "testing"
    ]
  },
  {
    "id": "5.18",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "IoT Sensor Anomaly Detector",
    "description": "Write a Python create a Python program that: 1. Reads sensor JSON logs sensor_id, timestamp, value. 2. Calculates averages and identifies statistical anomalies. 3. Saves results to sensor_anomalies.csv. 4. Option to generate a real-time plotly dashboard. 5. Handles data gaps. 6. Displays summary: anomalies per sensor, average value, and data points processed.",
    "tags": [
      "python",
      "csv",
      "json",
      "logging"
    ]
  },
  {
    "id": "5.19",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Airline Flight Performance",
    "description": "Write a Python build a Python program that: 1. Reads flight CSV data route, departure_time, arrival_time, cancellation_code. 2. Calculates avg duration, cancellation rate, and avg delay per route. 3. Saves results to flight_performance.xlsx. 4. Option to generate a PDF performance report. 5. Handles incorrect times. 6. Displays summary: most delayed route, cancellation %, flights analyzed.",
    "tags": [
      "python",
      "csv"
    ]
  },
  {
    "id": "5.20",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Healthcare Billing Summary",
    "description": "Write a Python create a Python program that: 1. Processes billing Excel data department, billing_amount, paid_status. 2. Calculates department totals, avg billing amount, and outstanding balances. 3. Saves results to billing_summary.json. 4. Option to generate a PDF report with charts. 5. Handles invalid amounts. 6. Displays summary: highest revenue department, average billing per patient, total outstanding.",
    "tags": [
      "python",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "5.21",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Energy Consumption Analyzer",
    "description": "Write a Python program that: 1. Processes energy CSV data timestamp, kWh_usage. 2. Calculates daily, weekly, monthly averages and peak hours. 3. Saves analysis to an Excel file with multiple sheets. 4. Option to generate trend line charts. 5. Handles invalid readings. 6. Displays summary: highest consumption, average monthly usage, and records processed.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "5.22",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Hospital Patient Record Statistics",
    "description": "Write a Python develop a Python utility that: 1. Processes patient CSV data: age, gender, diagnosis, readmission. 2. Calculates avg age, gender distribution, common diagnoses, and readmission rate. 3. Saves results to Excel with multiple sheets and charts. 4. Handles inconsistent data. 5. Displays summary: most common diagnosis, average age, and records analyzed.",
    "tags": [
      "python",
      "csv"
    ]
  },
  {
    "id": "5.23",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Transportation Punctuality Analyzer",
    "description": "Write a Python build a Python utility that: 1. Analyzes transport CSV data route_id, scheduled_time, actual_time. 2. Calculates avg delay, on-time performance %, busiest routes. 3. Saves results to punctuality_metrics.json. 4. Option to generate a delay heatmap by route and time. 5. Handles invalid timestamps. 6. Displays summary: most delayed route, overall punctuality %, and trips analyzed.",
    "tags": [
      "python",
      "csv",
      "json"
    ]
  },
  {
    "id": "5.24",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Project Task Completion Dashboard",
    "description": "Write a Python create a Python program that: 1. Reads task logs CSV due_date, completion_date, status. 2. Calculates avg completion time, overdue tasks, and on- time percentage. 3. Saves analysis to task_metrics.json. 4. report flag generates an HTML dashboard. 5. Handles invalid dates. 6. Displays summary: total tasks, on-time %, longest overdue task.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "5.25",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Online Store Review Analyzer",
    "description": "Write a Python design a Python utility that: 1. Processes store review CSV data product_id, rating, review_text. 2. Performs sentiment analysis on text, calculates average rating per product. 3. Saves results to Excel with summary and raw data sheets. 4. Generates bar charts and pie charts. 5. Handles missing text. 6. Displays summary: highest/lowest rated product, % positive sentiment.",
    "tags": [
      "python",
      "csv"
    ]
  },
  {
    "id": "5.26",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Tourism Revenue & Visitor Analysis",
    "description": "Write a Python create a Python program that: 1. Processes tourism Excel data country, arrival_date, departure_date, revenue. 2. Calculates visitors by country, avg stay duration, and revenue per country. 3. Saves results to Excel with multiple sheets. 4. Option to generate visual dashboards, maps. 5. Handles missing data. 6. Displays summary: top3 countries by visitors/revenue, avg stay, total revenue.",
    "tags": [
      "python"
    ]
  },
  {
    "id": "5.27",
    "category": "Data Analysis & Reporting",
    "subcategory": "Science, Environment & IoT",
    "title": "Manufacturing Production Log Consolidator",
    "description": "Write a Python design a Python script that: â¢ Consolidates production CSV logs, timestamp, units_produced, and defects. â¢ Calculates daily totals, defect rate, and average production time. â¢ Saves results to production_summary.json. â¢ Option to generate an HTML production dashboard. â¢ Handles corrupt data. â¢ Displays summary: highest output day, defect rate %, total units.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "6.1",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Keyword-Based Site Crawler",
    "description": "Write a Python crawler using requests and beautifulsoup4 that: 1. Starts at a specified homepage and systematically follows all internal links. 2. Extracts all visible text from each page and searches for a user-defined target keyword. 3. Stores every sentence containing the keyword, along with its source URL, in a results.json file. 4. Implements polite crawling: respects robots.txt, uses a configurable delay between requests, and sets a user- agent. 5. Generates a final report showing: total pages scanned, pages with matches, skipped non-HTML pages, and errors encountered.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "json",
      "scraping"
    ]
  },
  {
    "id": "6.2",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "E-commerce Price Monitor",
    "description": "Write a Python develop a Python scraper using scrapy or requests-html that: 1. Crawls an e-commerce site's product category pages to collect real-time pricing data. 2. For each product, extracts: name, price, currency, availability status, and product URL. 3. Saves the collected data into a timestamped CSV file prices_YYYY-MM-DD.csv. 4. Handles timeouts, missing fields, and unexpected page layouts gracefully. 5. Runs daily and compares prices to a previous run's CSV; produces a summary highlighting the average price per category and flagging price drops >20%.",
    "tags": [
      "python",
      "requests",
      "scrapy",
      "csv",
      "scraping"
    ]
  },
  {
    "id": "6.3",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "IT Job Listings Aggregator",
    "description": "Write a Python design a Python tool using scrapy that: 1. Crawls multiple job portal sites for IT sector job listings. 2. For each listing, extracts: job title, required skills, location, salary range if available, and application link. 3. Stores the information in a structured JSON file, it_jobs.json. 4. Gracefully skips expired postings and avoids duplicates by checking unique application links. 5. Includes retry mechanisms and a progress bar to monitor crawling status. 6. Generates a statistics summary showing the number of roles per location and the frequency of required skills.",
    "tags": [
      "python",
      "scrapy",
      "json",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "6.4",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Technology Blog Monitor",
    "description": "Write a Python build a Python crawler using feedparser and requests that: 1. Scans RSS feeds or blog sitemaps for posts mentioning specific technologies, \"Python\", \"Django\". 2. For each matching article, extracts: title, publication date, snippet text, and article link. 3. Saves the results into a tech_posts.json file. 4. Respects robots.txt and applies a polite delay between requests. 5. Logs both successful and failed fetch attempts. 6. Generates a report highlighting the number of articles found per keyword and the total number of feeds/sites visited.",
    "tags": [
      "python",
      "requests",
      "json",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "6.5",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Unified Grocery List Generator",
    "description": "Write a Python scraper using beautifulsoup4 that: 1. Visits recipe websites and collects ingredient lists from multiple recipe pages. 2. Parses and normalizes ingredient names, \"tbsp\" -> \"tablespoon\". 3. Merges duplicate items and counts their frequency across recipes. 4. Saves the combined, deduplicated grocery list to a grocery_list.csv file. 5. Implements robust error handling for unreachable pages and missing fields. 6. Generates a summary highlighting the top ten most common ingredients.",
    "tags": [
      "python",
      "beautifulsoup",
      "csv",
      "web",
      "scraping"
    ]
  },
  {
    "id": "6.6",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Multi-City Weather Forecast Aggregator",
    "description": "Write a Python develop a Python scraper using requests and pandas that: 1. Scrapes weather websites for daily forecasts of multiple predefined cities. 2. Extracts: temperature, humidity, wind speed, and condition description for each city. 3. Stores the data in a structured JSON file, weather_forecast.json. 4. Handles inaccessible pages and inconsistent data formats. 5. Includes an option to generate line charts of temperature trends over a week. 6. Produces a summary showing the hottest city, coldest city, and average humidity.",
    "tags": [
      "python",
      "pandas",
      "requests",
      "json",
      "web"
    ]
  },
  {
    "id": "6.7",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Online Course Catalog Scraper",
    "description": "Write a Python script using scrapy that: 1. Crawls online learning platforms to collect information on available courses. 2. Extracts: course title, instructor, duration, and enrollment numbers. 3. Saves the details into a courses.csv file. 4. Implements retry logic for unstable requests and skips duplicate courses. 5. Provides an optional subject flag to filter courses by category, \"Data Science\". 6. Displays a summary of the top3 most popular courses by enrollment and the total courses found.",
    "tags": [
      "python",
      "requests",
      "scrapy",
      "csv",
      "scraping"
    ]
  },
  {
    "id": "6.8",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Podcast Directory Scraper",
    "description": "Write a Python create a Python crawler using requests and beautifulsoup4 that: 1. Collects metadata from a podcast directory website. 2. For each podcast, extracts: title, description, host, episode count, and RSS feed URL. 3. Saves the results into podcasts.json. 4. Handles missing fields gracefully and retries failed requests. 5. Provides an option to generate a popular_podcasts.csv file filtered for shows with >100 episodes. 6. Prints a summary: number of podcasts collected, most common categories, average episode count.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "csv",
      "json"
    ]
  },
  {
    "id": "6.9",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Real Estate Listing Aggregator",
    "description": "Write a Python build a Python scraper using scrapy and openpyxl that: 1. Scans real estate websites for property listings, rentals and sales. 2. Extracts: price, size, number of rooms, location, and listing URL. 3. Saves the data into an Excel file with separate sheets for Rentals and Sales. 4. Handles missing values and skips duplicate listings by URL. 5. Provides an optional filter flag to limit results by city or price range. 6. Generates a summary showing average rental price, highest sale price, and total listings.",
    "tags": [
      "python",
      "scrapy",
      "web",
      "scraping",
      "file processing"
    ]
  },
  {
    "id": "6.10",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Research Paper Indexer",
    "description": "Write a Python develop a Python crawler using requests and scholarly that: 1. Indexes online research articles from academic databases or journal sites. 2. Extracts: title, authors, abstract, publication date, and DOI for each article. 3. Saves the collected data into research_papers.json. 4. Implements retry logic and handles missing fields. 5. Provides a filter flag to limit crawling to articles containing specific keywords. 6. Generates a summary: total articles found, top keywords, and most frequent journals.",
    "tags": [
      "python",
      "requests",
      "json",
      "sql",
      "scraping"
    ]
  },
  {
    "id": "6.11",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Flight Deal Finder",
    "description": "Write a Python create a Python crawler using requests and beautifulsoup4 that: 1. Scrapes airline websites for available flight deals. 2. Extracts: flight number, departure/arrival times, price, and route. 3. Saves results into flight_deals.json. 4. Handles invalid data and retries failed requests. 5. Provides an optional route flag to filter results, \"NYC- LON\". 6. Generates a summary showing the cheapest flight, the most expensive route, number of deals.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "json",
      "web"
    ]
  },
  {
    "id": "6.12",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Social Media Trend Tracker",
    "description": "Write a Python crawler using tweepy or requests that: 1. Tracks trending hashtags on a social media platform. 2. For each hashtag, records: number of mentions, related post count, top users. 3. Saves results into trending_hashtags.json. 4. Handles API rate limits and retries failed requests. 5. Provides an option to filter hashtags by topic, \"technology\". 6. Generates a summary: top5 hashtags, most engaging user, average mentions.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "scraping"
    ]
  },
  {
    "id": "6.13",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "App Store Scraper",
    "description": "Write a Python develop a Python crawler using scrapy that: 1. Collects app details from the mobile app stores, Google Play, and Apple App Store. 2. Extracts: app name, developer, rating, download count, and category. 3. Saves results into an Excel file, with sheets for each app category. 4. Handles missing or inconsistent fields. 5. Provides an optional flag to filter apps by min rating or min downloads. 6. Generates a summary: highest-rated app, most downloaded app, category with most apps.",
    "tags": [
      "python",
      "scrapy",
      "scraping",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "6.14",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "News Article Aggregator",
    "description": "Write a Python scraper using newspaper3k that: 1. Gathers news articles from a list of online publishers. 2. Extracts: headline, author, publication date, and article link. 3. Saves the collected data into news_articles.json. 4. Provides error handling for broken links and missing fields. 5. Adds a keyword flag to filter articles by topic, \"AI\". 6. Displays a summary: total articles, most frequent topic, top publisher by volume.",
    "tags": [
      "python",
      "json",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "6.15",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Technology Forum Scraper",
    "description": "Write a Python build a Python crawler using scrapy that: 1. Scrapes technology forums for discussion threads. 2. Extracts: thread title, author, timestamp, and reply count. 3. Saves the results into forum_discussions.csv. 4. Provides error handling for inaccessible threads. 5. Includes an optional top flag to highlight threads with the most replies. 6. Generates a summary: most active thread, most active forum, total threads processed.",
    "tags": [
      "python",
      "scrapy",
      "csv",
      "scraping",
      "logging"
    ]
  },
  {
    "id": "6.16",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Academic Conference Aggregator",
    "description": "Write a Python develop a Python crawler using requests and beautifulsoup4 that: 1. Collects academic conference information from event websites. 2. Extracts: conference title, location, dates, and keynote speakers. 3. Saves results into conferences.json. 4. Handles missing or inconsistent data. 5. Provides an option to generate a conferences.csv filtered by region or subject. 6. Displays a summary: number of conferences, common topics, city hosting the most events.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "csv",
      "json"
    ]
  },
  {
    "id": "6.17",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Online Bookstore Scraper",
    "description": "Write a Python create a Python scraper using scrapy and openpyxl that: 1. Gathers book details from online bookstore catalogs. 2. Extracts: title, author, price, availability, and ISBN. 3. Saves results into an Excel file, with sheets organized by genre. 4. Handles missing fields and retries failed requests. 5. Provides an optional max-price filter. 6. Generates a summary: cheapest book, most expensive book, average price.",
    "tags": [
      "python",
      "requests",
      "scrapy",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "6.18",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Video Game Release Scraper",
    "description": "Write a Python build a Python crawler using scrapy that: 1. Scrapes gaming platforms for upcoming game releases. 2. Extracts: game title, release date, developer, and genres. 3. Saves results into game_releases.csv. 4. Provides retry logic for unstable connections. 5. Adds an optional flag to filter by release year or genre. 6. Displays a summary: top3 anticipated games, genre with most releases, and total games.",
    "tags": [
      "python",
      "scrapy",
      "csv",
      "scraping",
      "logging"
    ]
  },
  {
    "id": "6.19",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Art Gallery Cataloguer",
    "description": "Write a Python develop a Python scraper using requests and beautifulsoup4 that: 1. Scrapes online art galleries for artwork information. 2. Extracts: artwork title, artist, year created, and price if available. 3. Saves results into an Excel file artwork.xlsx. 4. Handles duplicate records and failed requests. 5. Provides an optional filter for a specific artist or style. 6. Generates a summary: most prolific artist, highest-priced artwork, total artworks.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "6.20",
    "category": "Web Crawling & Scraping",
    "subcategory": "Crawlers & Aggregators",
    "title": "Online Auction Monitor",
    "description": "Write a Python create a Python scraper using requests and pandas that: 1. Monitors online auction websites for ongoing auctions. 2. Extracts: item name, current bid, seller name, and auction end time. 3. Saves the collected data into active_auctions.json. 4. Handles missing fields and retries failed requests. 5. Provides an optional ending-soon flag to highlight auctions ending in <24 hours. 6. Generates a summary: item with the highest bid, number of auctions ending soon, and total auctions. Downloaders & Checkers",
    "tags": [
      "python",
      "pandas",
      "requests",
      "json",
      "web"
    ]
  },
  {
    "id": "6.21",
    "category": "Web Crawling & Scraping",
    "subcategory": "Downloaders & Checkers",
    "title": "Academic Faculty Contact Scraper",
    "description": "Write a Python create a Python crawler using requests and beautifulsoup4 that: 1. Visits a university website's department pages to extract faculty contact information. 2. Collects name, department, and email address for each faculty member. 3. Removes duplicates, validates email formats, and stores clean data in contacts.csv. 4. Logs the number of new emails found since the last crawl by comparing to the existing CSV. 5. Implements retry logic for unstable connections and excludes links to external domains. 6. Outputs a summary: total pages visited, unique contacts found, and success rate of page processing.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "csv",
      "web"
    ]
  },
  {
    "id": "6.22",
    "category": "Web Crawling & Scraping",
    "subcategory": "Downloaders & Checkers",
    "title": "Image Gallery Downloader",
    "description": "Write a Python develop a Python crawler using requests and beautifulsoup4 that: 1. Downloads images from a photography gallery website. 2. Uses the image's alt text or the page title to generate a clean filename. 3. Maintains a JSON log downloaded_images.json mapping original URLs to local file paths. 4. Avoids duplicates by checking the log and file system. 5. Handles broken links and skips non-image content types. 6. Produces a summary of total images downloaded, skipped duplicates, and pages visited.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "json",
      "web"
    ]
  },
  {
    "id": "6.23",
    "category": "Web Crawling & Scraping",
    "subcategory": "Downloaders & Checkers",
    "title": "Broken Link Checker",
    "description": "Write a Python crawler using requests and scrapy that: 1. Starts at the homepage and recursively follows all internal links. 2. Tests each URL's HTTP response code, flagging any4xx or5xx errors. 3. Stores broken link details URL, status code, and parent page URL in a broken_links.csv file. 4. Logs the total crawl duration, number of links tested, and the percentage of broken links. 5. Helps administrators improve site reliability.",
    "tags": [
      "python",
      "requests",
      "scrapy",
      "csv",
      "scraping"
    ]
  },
  {
    "id": "6.24",
    "category": "Web Crawling & Scraping",
    "subcategory": "Downloaders & Checkers",
    "title": "Government PDF Report Archiver",
    "description": "Write a Python create a Python automation script using requests and beautifulsoup4 that: 1. Crawls a government open-data portal to find and download PDF reports. 2. For each PDF, record metadata: title, file size, and URL in a pdf_metadata.csv file. 3. Avoids duplicate downloads by checking the metadata log. 4. Retries failed downloads and skips non-PDF links. 5. Saves all PDFs to a local reports_archive/ directory. 6. Generates a summary: total PDFs collected, total size MB, and range of publication dates.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "csv",
      "scraping"
    ]
  },
  {
    "id": "7.1",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "YAML Configuration Loader & Validator",
    "description": "Write a Python script using PyYAML that: 1. Loads application settings from a specified YAML configuration file. 2. Validates that all keys in a predefined REQUIRED_KEYS list exist in the config. 3. If any required key is missing, it logs a clear error and stops execution gracefully. 4. Provides an option generate-template to create a default YAML template with all required keys commented out. 5. Handles YAMLError exceptions for malformed files with user-friendly messages. 6. Produces a summary showing: total parameters loaded, missing keys found, and overall validation status.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.2",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Multi-Environment YAML Comparator",
    "description": "Write a Python utility using deepdiff and PyYAML that: 1. Compares multiple YAML config files, config_dev.yaml, config_prod.yaml. 2. Validates that all files contain the same top-level keys. 3. Highlights value mismatches for shared keys. 4. Provides an option report to generate a detailed Markdown report of the differences. 5. Handles FileNotFoundError for missing files gracefully. 6. Generates a summary showing: total keys compared, mismatches found, and overall environment alignment status.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.3",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "CI/CD Pipeline YAML Validator",
    "description": "Write a Python tool using PyYAML that: 1. Parses a YAML file defining a CI/CD pipeline. 2. Validates that the required stages build, test, and deploy exist and contain a command list. 3. Logs errors for missing stages or empty command lists. 4. Provides an option fix to auto-fix common mistakes, like setting empty environment variables to. 5. Handles corrupted YAML files. 6. Generates a summary showing: stages validated, fixes applied, and pipeline validation status.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging",
      "testing"
    ]
  },
  {
    "id": "7.4",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Server Role YAML Validator",
    "description": "Write a Python tool using PyYAML and ipaddress that: 1. Parses a YAML file defining server roles in an infrastructure. 2. Validates that each server entry has: hostname string, role from an allowed list,, web, db, ip valid IP address. 3. Provides an option-export-valid to export a corrected YAML file with invalid entries removed. 4. Handles missing files gracefully. 5. Generates a summary showing: servers validated, invalid entries removed, and validation status.",
    "tags": [
      "python",
      "web",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.5",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Monitoring Tool YAML Validator",
    "description": "Write a Python program using PyYAML that: 1. Parses YAML configuration files for a monitoring tool. 2. Validates that alert rules have: threshold number, notification_channels list, escalation_policy string. 3. Provides an option apply-default-thresholds to auto- correct missing thresholds with default values. 4. Handles invalid YAML syntax. 5. Generates a summary showing: rules validated, corrections applied, and validation status.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "7.6",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Docker Compose YAML Validator",
    "description": "Write a Python program using PyYAML and requests for a registry check that: 1. Validates a docker-compose.yaml file. 2. Ensures required service fields exist: image, ports, volumes. 3. Provides an option set-latest-tag to auto-correct images missing a tag by appending: latest. 4. Handles malformed YAML. 5. Generates a summary showing: services validated, corrections applied, and validation result.",
    "tags": [
      "python",
      "requests",
      "docker",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.7",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "CI Job Runner YAML Validator",
    "description": "Write a Python program using PyYAML that: 1. Validates YAML configurations for a CI job runner. 2. Ensures each job has: name string, script string or list, tags list. 3. Validates that tags correspond to tags known by the runner system. 4. Provides an option assign-default-tag to assign a default tag if missing. 5. Handles malformed YAML. 6. Generates a summary showing: jobs validated, corrections applied, and validation result.",
    "tags": [
      "python",
      "machine learning"
    ]
  },
  {
    "id": "7.8",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Data Pipeline YAML Validator",
    "description": "Write a Python program using PyYAML that: 1. Validates YAML files defining data pipelines. 2. Ensures stages extract, transform, and load exist and their dependencies are correctly listed. 3. Provides an option add-placeholders to insert placeholder stages for missing required stages. 4. Handles malformed YAML. 5. Generates a summary showing: pipelines validated, corrections applied, and validation status. INI Tools",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.9",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "INI Credentials Manager",
    "description": "Write a Python develop a Python program using configparser that: 1. Reads credentials and environment variables from an INI configuration file. 2. Validates that sensitive fields, username, and password in the credentials section are not empty. 3. Validates that environment names in an environments section are from an allowed list: dev, prod. 4. Provides an option encrypt to encrypt sensitive values using cryptography before saving them back to the file. 5. Prints clear error messages suggesting corrections if validation fails. 6. Generates a summary showing: total keys validated, number of values encrypted, and overall config readiness.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.10",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Web App INI Validator",
    "description": "Write a Python develop a Python program using configparser that: 1. Validates an INI configuration file for a web application. 2. Ensures required sections, server, database, and logging exist. 3. Validates that the required keys, host, port in the server, and log-level in logging are present and valid. 4. Prints warnings for unexpected keys not in an ALLOWED_KEYS list per section. 5. Provides an option clean to generate a new INI file with only valid, expected fields. 6. Generates a summary showing: sections validated, keys verified, and pass/fail status.",
    "tags": [
      "python",
      "sql",
      "web",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.11",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Scheduled Job INI Validator",
    "description": "Write a Python develop a Python script using configparser and croniter that: 1. Reads an INI file where each section defines a scheduled job. 2. Validates that each job section has: name, schedule valid cron syntax, and script_path existing file. 3. Logs invalid jobs to invalid_jobs.ini. 4. Provides an option disable-invalid to comment out invalid job sections in the original file. 5. Handles missing fields and invalid cron syntax gracefully. 6. Generates a summary showing: jobs validated, jobs disabled, and scheduler readiness.",
    "tags": [
      "python",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.12",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Multi-User INI Comparator & Merger",
    "description": "Write a Python develop a Python program using configparser that: 1. Compares INI configuration files for multiple users, user1.ini, user2.ini. 2. Identifies inconsistencies: missing sections, extra sections, differing values. 3. Provides an option merge to merge consistent sections into a master.ini file, flagging conflicts in comments. 4. Handles corrupted files by skipping them and logging an error. 5. Generates a summary showing: files processed, inconsistencies found, and master file creation success.",
    "tags": [
      "python",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.13",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Email Client INI Validator",
    "description": "Write a Python develop a Python program using configparser that: 1. Validates INI files for email client configurations. 2. Checks a SMTP section for required fields: server, port, username, encryption. 3. Validates that encryption is either TLS or SSL. 4. Provides an option fix-ports to auto-correct missing ports587 for TLS,465 for SSL. 5. Handles malformed INI files. 6. Generates a summary showing: clients validated, corrections made, and overall validation success.",
    "tags": [
      "python",
      "machine learning",
      "cli",
      "email",
      "file processing"
    ]
  },
  {
    "id": "7.14",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Desktop App INI Validator",
    "description": "Write a Python develop a Python program using configparser that: 1. Validates INI configuration files for a desktop application. 2. Ensures common sections, general, ui, and network exist with required keys. 3. Provides an option to suggest corrections for unknown or deprecated keys based on a known schema. 4. Handles corrupted INI files. 5. Generates a summary showing: sections validated, invalid keys detected, and pass/fail status.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.15",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Virtual Environment INI Validator",
    "description": "Write a Python develop a Python script using configparser that: 1. Validates INI files used to define Python virtual environments. 2. Ensures a virtualenv section exists with keys: path string, python_version string,3.9, dependencies list. 3. Provides an option suggest-path to suggest a default path if missing. 4. Handles malformed INI files. 5. Generates a summary showing: environments validated, corrections applied, and validation status.",
    "tags": [
      "python",
      "file processing"
    ]
  },
  {
    "id": "7.16",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Proxy Settings INI Validator",
    "description": "Write a Python develop a Python script using configparser that: 1. Validates INI files for proxy configurations. 2. Ensures a proxy section exists with keys: host, port number,1-65535, username, password. 3. Provides an option fix-port to set an invalid port to a safe default,8080. 4. Handles malformed INI files. 5. Generates a summary showing: proxies validated, corrections applied, and validation success.",
    "tags": [
      "python",
      "file processing"
    ]
  },
  {
    "id": "7.17",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Chat App INI Validator",
    "description": "Write a Python develop a Python script using configparser that: 1. Validates INI files for chat application configurations. 2. Ensures a connection section with: server_address, port, username, encryption_method from allowed list:, TLS, AES. 3. Provides an option set-default-encryption to replace an invalid encryption method with TLS. 4. Handles malformed INI files. 5. Generates a summary showing: applications validated, corrections applied, and config validity. JSON Tools",
    "tags": [
      "python",
      "json",
      "file processing"
    ]
  },
  {
    "id": "7.18",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "JSON Task Schema Validator",
    "description": "Write a Python create a Python script using jsonschema that: 1. Parses a JSON file containing an array of project tasks. 2. Validates each task object against a schema requiring fields: title string, deadline string, ISO date, priority string, enum: low, medium, high. 3. Logs invalid entries to a separate validation_errors.json file. 4. Provides an option apply-defaults to assign defaults for missing non-required fields, status: \"pending\". 5. Outputs a cleaned and validated JSON file. 6. Generates a summary showing: tasks processed, tasks corrected, and percentage successfully validated.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.19",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "API Client JSON Config Validator",
    "description": "Write a Python create a Python script using jsonschema and requests that: 1. Parses a JSON configuration file containing API client settings. 2. Validates that each client config has: a valid URL, a headers object, and a non-empty token. 3. Logs errors for invalid tokens or missing headers to api_errors.log. 4. Provides an option refresh-tokens to automatically refresh expired tokens via a predefined API endpoint. 5. Handles JSONDecodeError for malformed files. 6. Generates a summary showing: API configs processed, configs auto-corrected, and overall validation pass rate.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "7.20",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "User Preferences JSON Validator",
    "description": "Write a Python create a Python tool using jsonschema that: 1. Parses a JSON file containing user application preferences. 2. Validates fields: theme enum: light, dark, font_size number, min:8, max:72, language string. 3. Replaces invalid values with defaults: theme: light, font_size:12, language: en and logs the changes. 4. Provides an option-export-corrected to save the corrected preferences to a new JSON file. 5. Handles missing files by creating a default preferences file. 6. Generates a summary showing: preferences validated, defaults applied, and validation status.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.21",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "ML Experiment JSON Validator",
    "description": "Write a Python create a Python script using jsonschema that: 1. Validates JSON configuration files for machine learning experiments. 2. Ensures required fields exist: dataset_path string, model_type string, hyperparameters object, output_dir string. 3. Validates that hyperparameter values are within specified ranges, learning_rate > 0. 4. Provides an option apply-defaults to generate corrected JSON files with valid default hyperparameters. 5. Handles malformed JSON gracefully. 6. Generates a summary showing: experiments validated, corrections applied, and validation pass rate.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.22",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "IoT Device JSON Validator",
    "description": "Write a Python create a Python script using jsonschema and semantic_version that: 1. Loads JSON configurations for IoT devices. 2. Validates required fields: device_id string, firmware_version semantic version string, connection_params object. 3. Provides an option-update-firmware to auto-update invalid firmware_version to the latest known version. 4. Handles malformed JSON and invalid version strings. 5. Generates a summary showing: devices validated, configurations corrected, and deployment readiness.",
    "tags": [
      "python",
      "json",
      "testing"
    ]
  },
  {
    "id": "7.23",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Web App JSON Data Validator",
    "description": "Write a Python create a Python script using jsonschema that: 1. Validates a JSON file containing data entries for a web app. 2. Ensures each entry has fields: title string, maxlen:100, description string, maxlen:500, author string. 3. Provides an option truncate to automatically truncate overly long string values to their max length. 4. Handles malformed JSON. 5. Generates a summary showing: entries validated, corrections applied, and validation success rate.",
    "tags": [
      "python",
      "json",
      "web",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.24",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "API Definition JSON Validator",
    "description": "Write a Python create a Python tool using jsonschema that: 1. Parses a JSON file defining API endpoints. 2. Validates that each endpoint has: a URL string, a method enum: GET, POST, PUT, DELETE, a parameters array. 3. Provides an option method to auto-correct invalid HTTP methods to GET. 4. Handles corrupted JSON files. 5. Generates a summary showing: endpoints validated, corrections applied, and validation outcome.",
    "tags": [
      "python",
      "api",
      "json",
      "file processing"
    ]
  },
  {
    "id": "7.25",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "E-commerce Product JSON Validator",
    "description": "Write a Python create a Python utility using jsonschema that: 1. Validates a JSON file containing e-commerce product data. 2. Ensures each product has: name string, price number, min:0, stock integer, min:0, category string from allowed list. 3. Provides an option assign-uncategorized to auto-correct missing categories to \"Uncategorized\". 4. Handles malformed JSON. 5. Generates a summary showing: products validated, corrections applied, and dataset validity.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.26",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Web Server JSON Validator",
    "description": "Write a Python create a Python tool using jsonschema and pathlib that: 1. Validates JSON configuration files for web servers. 2. Ensures required fields: server_name, host, ssl object. 3. Validates that ssl.cert_path and ssl.key_path point to existing files if SSL is enabled. 4. Provides an option disable-ssl to auto-correct invalid SSL configs by disabling SSL and logging a warning. 5. Handles malformed JSON. 6. Generates a summary showing: servers validated, corrections applied, and overall validity.",
    "tags": [
      "python",
      "json",
      "web",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.27",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Employee Record JSON Validator",
    "description": "Write a Python create a Python script using jsonschema and email-validator that: 1. Validates a JSON file containing employee records. 2. Ensures each record has: name string, email valid format, department string, salary number, min:0. 3. Provides an option-assign-department to auto-correct missing departments to \"General\". 4. Handles malformed JSON and invalid email formats. 5. Generates a summary showing: employees validated, corrections applied, and dataset validation outcome. Metadata, Config, & Validators",
    "tags": [
      "python",
      "json",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "7.28",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Microservice Dependency Validator",
    "description": "Write a Python program using PyYAML and networkx that: 1. Loads a YAML config file defining microservices and their dependencies. 2. Validates that each service has: a name string, a port number, and a depends_on list. 3. Checks for circular dependencies using a graph and logs them as critical errors. 4. Provides an option to visualize and export a dependency graph as a PNG image using matplotlib. 5. Generates a summary showing: services validated, circular dependencies found, and deployment readiness.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.29",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Pattern-Based File Metadata Scraper",
    "description": "Write a Python design a Python tool using glob and os that: 1. Recursively scans a directory for files matching a user- provided pattern, *.log, *.cfg. 2. For each matching file, collect metadata: filename, size_bytes, last_modified, extension. 3. Saves the metadata to a CSV file, file_metadata.csv. 4. Handles inaccessible directories gracefully by logging the error. 5. Generates a final summary showing: total_files_found, largest_file_size, most_common_extension.",
    "tags": [
      "python",
      "csv",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.30",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "YAML Task Executor",
    "description": "Write a Python utility using PyYAML that: 1. Reads a YAML configuration file containing an operation add, subtract, multiply, divide and an argument list of numbers. 2. Validates that the required fields operation, arguments are present. 3. Executes the mathematical operation and writes the result to an output YAML file, including metadata timestamp, operation. 4. Handles invalid operations and missing keys with clear error messages. 5. Generates a summary report showing: operation_performed, arguments_used, result.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.31",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Multi-Format Config Validator",
    "description": "Write a Python design a Python automation script using configparser and pyyaml that: 1. Automatically detects and validates configuration files.ini, .yaml in a directory. 2. Checks for the presence of required keys defined per file type or in a schema. 3. Logs validation results pass/fail to a JSON file validation_results.json, including error messages for failures. 4. Prints a summary table to the console showing: files_processed, passed, and failed.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "7.32",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Metadata & EXIF Extractor",
    "description": "Write a Python automation script using os and PIL for images that: 1. Extracts filesystem metadata name, size, and dates for all files in a directory. 2. For image files JPEG, PNG, additionally extracts EXIF data, GPS, and camera model. 3. Saves all metadata to a timestamped JSON file metadata_<timestamp>.json. 4. Generates a summary report showing: files_processed, files_skipped, images_with_exif.",
    "tags": [
      "python",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "7.33",
    "category": "Config, Metadata & File Security",
    "subcategory": "Metadata, Config, & Validators",
    "title": "Markdown to Multi-Format Exporter",
    "description": "Write a Python create a Python automation tool using markdown and pandoc via subprocess that: 1. Converts Markdown.md files to PDF, DOCX, and HTML formats. 2. Preserves formatting like headings, lists, and code blocks. 3. Processes entire directories, saving outputs to an exports/ subfolder. 4. Handles unsupported syntax gracefully. 5. Generates a summary showing: files_converted, formats_generated, export_directory. Security Tools",
    "tags": [
      "python",
      "automation",
      "machine learning",
      "file processing",
      "security"
    ]
  },
  {
    "id": "7.34",
    "category": "Config, Metadata & File Security",
    "subcategory": "Security Tools",
    "title": "Secure File Deletion Tool",
    "description": "Write a Python create a Python tool using os and random that: 1. Securely deletes a file by overwriting it with random data multiple times, user-configurable passes, renaming it, then deleting it. 2. Includes a dry run mode that only reports what would be deleted without taking action. 3. Generates a log file deletion_log.json showing for each file: filename, overwrite_passes, deletion_status. 4. Provides a summary of files deleted, skipped, or failed.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "7.35",
    "category": "Config, Metadata & File Security",
    "subcategory": "Security Tools",
    "title": "Recursive Secure Directory Wipe",
    "description": "Write a Python script using shutil and os that: 1. Recursively traverses a directory and its subdirectories. 2. Securely deletes every file using multiple overwrite passes and then removes empty directories. 3. Requires explicit user confirmation via a recursive flag to prevent accidental use. 4. Logs every deleted file and directory to wipe_log.json. 5. Handles permission errors gracefully by skipping and logging. 6. Generates a summary showing: files_deleted, directories_removed, errors_encountered.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging",
      "security"
    ]
  },
  {
    "id": "8.1",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Geocoding & Location",
    "title": "Address to Coordinates Converter",
    "description": "Write a Python script using geopy that: 1. Reads a CSV file containing a list of customer addresses. 2. Uses a geocoding API, Nominatim, and Google Geocoding to convert each address to latitude and longitude. 3. Saves the original data plus the coordinates into a new CSV file. 4. Handles invalid or incomplete addresses by skipping them and logging the error. 5. Provides an option to retry failed lookups with a different geocoding provider. 6. Generates a summary showing: total addresses processed, successful conversions, and failure percentage.",
    "tags": [
      "python",
      "api",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.2",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Geocoding & Location",
    "title": "Reverse Geocoding API Client",
    "description": "Write a Python program using requests that: 1. Reads a JSON file containing latitude and longitude coordinates. 2. Uses a reverse geocoding API to convert each coordinate pair into a structured address. 3. Appends the address data, street, city, and country to the original JSON structure. 4. Handles API rate limiting with retries and exponential backoff. 5. Generates a summary showing: coordinates processed, successful conversions, and the most common city.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "8.3",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Geocoding & Location",
    "title": "Business Listings Geocoder",
    "description": "Write a Python create a Python tool using geopy and openpyxl that: 1. Reads an Excel file containing business names and addresses. 2. Geocodes each address to obtain latitude and longitude. 3. Appends the coordinates as new columns in the same Excel file. 4. Handles failed geocoding attempts and allows retries with a fallback provider. 5. Generates a summary showing: businesses processed, successful geocodes, and retry rate.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.4",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Geocoding & Location",
    "title": "IP to Geolocation Enricher",
    "description": "Write a Python develop a Python utility using a geolocation API, ipapi, that: 1. Reads a CSV dataset containing IP addresses. 2. Enriches each IP with geolocation data: city, region, and country code. 3. Saves the enriched dataset to a new CSV file. 4. Handles API failures and invalid IPs. 5. Generates a summary showing: IPs processed, successful lookups, and the most common country.",
    "tags": [
      "python",
      "api",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.5",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Geocoding & Location",
    "title": "Postal Code to Location",
    "description": "Write a Python create a Python tool using a postal code API, GeoNames, that: 1. Reads a dataset containing postal codes. 2. Uses an API to convert each postal code to a latitude and longitude. 3. Appends the coordinates to the dataset and exports to CSV or JSON. 4. Handles invalid or non-existent postal codes. 5. Generates a summary showing: postal codes processed, successful conversions, and the most common country. Mapping & Visualization",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "8.6",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Photo GPS Map Plotter",
    "description": "Write a Python develop a Python program using PIL and folium that: 1. Processes a folder of JPEG images, extracting GPS coordinates from EXIF metadata. 2. Plots each geotagged photo's location on an interactive Leaflet map photo_map.html. 3. Provides options to filter images by the DateTimeOriginal EXIF tag. 4. Handles images without GPS data by skipping them. 5. Generates a summary showing: images processed, coordinates extracted, and map generation status.",
    "tags": [
      "python",
      "machine learning"
    ]
  },
  {
    "id": "8.7",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Wi-Fi Hotspot Density Heatmap",
    "description": "Write a Python create a Python utility using folium and pandas that: 1. Reads a dataset of Wi-Fi hotspots CSV/JSON containing latitude and longitude. 2. Generates an interactive heatmap visualization showing hotspot density. 3. Provides options to adjust heatmap radius, blur, and gradient colors. 4. Handles invalid coordinates by skipping them. 5. Generates a summary showing: hotspots processed, invalid entries skipped, and heatmap creation status.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "8.8",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Earthquake Data Mapper",
    "description": "Write a Python script using a seismic API, USGS and folium that: 1. Fetches recent earthquake data: magnitude, depth, and coordinates. 2. Plots each event on an interactive map with color-coded markers by magnitude and popups. 3. Provides options to filter by date range and minimum magnitude. 4. Handles API errors gracefully. 5. Generates a summary showing: earthquakes processed, average magnitude, and largest event.",
    "tags": [
      "python",
      "api"
    ]
  },
  {
    "id": "8.9",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Bike-Sharing Station Map",
    "description": "Write a Python develop a Python script using folium and pandas that: 1. Reads a dataset of bike-sharing stations' names, coordinates, and bike availability. 2. Plots stations on an interactive map with circle markers scaled by available bikes. 3. Provides options to filter stations by city or operator. 4. Handles missing data. 5. Generates a summary showing: stations processed, invalid entries skipped, and map status.",
    "tags": [
      "python",
      "pandas",
      "machine learning"
    ]
  },
  {
    "id": "8.10",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "GPS Speed/Altitude Chart",
    "description": "Write a Python program using matplotlib and pandas that: 1. Reads a CSV file of GPS logs, timestamps, speed, and altitude. 2. Plots a dual-axis line chart showing speed and altitude over time. 3. Provides an option to export the chart as a PNG image. 4. Handles malformed data. 5. Generates a summary showing: data points processed, average speed, and max altitude.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "8.11",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Hiking Trail Elevation Profiler",
    "description": "Write a Python program using gpxpy and matplotlib that: 1. Parses GPX files from hiking trails. 2. Extracts elevation data and distance traveled. 3. Plots an elevation profile over distance and exports it as an image or CSV. 4. Handles corrupted GPX files. 5. Generates a summary showing: trails processed, average elevation gain, and longest trail.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.12",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Bus Stops to GeoJSON",
    "description": "Write a Python develop a Python tool using geopandas that: 1. Converts a CSV of bus stops stop_id, name, lat, lon, routes into a GeoJSON file. 2. Preserves all attributes as properties. 3. Provides an option to group stops by route. 4. Handles invalid entries. 5. Generates a summary showing: stops processed, GeoJSON features created, and success status.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "file processing"
    ]
  },
  {
    "id": "8.13",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Mapping & Visualization",
    "title": "Traffic Incident Heatmap",
    "description": "Write a Python develop a Python script using folium and pandas that: 1. Reads a dataset of traffic incidents with coordinates and severity. 2. Generates a heatmap visualization, weighted by incident severity. 3. Provides options to filter incidents by date. 4. Handles invalid entries. 5. Generates a summary showing: incidents processed, categories found, and heatmap status. Routing & Distance",
    "tags": [
      "python",
      "pandas"
    ]
  },
  {
    "id": "8.14",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "Delivery Point Route Optimizer",
    "description": "Write a Python create a Python utility using networkx and osmnx that: 1. Reads a JSON file of delivery points, each with latitude and longitude. 2. Calculates the shortest possible route visiting all points, Traveling Salesperson Problem. 3. Exports the optimized route as a GeoJSON LineString. 4. Handles invalid coordinates by excluding them from the calculation. 5. Generates a summary showing: points processed, total route distance, and % improvement over original order.",
    "tags": [
      "python",
      "json",
      "file processing",
      "testing"
    ]
  },
  {
    "id": "8.15",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "Coordinate Distance Calculator",
    "description": "Write a Python script using the haversine formula that: 1. Reads a CSV file containing two sets of coordinates lat1, lon1, lat2, lon2. 2. Calculates the great-circle distance between each pair in both kilometers and miles. 3. Saves the results, including distances, to a new CSV file. 4. Handles invalid coordinate pairs, out-of-range values. 5. Provides a precision flag to round results to N decimal places. 6. Generates a summary showing: pairs processed, average distance, and maximum distance.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.16",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "Driving Route Calculator",
    "description": "Write a Python program using a routing API, OSRM, and Google Directions that: 1. Calculates driving routes between two addresses provided in a CSV file. 2. Retrieves route details: distance, duration, and step-by- step instructions. 3. Saves the results to a structured JSON file. 4. Handles invalid addresses and API errors. 5. Provides an option to request alternative routes. 6. Generates a summary showing: routes calculated, average distance, and longest route.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "8.17",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "Travel Time Matrix Generator",
    "description": "Write a Python create a Python script using a routing API that: 1. Reads a list of city pairs from a CSV file. 2. Fetches driving time and distance for each pair from the API. 3. Saves the results as a travel time matrix CSV. 4. Handles API rate limits with retries. 5. Generates a summary showing: city pairs processed, successful requests, and average travel time.",
    "tags": [
      "python",
      "requests",
      "api",
      "csv",
      "machine learning"
    ]
  },
  {
    "id": "8.18",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "Inter-City Distance Matrix",
    "description": "Write a Python create a Python script using haversine and pandas that: 1. Reads a JSON file of city names and their coordinates. 2. Calculates a distance matrix between every pair of cities. 3. Exports the matrix to a CSV file. 4. Handles invalid coordinates. 5. Provides a unit flag for km or miles. 6. Generates a summary showing: city pairs processed, maximum distance, and average distance.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "8.19",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "Shapefile to GeoJSON Converter",
    "description": "Write a Python develop a Python program using geopandas that: 1. Converts ESRI Shapefiles.shp into GeoJSON format. 2. Retains all original geometry and attribute data. 3. Provides an option simplify to simplify geometries, reduce vertex count for a smaller file size. 4. Handles corrupted or missing component files.dbf, .shx. 5. Generates a summary showing: features processed, conversion success rate, and file size reduction %.",
    "tags": [
      "python",
      "pandas",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.20",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "GPS Logs to GPX Converter",
    "description": "Write a Python create a Python tool using gpxpy and pandas that: 1. Reads GPS logs from a CSV file with columns: timestamp, latitude, longitude, elevation. 2. Converts the logs into one or more GPX files, one per day. 3. Handles missing values by interpolation or by skipping incomplete records. 4. Generates a summary showing: log entries processed, GPX files created, and data completeness %.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "8.21",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "GeoJSON Merger",
    "description": "Write a Python develop a Python script using geopandas that: 1. Merges multiple GeoJSON files into a single, unified GeoJSON dataset. 2. Preserves all geometries and normalizes attribute fields across files. 3. Provides an option resolve-overlaps to handle overlapping features, union, intersection. 4. Handles corrupted files by skipping them and logging an error. 5. Generates a summary showing: files merged, total features in output, and conflicts resolved.",
    "tags": [
      "python",
      "pandas",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "8.22",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Routing & Distance",
    "title": "GPX to CSV Track Converter",
    "description": "Write a Python develop a Python program using gpxpy and pandas that: 1. Parses GPX files containing GPS tracks. 2. Extracts trackpoints timestamp, latitude, longitude, elevation into a CSV file. 3. Provides an option to split tracks into segments based on time gaps >1 hour. 4. Handles corrupted XML in GPX files. 5. Generates a summary showing: trackpoints processed, segments created, and export success. Format Conversion",
    "tags": [
      "python",
      "pandas",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.23",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Format Conversion",
    "title": "GeoJSON Validator & Corrector",
    "description": "Write a Python create a Python utility using jsonschema and geopandas that: 1. Validates GeoJSON files against the official schema. 2. Checks for valid geometry types, coordinate bounds, and required properties. 3. Provides an option to auto-correct minor issues, add an empty properties object. 4. Handles malformed files gracefully. 5. Generates a summary showing: features validated, corrections applied, and validation outcome.",
    "tags": [
      "python",
      "pandas",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.24",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Format Conversion",
    "title": "CSV to KML Converter",
    "description": "Write a Python create a Python tool using simplekml that: 1. Converts a CSV file containing points, lat, lon, name, description into a KML file. 2. Retains all additional attributes as KML placemark descriptions. 3. Provides an option to group points into folders based on a category column. 4. Handles missing values. 5. Generates a summary showing: points processed, KML files generated, and success rate.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.25",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Format Conversion",
    "title": "DMS to Decimal Degrees",
    "description": "Write a Python program using pyproj or custom parsing that: 1. Reads a CSV file containing coordinates in Degrees- Minutes-Seconds DMS format. 2. Converts them to decimal degrees. 3. Saves the results to a new CSV file. 4. Handles invalid DMS formats gracefully. 5. Provides a precision flag for decimal places. 6. Generates a summary showing: coordinates converted, invalid entries skipped, and success rate.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.26",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Format Conversion",
    "title": "GeoJSON Census Data Merger",
    "description": "Write a Python develop a Python utility using geopandas that: 1. Enriches a GeoJSON file with population data from a census CSV. 2. Spatially joins GeoJSON features to census regions based on coordinates. 3. Handles features that do not match any census region. 4. Exports the enriched GeoJSON. 5. Generates a summary showing: features processed, successful matches, and match failure rate.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "8.27",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Format Conversion",
    "title": "KML to CSV Converter",
    "description": "Write a Python program using beautifulsoup4 or xml.etree that: 1. Parses KML files, extracting placemark names, descriptions, and coordinates. 2. Converts the data into a structured CSV file. 3. Handles malformed KML. 4. Provides batch processing for multiple files. 5. Generates a summary showing: placemarks processed, files converted, and success status. Analysis & Tracking",
    "tags": [
      "python",
      "beautifulsoup",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "8.28",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Analysis & Tracking",
    "title": "Multi-City Weather Forecast Fetcher",
    "description": "Write a Python script using requests and pandas that: 1. Integrates with a weather API, OpenWeatherMap, to fetch forecasts for a list of cities. 2. For each city, it retrieves its coordinates, temperature range, and other metrics. 3. Saves the results city, lat, lon, temp_min, temp_max to a CSV file. 4. Handles invalid city names and API errors gracefully. 5. Provides a units flag to choose between metric Â°C and imperial Â°F. 6. Generates a summary showing: cities processed, successful API calls, and average forecasted temp.",
    "tags": [
      "python",
      "pandas",
      "requests",
      "api",
      "csv"
    ]
  },
  {
    "id": "8.29",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Analysis & Tracking",
    "title": "Ride-Sharing Trip Analyzer",
    "description": "Write a Python develop a Python program using pandas and haversine that: 1. Reads a JSON dataset of ride-sharing trips with pickup and drop-off coordinates. 2. Calculates the trip distance for each ride using the Haversine formula. 3. Transforms the data into a tabular format and exports to CSV or Excel. 4. Handles missing or malformed coordinates. 5. Generates a summary showing: trips processed, average trip distance, and longest trip.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json"
    ]
  },
  {
    "id": "8.30",
    "category": "Geolocation & Data Transformation",
    "subcategory": "Analysis & Tracking",
    "title": "GPS Coordinate Clustering",
    "description": "Write a Python create a Python script using scikit-learn that: 1. Reads a CSV file containing latitude and longitude coordinates. 2. Applies K-Means clustering to group the points into a specified number of clusters. 3. Outputs a new CSV file with an additional cluster_id column. 4. Handles missing coordinates. 5. Generates a summary showing: points processed, clusters created, and average within-cluster distance.",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "9.1",
    "category": "End-to-End Automation Projects",
    "subcategory": "Business & Finance",
    "title": "Daily Stock Market Data Pipeline",
    "description": "Write a Python design an end-to-end Python automation system that: 1. Collects daily stock market data, open, close, volume for a list of tickers from a financial API, Alpha Vantage, Yahoo Finance. 2. Validates API responses and handles missing values by imputation or flagging. 3. Implements retry logic with exponential backoff for failed API requests. 4. Stores the cleaned data in a PostgreSQL database with a structured schema. 5. Generates summary reports using pandas and matplotlib with daily performance and trends. 6. Emails the PDF reports to a list of stakeholders using smtplib. 7. Generates a final summary showing: stocks processed, database rows updated, and email delivery status.",
    "tags": [
      "python",
      "pandas",
      "requests",
      "api",
      "sql"
    ]
  },
  {
    "id": "9.2",
    "category": "End-to-End Automation Projects",
    "subcategory": "Business & Finance",
    "title": "E-commerce Price Monitoring & Alert System",
    "description": "Write a Python create a Python-based automation project that: 1. Monitors product prices from multiple online stores by scraping product pages daily. 2. Stores historical price data in a SQLite or PostgreSQL database for trend analysis. 3. Compares current prices to historical averages and user- defined discount thresholds. 4. Sends SMS via Twilio or email alerts when a significant price drop is detected. 5. Handles website structure changes gracefully with adaptive parsing and retries. 6. Generates a summary showing: products tracked, price changes detected, and notifications sent.",
    "tags": [
      "python",
      "api",
      "sql",
      "web",
      "scraping"
    ]
  },
  {
    "id": "9.3",
    "category": "End-to-End Automation Projects",
    "subcategory": "Business & Finance",
    "title": "Small Business Expense Tracker",
    "description": "Write a Python project that: 1. Uses OCR Tesseract via pytesseract to extract text from scanned invoice and receipt images. 2. Categorizes expenses using keyword matching against a predefined list of categories. 3. Stores categorized expenses in a PostgreSQL database. 4. Generates monthly PDF reports ReportLab, summarizing spending by category. 5. Emails the report to the finance team. 6. Handles corrupted/unreadable files by moving them to a quarantine folder. 7. Generates a summary showing: invoices processed, expenses categorized, and report delivery success.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "9.4",
    "category": "End-to-End Automation Projects",
    "subcategory": "Business & Finance",
    "title": "Public Transportation Data Aggregator",
    "description": "Write a Python develop a Python automation pipeline that: 1. Scrapes bus routes, stops, and schedules from public transportation websites. 2. Stores the structured data in a PostgreSQL database with normalized tables. 3. Provides a REST API FastAPI/Flask for querying routes and arrival times. 4. Builds a simple frontend dashboard HTML, JavaScript showing real-time arrivals. 5. Handles incomplete or missing schedule data gracefully. 6. Generates a summary showing: routes collected, database entries added, and dashboard update status.",
    "tags": [
      "python",
      "api",
      "sql",
      "web",
      "scraping"
    ]
  },
  {
    "id": "9.5",
    "category": "End-to-End Automation Projects",
    "subcategory": "Business & Finance",
    "title": "Online Order Processing System",
    "description": "Write a Python create an end-to-end automation pipeline that: 1. Fetches new order data from a e-commerce platform's API, Shopify, WooCommerce. 2. Validates order fields' address, items and stores them in a PostgreSQL database. 3. Generates invoices as PDFs ReportLab for each order. 4. Emails invoices to customers and updates local stock levels in the database. 5. Handles failed payments by flagging orders and invalid entries by skipping. 6. Generates a summary showing: orders processed, invoices generated, and stock updates applied. News, Media & Social",
    "tags": [
      "python",
      "api",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.6",
    "category": "End-to-End Automation Projects",
    "subcategory": "News, Media & Social",
    "title": "Job Posting Aggregator & Reporter",
    "description": "Write a Python develop a Python automation pipeline that: 1. Scrapes job postings from multiple career websites, LinkedIn, and Indeed using scrapy or BeautifulSoup. 2. Stores the raw posting data in a MongoDB collection for flexibility. 3. Applies keyword-based filtering to identify and tag relevant opportunities. 4. Summarizes the filtered results into a structured PDF report, company, role, link using reportlab. 5. Automatically emails the report to a list of subscribed users. 6. Handles website changes with robust selectors and logs failed scrapes. 7. Generates a summary showing: postings collected, postings after filtering, and reports delivered.",
    "tags": [
      "python",
      "beautifulsoup",
      "scrapy",
      "web",
      "scraping"
    ]
  },
  {
    "id": "9.8",
    "category": "End-to-End Automation Projects",
    "subcategory": "News, Media & Social",
    "title": "Social Media Scheduler & Performance Tracker",
    "description": "Write a Python automation project that: 1. Reads scheduled social media posts platform, content, and time from a JSON configuration file. 2. Publishes posts to respective platforms, Twitter, Facebook, using their APIs. 3. Tracks engagement metrics like likes, shares for published posts at regular intervals. 4. Stores post details and metrics in a SQL database. 5. Generates performance reports in Excel pandas.to_excel. 6. Handles API rate limits with retries and invalid posts by skipping. 7. Generates a summary showing: posts published, engagements tracked, and reports generated. Monitoring & Reporting",
    "tags": [
      "python",
      "pandas",
      "api",
      "json",
      "sql"
    ]
  },
  {
    "id": "9.9",
    "category": "End-to-End Automation Projects",
    "subcategory": "Monitoring & Reporting",
    "title": "Server Health Monitoring & Alerting",
    "description": "Write a Python develop an automation project that: 1. Periodically collects server health metrics, CPU, memory, and disk from a network of machines using psutil and SSH. 2. Stores the time-series data in InfluxDB. 3. Generates and updates automated dashboards in Grafana for visualization. 4. Triggers alerts via email or Slack using webhooks when metric thresholds are exceeded. 5. Handles unreachable servers by logging errors and retrying. 6. Generates a summary showing: servers monitored, alerts triggered, and dashboard refresh status.",
    "tags": [
      "python",
      "web",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "9.10",
    "category": "End-to-End Automation Projects",
    "subcategory": "Monitoring & Reporting",
    "title": "Customer Feedback Analysis & Reporting",
    "description": "Write a Python create a Python automation pipeline that: 1. Collects customer feedback responses from an online form, Google Forms CSV export. 2. Stores raw responses in a SQL database. 3. Performs sentiment analysis on text responses using VADER or TextBlob. 4. Generates weekly Excel reports using openpyxl summarizing sentiment distribution and key themes. 5. Emails the reports to a distribution list of managers. 6. Handles invalid or incomplete responses by skipping or flagging. 7. Generates a summary showing: surveys collected, sentiment distribution, and report delivery status.",
    "tags": [
      "python",
      "csv",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.11",
    "category": "End-to-End Automation Projects",
    "subcategory": "Monitoring & Reporting",
    "title": "Website Uptime Monitor",
    "description": "Write a Python develop a Python end-to-end project that: 1. Periodically checks a list of URLs for uptime and records response times. 2. Stores the log data in a SQL database. 3. Triggers alerts via email or SMS if a site is down for more than a configured threshold. 4. Provides a dashboard HTML showing uptime percentages for each site. 5. Handles unreachable URLs by retrying and then logging as down. 6. Generates a summary showing: websites monitored, downtime events, and alerts triggered.",
    "tags": [
      "python",
      "sql",
      "web",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "9.12",
    "category": "End-to-End Automation Projects",
    "subcategory": "Monitoring & Reporting",
    "title": "Air Quality Data Processor",
    "description": "Write an automation pipeline that: 1. Collects raw air quality sensor data PM2.5, CO2 from CSV log files. 2. Cleans the data handling outliers, null values and stores it in a database. 3. Generates daily line charts using matplotlib showing pollution trends. 4. Emails the charts to relevant environmental agencies. 5. Handles corrupted sensor data gracefully. 6. Generates a summary showing: sensor readings processed, pollutants tracked, and charts distributed. Academic & Research",
    "tags": [
      "csv",
      "sql",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "9.13",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "Wearable Health Data Reporter",
    "description": "Write a Python develop a Python automation system that: 1. Collects health data steps, heart rate, and calories from the wearable device API exports JSON. 2. Validates and stores the data in a PostgreSQL database per user. 3. Generates personalized weekly PDF reports reportlab, for each user. 4. Automatically emails the reports to users. 5. Handles corrupted or malformed JSON logs. 6. Generates a summary showing: users processed, records stored, and reports delivered.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "9.14",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "Academic Paper Metadata Harvester",
    "description": "Write a Python develop a Python project that: 1. Scrapes metadata title, authors, abstract, keywords from academic publication sites, arXiv, IEEE. 2. Downloads the corresponding PDFs to an organized filesystem hierarchy /year/journal/. 3. Extracts additional metadata from PDFs where possible. 4. Stores all metadata in an SQLite database for indexing. 5. Provides a simple search API for Flask over the collected papers. 6. Handles broken links and missing metadata. 7. Generates a summary showing: papers processed, metadata entries, and API update status.",
    "tags": [
      "python",
      "api",
      "sql",
      "scraping",
      "file processing"
    ]
  },
  {
    "id": "9.15",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "Online Course Catalog Aggregator",
    "description": "Write an automation pipeline that: 1. Scrapes course metadata title, instructor, rating, and price from online learning platforms. 2. Stores the data in a SQL database and categorizes courses by subject. 3. Generates weekly PDF reports summarizing new and popular courses. 4. Emails the reports to a subscribed mailing list. 5. Handles duplicate entries and invalid data. 6. Generates a summary showing: courses collected, categories assigned, and reports distributed.",
    "tags": [
      "sql",
      "scraping",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "9.16",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "Academic Assignment Auto-Grader",
    "description": "Write an automation system that: 1. Collects assignment submissions code, documents from a learning management system LMS API or folder. 2. Applies plagiarism detection between submissions using difflib or external services. 3. Grades assignments automatically based on a rubric defined in a YAML file. 4. Stores grades and feedback in a database. 5. Emails results to students. 6. Handles corrupted submissions and invalid file formats. 7. Generates a summary showing: submissions processed, grades assigned, and plagiarism checks completed.",
    "tags": [
      "api",
      "sql",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "9.17",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "Academic Conference Tracker",
    "description": "Write a Python automation project that: 1. Monitors multiple academic conference websites for CFPs and event details. 2. Stores extracted data name, date, location, and deadline in a database. 3. Categorizes conferences by research field. 4. Generates monthly report PDFs listing upcoming conferences. 5. Emails the reports to a list of researchers. 6. Handles failed scrapes and duplicate entries. 7. Generates a summary showing: conferences collected, categories assigned, and reports distributed. Smart Systems & IoT",
    "tags": [
      "python",
      "sql",
      "web",
      "scraping",
      "automation"
    ]
  },
  {
    "id": "9.18",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "Weather Forecast Aggregation & Distribution",
    "description": "Write a Python develop an end-to-end automation system that: 1. Ingests weather forecast data for multiple cities from an API, OpenWeatherMap. 2. Enriches the data with geolocation details: city, country, latitude, and longitude. 3. Stores the forecasts in a SQL database. 4. Generates daily HTML dashboards Jinja2 templates showing forecast trends. 5. Emails the HTML dashboard to a list of subscribed users. 6. Handles API downtime by using the most recent cached data. 7. Generates a summary showing: cities processed, forecasts stored, and dashboards distributed.",
    "tags": [
      "python",
      "api",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.19",
    "category": "End-to-End Automation Projects",
    "subcategory": "Academic & Research",
    "title": "IoT Sensor Data Processing & Anomaly Detection",
    "description": "Write a Python create a Python automation pipeline that: 1. Reads IoT sensor data from CSV log files in a designated folder. 2. Cleans and validates the data, checking for outliers, missing values. 3. Stores the processed data in a time-series database, InfluxDB or SQLite. 4. Generates time-series visualizations using matplotlib of sensor readings. 5. Applies simple anomaly detection, Z-score to identify unusual behavior. 6. Sends alerts via email/SMS when anomalies are detected. 7. Handles corrupted log files by skipping and logging errors. 8. Generates a summary showing: sensor entries processed, anomalies detected, and alerts delivered.",
    "tags": [
      "python",
      "csv",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.20",
    "category": "End-to-End Automation Projects",
    "subcategory": "Smart Systems & IoT",
    "title": "Smart City Data Integration Platform",
    "description": "Write a Python create an end-to-end automation project that: 1. Collects real-time data from multiple city APIs: traffic flow, weather conditions, air quality. 2. Integrates and normalizes the data into a single time-series database. 3. Generates a real-time dashboard, Grafana or custom HTML visualizing all datasets. 4. Triggers alerts, SMS to city officials for abnormal conditions, high pollution, and traffic jams. 5. Handles delayed or missing API data gracefully using interpolation or last-known values. 6. Generates a summary showing: datasets integrated, alerts triggered, and dashboard update status. Enterprise Management",
    "tags": [
      "python",
      "api",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.21",
    "category": "End-to-End Automation Projects",
    "subcategory": "Enterprise Management",
    "title": "GitHub Repository Analytics",
    "description": "Write a Python automation system that: 1. Uses the GitHub API to fetch daily statistics, stars, forks, and open issues for a list of repositories. 2. Stores the historical data in a time-series database. 3. Generates dashboards HTML files with plotly charts showing project trends. 4. Emails weekly summary reports to a list of developers. 5. Handles API rate limits with authentication and retry logic. 6. Generates a summary showing: repositories monitored, data points collected, and reports delivered.",
    "tags": [
      "python",
      "api",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.22",
    "category": "End-to-End Automation Projects",
    "subcategory": "Enterprise Management",
    "title": "Automated Document Manager",
    "description": "Write a Python create a Python project that: 1. Monitors a designated folder for new files: PDF, image, text. 2. Classifies files by type using file extensions and libmagic. 3. Moves files to organized directory structures, ~/Documents/Invoices/2023/. 4. Extracts basic metadata, author, and creation date where possible. 5. Maintains a searchable index SQLite database of all files and their metadata. 6. Handles corrupted files by moving them to a _corrupted folder. 7. Generates a summary showing: files processed, classifications made, and index updated.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "9.23",
    "category": "End-to-End Automation Projects",
    "subcategory": "Enterprise Management",
    "title": "Employee Attendance Manager",
    "description": "Write a Python create a Python automation system that: 1. Collects attendance log files from a biometric device in CSV format. 2. Validates entries, checking for duplicates, invalid employee IDs and stores them in a database. 3. Generates daily Excel reports showing attendance percentages by department. 4. Emails the reports to department managers. 5. Handles missing logs or malformed data. 6. Generates a summary showing: employees processed, logs validated, and reports delivered.",
    "tags": [
      "python",
      "csv",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "9.24",
    "category": "End-to-End Automation Projects",
    "subcategory": "Enterprise Management",
    "title": "Event Registration & Ticketing System",
    "description": "Write a Python create a Python automation pipeline that: 1. Collects event registration data from a web form, via a POST endpoint or CSV export. 2. Validates entries and stores them in a database. 3. Generates a PDF tickets reportlab with unique QR codes for each registrant. 4. Emails the tickets to the attendees automatically. 5. Handles duplicate emails and invalid data. 6. Generates a summary showing registrations processed, tickets generated, and emails sent.",
    "tags": [
      "python",
      "csv",
      "sql",
      "web",
      "automation"
    ]
  },
  {
    "id": "9.25",
    "category": "End-to-End Automation Projects",
    "subcategory": "Enterprise Management",
    "title": "Medical Appointment Scheduler",
    "description": "Write a Python develop a Python end-to-end project that: 1. Collects patient appointment requests via a web form stored in a database. 2. Checks a doctor's availability schedule stored in another database table. 3. Automatically assigns available slots and avoids conflicts. 4. Sends confirmation details, time, and doctor to patients via SMS/email. 5. Handles rescheduling and cancellation requests. 6. Generates a summary showing: appointments processed, conflicts resolved, and confirmations sent.",
    "tags": [
      "python",
      "requests",
      "sql",
      "web",
      "machine learning"
    ]
  },
  {
    "id": "10.1",
    "category": "APIs & Cloud Automation",
    "subcategory": "APIs & Data Pipelines",
    "title": "REST API Data Ingestion Pipeline",
    "description": "Write a Python script using requests and SQLAlchemy that: â¢ Consumes a public REST API, sending GET requests to retrieve paginated JSON data. â¢ Parses the JSON responses and stores the structured data into a PostgreSQL database. â¢ Implements error handling for HTTP errors,429 rate limits,401/403 auth issues. â¢ Includes retry logic with exponential backoff for failed requests. â¢ Validates JSON schema before insertion to handle malformed responses. â¢ Generates a summary showing: total requests sent, successful records stored, and data integrity status.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "sql"
    ]
  },
  {
    "id": "10.2",
    "category": "APIs & Cloud Automation",
    "subcategory": "APIs & Data Pipelines",
    "title": "GraphQL API Client & Query Runner",
    "description": "Write a Python develop a Python program using gql and requests that: â¢ Reads GraphQL queries from.graphql files or a JSON configuration. â¢ Executes queries against a GraphQL endpoint, handling authentication headers. â¢ Saves the response data to structured CSV or JSON files. â¢ Handles malformed queries and network failures with retries. â¢ Generates a summary showing: queries executed, results retrieved, and operation success rate.",
    "tags": [
      "python",
      "requests",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "10.3",
    "category": "APIs & Cloud Automation",
    "subcategory": "APIs & Data Pipelines",
    "title": "GitHub Issue Reporter",
    "description": "Write a Python develop a Python script using PyGithub that: 1. Fetches issues from multiple GitHub repositories via the API. 2. Filters issues by status open and labels bug. 3. Generates a CSV report summarizing each bug title, assignee, and URL. 4. Handles API rate limiting with retries and caching. 5. Generates a summary showing: repositories analyzed, issues retrieved, and report generation status.",
    "tags": [
      "python",
      "api",
      "csv"
    ]
  },
  {
    "id": "10.4",
    "category": "APIs & Cloud Automation",
    "subcategory": "APIs & Data Pipelines",
    "title": "API Documentation Generator",
    "description": "Write a Python create a Python utility using prance or openapi-spec-validator that: 1. Parses an OpenAPI/Swagger YAML or JSON specification file. 2. Generates styled HTML documentation using a template, ReDoc, and Swagger UI. 3. Provides options to output PDFs using weasyprint. 4. Handles malformed spec files with validation errors. 5. Generates a summary showing: endpoints documented, output formats generated, and validation status.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "10.5",
    "category": "APIs & Cloud Automation",
    "subcategory": "APIs & Data Pipelines",
    "title": "Multi-Source Data Integration Pipeline",
    "description": "Write a Python develop a Python project using requests and pandas that: 1. Fetches data from multiple different APIs, weather, finance, and news. 2. Merges the datasets into a unified Pandas DataFrame based on a common key, timestamp. 3. Generates a combined report CSV/HTML from the merged data. 4. Handles API errors and schema mismatches gracefully. 5. Generates a summary showing: APIs integrated, records merged, and report generation success. Cloud Services",
    "tags": [
      "python",
      "pandas",
      "requests",
      "api",
      "csv"
    ]
  },
  {
    "id": "10.6",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "AWS S3 Secure File Uploader",
    "description": "Write a Python automation script using boto3 that: 1. Connects to an AWS S3 bucket using IAM roles or credentials from environment variables. 2. Uploads all files from a specified local directory to the bucket. 3. Applies server-side encryption SSE-S3 or SSE-KMS to each file during upload. 4. Validates file integrity by comparing local MD5 checksums with S3 ETags after upload. 5. Handles ClientError exceptions for authentication and permission failures gracefully. â¢ Generates a summary showing: files uploaded, total storage used MB, and checksum validation status.",
    "tags": [
      "python",
      "aws",
      "automation",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "10.7",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "AWS EC2 Instance Lifecycle Manager",
    "description": "Write a Python develop a Python tool using boto3 that: 1. Reads instance configuration, instance type, region, tags, and AMI from a YAML file. 2. Launches new EC2 instances and terminates instances based on their InstanceId. 3. Monitors instance status using waiters until they reach a running or terminated state. 4. Handles ClientError exceptions for quota limits and invalid permissions gracefully. 5. Generates a summary showing: instances launched, instances terminated, and lifecycle operation success rate.",
    "tags": [
      "python",
      "machine learning",
      "cli",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "10.8",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "Azure Blob Storage Manager",
    "description": "Write a Python automation script using azure-storage-blob that: 1. Connects to an Azure Blob Storage container using a connection string. 2. Uploads files from a local directory and deletes blobs older than a specified retention period. 3. Applies Azure Blob lifecycle management policies if configured. 4. Handles AzureError exceptions for authentication and network issues. 5. Generates a summary showing: files uploaded, blobs deleted, and lifecycle policy execution status.",
    "tags": [
      "python",
      "automation",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "10.9",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "GCP VM Provisioning Automator",
    "description": "Write a Python create a Python automation system using Google Cloud Compute that: 1. Reads VM configuration machine type, zone, disk image from a JSON file. 2. Provisions Google Compute Engine instances and automatically installs dependencies using startup scripts. 3. Implements a rollback mechanism to delete instances if post-deployment validation fails. 4. Handles GoogleAPICallError for quota and permission issues. 5. Generates a summary showing: VMs created, resources allocated, and successful provisioning rate.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "10.10",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "AWS Lambda Function Deployer",
    "description": "Write a Python develop a Python pipeline using boto3 that: 1. Packages Python code and dependencies into a ZIP deployment package. 2. Deploys or updates an AWS Lambda function using the deployment package. 3. Triggers the function and monitors its execution logs in CloudWatch. 4. Implements retry logic for failed function invocations. 5. Handles IAM permission errors gracefully. 6. Generates a summary showing: functions deployed, invocations processed, and success rate with retries.",
    "tags": [
      "python",
      "aws",
      "machine learning",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "10.11",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "AWS CloudWatch Log Alerting",
    "description": "Write a Python automation script using boto3 that: 1. Collects logs from multiple AWS services, Lambda, and EC2 using CloudWatch Logs Insights. 2. Detects error patterns, \"ERROR\", \"Exception\" in the log streams. 3. Triggers alerts to Slack or email via SNS or webhooks. 4. Provides an option to archive old logs to S3. 5. Handles CloudWatch throttling with exponential backoff. 6. Generates a summary showing: log groups scanned, errors detected, and alerts triggered.",
    "tags": [
      "python",
      "aws",
      "web",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "10.12",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "AWS DynamoDB Batch Operations",
    "description": "Write a Python create a Python automation system using boto3 that: 1. Connects to an AWS DynamoDB table. 2. Performs batch write operations to insert JSON records from a file. 3. Queries the table based on defined parameters. 4. Handles ProvisionedThroughputExceededException with retries and backoff. 5. Generates a summary showing: records inserted, queries executed, and throughput usage.",
    "tags": [
      "python",
      "aws",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "10.13",
    "category": "APIs & Cloud Automation",
    "subcategory": "Cloud Services",
    "title": "AWS IAM User Manager",
    "description": "Write a Python develop a Python pipeline using boto3 that: 1. Reads IAM user and policy configurations from a YAML file. 2. Creates, updates, or deletes IAM users programmatically. 3. Attaches managed policies or inline policies to users. 4. Handles EntityAlreadyExistsException and NoSuchEntityException gracefully. 5. Generates a summary showing: users managed, policies attached, and IAM operation success rate. Productivity & Project Management",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "10.14",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "Google Sheets Automated Data Processor",
    "description": "Write a Python create a Python pipeline using gspread and oauth2client that: 1. Authenticates with the Google Sheets API using OAuth2.0 credentials. 2. Reads structured data from a specified worksheet in a Google Sheet. 3. Processes the data, cleans, transforms, and calculates new values. 4. Writes the results back to a new sheet or range in the same workbook. 5. Handles expired OAuth tokens by automatically refreshing them. 6. Handles malformed data rows by skipping and logging errors. 7. Generates a summary showing: rows read, cells updated, and synchronization status.",
    "tags": [
      "python",
      "api",
      "cli",
      "logging"
    ]
  },
  {
    "id": "10.15",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "Trello Project Management Automator",
    "description": "Write a Python develop a Python program using py-trello or requests that: 1. Authenticates with the Trello API using OAuth keys and tokens. 2. Creates new cards, updates checklists, and moves cards between lists on a board. 3. Assigns team members to cards based on a configuration file. 4. Handles network timeouts and HTTPError responses gracefully. 5. Generates a summary showing: cards created, cards updated, and board synchronization status.",
    "tags": [
      "python",
      "requests",
      "api",
      "file processing"
    ]
  },
  {
    "id": "10.16",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "Notion Database Synchronizer",
    "description": "Write a Python develop a Python utility using notion-client that: 1. Authenticates with the Notion API using an integration token. 2. Reads structured data from a CSV file and creates new pages in a Notion database. 3. Updates existing database entries and archives old ones by moving them to an \"Archive\" database. 4. Handles API rate limits and authentication errors gracefully. 5. Generates a summary showing: pages created, pages updated, and database sync status.",
    "tags": [
      "python",
      "api",
      "csv",
      "sql",
      "cli"
    ]
  },
  {
    "id": "10.17",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "Jira Issue Management Automator",
    "description": "Write a Python develop a Python script using Jira library that: 1. Authenticates with the Jira API using username/password or API token. 2. Creates new issues, transitions them through workflows, and assigns users based on rules in a config file. 3. Handles JIRAError for authentication and invalid field errors. 4. Generates a summary showing: issues created, transitions performed, and Jira sync status.",
    "tags": [
      "python",
      "api",
      "file processing"
    ]
  },
  {
    "id": "10.18",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "Google Calendar Event Scheduler",
    "description": "Write a Python create a Python utility using the google-api-python-client that: 1. Authenticates with the Google Calendar API using OAuth2.0. 2. Reads event details, title, time, and attendees from a JSON file. 3. Creates events on a shared calendar and sends email invites to attendees. 4. Handles event conflicts by adjusting times or notifying the user. 5. Generates a summary showing: events created, conflicts resolved, and invites sent.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "10.19",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "Asana Task Management Automator",
    "description": "Write a Python script using the asana library that: 1. Authenticates with the Asana API using a Personal Access Token. 2. Creates tasks, assigns them to users, and updates their progress based on a config file. 3. Handles rate limits and authentication errors gracefully. 4. Generates a summary showing: tasks created, tasks updated, and project sync status.",
    "tags": [
      "python",
      "api",
      "file processing"
    ]
  },
  {
    "id": "10.20",
    "category": "APIs & Cloud Automation",
    "subcategory": "Productivity & Project Management",
    "title": "GitHub Actions Workflow Trigger",
    "description": "Write a Python develop a Python script using github3.py or requests that: 1. Authenticates with the GitHub API using a Personal Access Token PAT. 2. Triggers GitHub Actions workflows for a repository by dispatching a workflow_dispatch event. 3. Monitors the triggered workflow run until completion and fetches its logs. 4. Stores the logs in a file or database for auditing. 5. Handles invalid workflow IDs or authentication errors. 6. Generates a summary showing: workflows triggered, final statuses success/failure, and log storage success. System Utilities",
    "tags": [
      "python",
      "requests",
      "api",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "10.21",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Automate SFTP File Transfer with Integrity",
    "description": "Write a Python develop a Python utility using Paramiko that: 1. Reads source and destination paths from a YAML configuration file. 2. Transfers files from a local directory to a remote server via SFTP. 3. Validates file integrity after transfer by comparing MD5 checksums. 4. Handles permission errors, dropped connections, and partial transfers gracefully. 5. Generates a summary showing: files transferred, total data size in MB, and checksum validation status.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "10.22",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "OAuth2.0 API Client with Token Management",
    "description": "Write a Python 1. Create a Python program using requests_oauthlib that: 2. Authenticates with an API using the OAuth2.0 client credentials flow. 3. Automatically retrieves and refreshes access tokens upon expiration. 4. Securely stores tokens using keyring or an encrypted config file. 5. Fetches protected data from the API using the valid token. 6. Handles authentication failures, invalid client secret, and refresh errors gracefully. 7. Generates a summary showing: API calls made, token refreshes performed, and authentication success rate.",
    "tags": [
      "python",
      "requests",
      "api",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "10.23",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Process Monitor & Resource Hog Killer",
    "description": "Write a Python script using psutil that: 1. Monitors all running processes on a system, logging their CPU and memory usage. 2. Flags processes that exceed user-defined thresholds for CPU or memory. 3. Provides an option auto-terminate to automatically terminate offending processes. 4. Handles AccessDenied exceptions for processes without sufficient permissions. 5. Generates a summary showing: processes monitored, alerts triggered, and processes terminated.",
    "tags": [
      "python",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "10.24",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Remote SSH Command Executor",
    "description": "Write a Python develop a Python script using paramiko that: 1. Reads a list of remote servers and commands from an inventory file. 2. Establishes SSH connections and executes the commands sequentially. 3. Captures and logs the stdout and stderr from each command. 4. Handles connection timeouts and authentication failures gracefully. 5. Generates a summary showing: servers accessed, commands executed, and remote operation success rate.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "10.25",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Multi-Server Disk Usage Monitor",
    "description": "Write a Python program using paramiko and sqlite3 that: 1. Connects to multiple servers via SSH and runs df commands to check disk usage. 2. Generates alerts if usage exceeds a configured threshold90%. 3. Stores the collected statistics in a central SQLite database. 4. Handles unreachable servers or authentication failures gracefully. 5. Generates a summary showing: servers monitored, alerts triggered, and data collection success rate.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "monitoring"
    ]
  },
  {
    "id": "10.26",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Security Audit Log Processor",
    "description": "Write a Python create a Python pipeline using pandas and SQLAlchemy that: 1. Parses system audit logs, /var/log/secure, Windows Event Logs. 2. Categorizes events by severity: INFO, WARNING, CRITICAL. 3. Stores structured results in a PostgreSQL database. 4. Provides a simple dashboard HTML showing security event trends. 5. Handles large log volumes with batch processing. 6. Generates a summary showing: logs processed, critical events identified, and dashboard update status.",
    "tags": [
      "python",
      "pandas",
      "sql",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "10.27",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Cron Job Management Utility",
    "description": "Write a Python create a Python tool using python-crontab that: 1. Programmatically adds, updates, and deletes user cron jobs on a Unix system. 2. Exports the current crontab to a backup file before making changes. 3. Handles permission errors and invalid cron syntax gracefully. 4. Generates a summary showing: cron jobs modified, backups created, and modification success status.",
    "tags": [
      "python",
      "file processing"
    ]
  },
  {
    "id": "10.28",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Automated Hardware Inventory Collector",
    "description": "Write a Python develop a Python script using paramiko and SQLAlchemy that: 1. SSHs into a list of servers and gathers hardware details, CPU model, RAM, and disk size. 2. Stores the collected inventory data in a central PostgreSQL database. 3. Handles unreachable servers or OS compatibility issues gracefully. 4. Generates a summary showing: servers scanned, inventory records stored, and data completeness.",
    "tags": [
      "python",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "10.29",
    "category": "APIs & Cloud Automation",
    "subcategory": "System Utilities",
    "title": "Log Archiver & Cleanup Utility",
    "description": "Write a Python utility using gzip and shutil that: 1. Archives log files older than a specified number of days,30, by compressing them. 2. Moves the archives to a designated directory and deletes original logs. 3. Handles locked or busy files gracefully. 4. Generates a summary showing: logs archived, disk space freed, and cleanup completion status.",
    "tags": [
      "python",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "11.1",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Automated API Test Runner",
    "description": "Write a Python develop a Python tool using requests and jsonschema that: 1. Reads API endpoint definitions URL, method, expected status code, and response schema from a JSON file. 2. Sends requests to each endpoint and validates the response code and JSON structure. 3. Exports detailed test results to an HTML report using Jinja2. 4. Handles timeouts and connection errors gracefully. 5. Generates a summary showing: endpoints tested, validation pass rate, and schema compliance.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "11.2",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Scheduled Maintenance Task Runner",
    "description": "Write a Python create a Python script using schedule and subprocess that: 1. Reads task definitions, command, schedule, and timeout from a YAML config file. 2. Executes system commands, backup scripts, and cleanup jobs at their specified times. 3. Logs the output and exit status of each task to a file. 4. Handles task execution errors and timeouts gracefully. 5. Generates a summary showing: tasks executed, successful completions, and failures.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "11.3",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Docker Container Lifecycle Manager",
    "description": "Write a Python utility using Docker SDK that: 1. Connects to the Docker daemon to list, start, stop, and monitor containers. 2. Automatically restarts containers that have an unhealthy status or have exited unexpectedly. 3. Handles errors related to missing images or insufficient permissions. 4. Generates a summary showing: containers managed, restart operations performed, and overall uptime percentage.",
    "tags": [
      "python",
      "docker",
      "machine learning",
      "monitoring"
    ]
  },
  {
    "id": "11.4",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Encrypted Configuration Backup to Cloud",
    "description": "Write a Python create a Python script using cryptography and boto3 that: 1. Compresses a directory of configuration files into a timestamped archive. 2. Encrypts the archive using a symmetric key, Fernet. 3. Uploads the encrypted archive to a cloud storage bucket, AWS S3. 4. Handles failed uploads, network errors, and missing files gracefully. 5. Generates a summary showing: files backed up, archive size, and cloud upload success status.",
    "tags": [
      "python",
      "aws",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "11.5",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Jenkins Build Pipeline Trigger",
    "description": "Write a Python tool using JenkinsAPI or requests that: 1. Reads job names and parameters from a YAML configuration file. 2. Triggers Jenkins build pipelines via the REST API. 3. Polls the job status until completion and validates the build outcome. 4. Handles authentication errors and network timeouts gracefully. 5. Generates a summary showing: builds triggered, success rate, and pipeline completion status.",
    "tags": [
      "python",
      "requests",
      "api",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "11.6",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Real-Time Log Monitor & Alerting",
    "description": "Write a Python create a Python utility using watchdog and re that: 1. Monitors log files in real-time for predefined error patterns, \"ERROR\", \"CRITICAL\". 2. Triggers alerts email/Slack immediately upon detection of a critical pattern. 3. Stores matched log entries in a SQLite database for auditing. 4. Handles large log files efficiently with tailing and rotation awareness. 5. Generates a summary showing: log files scanned, alerts generated, and critical patterns detected.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "11.7",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Kubernetes Pod Health Monitor & Autorester",
    "description": "Write a Python program using Kubernetes client library that: 1. Connects to a Kubernetes cluster and lists all pods in a specified namespace. 2. Monitors pod statuses and automatically restarts any pod in CrashLoopBackOff or Error state. 3. Provides options to scale deployments up/down based on CPU metrics. 4. Handles cluster authentication errors and ApiExceptions gracefully. 5. Generates a summary showing: pods monitored, restarts performed, and scaling operations.",
    "tags": [
      "python",
      "api",
      "cli",
      "monitoring"
    ]
  },
  {
    "id": "11.8",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Python Application Deployment Automator",
    "description": "Write a Python create a Python script using fabric or paramiko that: 1. Packages a local Python application into a tarball including requirements.txt. 2. Uploads the package to a remote server via SCP/SFTP. 3. Installs dependencies in a virtual environment and restarts the application service, systemd. 4. Implements a rollback mechanism to revert to the previous version if deployment fails. 5. Generates a summary showing: deployments attempted, success rate, and service status.",
    "tags": [
      "python",
      "machine learning"
    ]
  },
  {
    "id": "11.9",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Cloud VM Provisioning Automator",
    "description": "Write a Python project using a cloud SDK, boto3, and google- cloud-compute that: 1. Provision new virtual machines on a cloud provider from a JSON configuration. 2. Configures network settings, VPC, firewall rules and deploys a base application image. 3. Handles authentication errors and API quota issues gracefully. 4. Generates a summary showing: VMs provisioned, configurations applied, and provisioning success.",
    "tags": [
      "python",
      "api",
      "json"
    ]
  },
  {
    "id": "11.10",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "CI/CD Test Gatekeeper",
    "description": "Write a Python develop a Python script that: 1. Reads test definitions from a YAML file, unit tests, and integration tests. 2. Executes tests in a local or remote environment, capturing results. 3. Stores test results and pass/fail status in a database. 4. Only triggers a build pipeline via API call if all tests pass. 5. Handles test environment setup errors gracefully. 6. Generates a summary showing: tests executed, pass rate, and build trigger status.",
    "tags": [
      "python",
      "api",
      "sql",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "11.11",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Slack/Teams Notification Sender",
    "description": "Write a Python develop a Python script using slack_sdk or pymsteams that: 1. Sends notifications to a Slack channel or Microsoft Teams webhook. 2. Reads message content and channel configuration from a YAML file. 3. Handles API authentication failures and rate limits. 4. Generates a summary showing: messages sent, channels notified, and delivery success rate.",
    "tags": [
      "python",
      "api",
      "web",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "11.12",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Dynamic Rate-Limit Handler",
    "description": "Write a Python program using requests and Redis for queue that: 1. Monitors API response headers for rate-limit information X-RateLimit-Remaining. 2. Dynamically adjusts request frequency to stay within limits. 3. Queues failed requests for retry after the rate-limit reset period. 4. Logs performance metrics requests/minute, retry count. 5. Handles malformed responses without rate-limit headers. 6. Generates a summary showing: requests sent, retries performed, and rate-limit compliance.",
    "tags": [
      "python",
      "requests",
      "api",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "11.13",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Cloud Storage Sync Tool",
    "description": "Write a Python develop a Python script using boto3 or google-cloud-storage that: 1. Synchronizes files between local directories and cloud storage buckets S3, GCS. 2. Validates file integrity after transfer using checksums. 3. Handles permission errors and network connectivity issues with retries. 4. Generates a summary showing: operations upload/download/delete performed, total data size, and checksum validation status.",
    "tags": [
      "python",
      "file processing"
    ]
  },
  {
    "id": "11.14",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "SSL Certificate Renewal Automator",
    "description": "Write a Python program using cryptography and paramiko that: 1. Connects to a certificate authority, Let's Encrypt via certbot to request new SSL certs. 2. Deploys the renewed certificates to one or more web servers, Nginx, and Apache. 3. Validates that the new certificate is active and correctly installed. 4. Handles expiration and validation errors gracefully. 5. Generates a summary showing: certificates renewed, servers updated, and validation success.",
    "tags": [
      "python",
      "web"
    ]
  },
  {
    "id": "11.15",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "REST API Performance Tester",
    "description": "Write a Python create a Python script using locust or requests with timing that: 1. Sends a high volume of requests to a REST API endpoint. 2. Measures and logs response latency and throughput. 3. Exports performance metrics, p95 latency, and RPS to a CSV report. 4. Handles server timeouts and errors without crashing. 5. Generates a summary showing: requests sent, average latency, and threshold compliance.",
    "tags": [
      "python",
      "requests",
      "api",
      "csv",
      "logging"
    ]
  },
  {
    "id": "11.16",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Infrastructure Readiness Validator",
    "description": "Write a Python create a Python end-to-end automation system using multiple SDKs that: 1. Checks server health, SSH, network connectivity ping, and application response HTTP via APIs. 2. Generates a PDF readiness report using reportlab. 3. Returns a non-zero exit code if any critical check fails. 4. Handles connection failures and timeouts gracefully. 5. Generates a summary showing: checks performed, failures detected, and overall readiness status. DevOps & Orchestration",
    "tags": [
      "python",
      "api",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.17",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Kubernetes Deployment Scaler",
    "description": "Write a Python script using Kubernetes client library that: 1. Authenticates with a Kubernetes cluster in-cluster config or kubeconfig file. 2. Scales a specified deployment up or down to a desired replica count. 3. Monitors the rollout status until all new pods are ready. 4. Handles ApiException for authentication errors and invalid resources. 5. Generates a summary showing: scaling operations performed, pods updated, and successful scale status.",
    "tags": [
      "python",
      "api",
      "cli",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "11.18",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Apache Airflow DAG Orchestrator",
    "description": "Write a Python create a Python pipeline that defines an Airflow DAG using the Airflow SDK: 1. Defines multiple tasks, PythonOperator, and BashOperator for scraping, cleaning, and reporting. 2. Sets task dependencies to create a directed acyclic graph DAG. 3. Handles task failures with built-in retry mechanisms and alerting. 4. Monitors DAG run progress via the Airflow API or metadata database. 5. Generates a summary showing: DAGs executed, tasks retried, and overall pipeline success.",
    "tags": [
      "python",
      "api",
      "sql",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "11.19",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Celery Distributed Task Queue",
    "description": "Write a Python create a Python program that sets up a Celery application: 1. Defines Celery tasks that process data from a message queue, Redis/RabbitMQ. 2. Uses multiple workers to execute tasks in parallel. 3. Logs task results and failures in a PostgreSQL database. 4. Implements retry logic for failed tasks using @task.autoretry_for. 5. Handles worker crashes by reassigning tasks. 6. Generates a summary showing: tasks executed, retries performed, and job completion rate.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "11.20",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "GitLab CI/CD Pipeline Manager",
    "description": "Write a Python develop a Python program using python-gitlab that: 1. Authenticates with the GitLab API using a private token. 2. Triggers CI/CD pipelines for a project on a specific branch or tag. 3. Monitors pipeline jobs until completion and fetches trace logs. 4. Stores the logs in an audit database. 5. Handles failed jobs by retrying them automatically. 6. Generates a summary showing: pipelines executed, success rate, and log capture status.",
    "tags": [
      "python",
      "api",
      "sql",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "11.21",
    "category": "DevOps & System Automation",
    "subcategory": "DevOps Tools",
    "title": "Prefect Flow Orchestrator",
    "description": "Write a Python automation system using Prefect that: 1. Defines a Prefect Flow composed of multiple tasks. 2. Deploys the flow to a Prefect server or Prefect Cloud. 3. Triggers flow runs and monitors their progress via the Prefect API. 4. Implements retry logic and fallbacks for failed tasks. 5. Handles network disruptions between agents and the server. 6. Generates a summary showing: flows executed, retries performed, and overall success rate. Distributed & Multi-Cloud",
    "tags": [
      "python",
      "api",
      "automation",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "11.22",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Multi-Cloud Backup Validator",
    "description": "Write a Python automation system using boto3, azure-storage- blob, and google-cloud-storage that: 1. Uploads the same set of files to AWS S3, Azure Blob Storage, and Google Cloud Storage simultaneously. 2. Validates each upload by comparing local and cloud checksums. 3. Handles provider-specific authentication and quota errors gracefully. 4. Generates a summary showing: files backed up per cloud, total storage used, and redundancy validation status.",
    "tags": [
      "python",
      "aws",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "11.23",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Dropbox Folder Synchronizer",
    "description": "Write a Python automation project using Dropbox SDK that: 1. Authenticates with the Dropbox API using an OAuth2 token. 2. Syncs a local folder to a Dropbox path, uploading new files and downloading remote changes. 3. Archives files deleted locally to a _deleted_archive folder in Dropbox. 4. Handles token expiration and network errors gracefully. 5. Generates a summary showing: files uploaded, files synced, and archive operation success.",
    "tags": [
      "python",
      "api",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "11.24",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Microsoft Teams Chat Logger",
    "description": "Write a Python develop a Python project using pymsteams and msal that: 1. Authenticates with the Microsoft Graph API to access Teams data. 2. Sends meeting summary messages to a Teams channel. 3. Fetches chat history from specified teams and channels and stores it in a SQL database. 4. Exports chat logs to PDF reports. 5. Handles OAuth token expiration gracefully. 6. Generates a summary showing: messages processed, summaries sent, and logs stored.",
    "tags": [
      "python",
      "api",
      "sql",
      "logging"
    ]
  },
  {
    "id": "11.25",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Apache Spark ETL Pipeline",
    "description": "Write a Python create a Python automation tool using PySpark that: 1. Submits a Spark application to a cluster, standalone, YARN, or Kubernetes. 2. Reads CSV files from cloud storage S3/GCS, applies transformations filters, aggregate, joins. 3. Writes the processed results to a database, PostgreSQL, Parquet. 4. Handles worker failures and speculative execution. 5. Generates a summary showing: files processed, transformations applied, and job success status.",
    "tags": [
      "python",
      "csv",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.26",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Google Cloud Pub/Sub Message Broker",
    "description": "Write a Python utility using Google Cloud PubSub that: 1. Publishes messages to a Pub/Sub topic from a data stream, Kafka, local file. 2. Creates a subscription to the topic and processes messages in real time. 3. Implements error handling for acknowledged/unacknowledged messages. 4. Handles connectivity issues with retries. 5. Generates a summary showing: messages published, messages consumed, and processing success rate.",
    "tags": [
      "python",
      "file processing"
    ]
  },
  {
    "id": "11.27",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Salesforce Data Sync & Reporter",
    "description": "Write a Python create a Python script using simple-salesforce that: 1. Authenticates with the Salesforce REST API using OAuth2 username-password flow. 2. Fetches customer data using SOQL queries. 3. Updates records based on business logic. 4. Generates daily reports in CSV format from the queried data. 5. Handles expired sessions by re-authenticating. 6. Generates a summary showing: records fetched, records updated, and reports generated.",
    "tags": [
      "python",
      "api",
      "csv",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "11.28",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Multi-Cloud Monitoring Aggregator",
    "description": "Write a Python develop a Python utility using boto3, azure-mgmt-monitor, and google-cloud-monitoring that: 1. Collects compute metrics CPU, memory, disk from EC2, Azure VMs, and GCP Compute Instances. 2. Stores all metrics in a central Prometheus or InfluxDB database. 3. Generates a unified Grafana dashboard for cross-cloud visibility. 4. Handles instances that are stopped or terminated during collection. 5. Generates a summary showing: services monitored, metrics collected, and dashboard update status.",
    "tags": [
      "python",
      "sql",
      "monitoring"
    ]
  },
  {
    "id": "11.29",
    "category": "DevOps & System Automation",
    "subcategory": "Distributed & Multi-Cloud",
    "title": "Zoom Meeting Scheduler & Reporter",
    "description": "Write a Python create a Python project using Zoomus or requests that: 1. Authenticates with the Zoom API using OAuth or JWT. 2. Schedules meetings based on details in a JSON configuration file. 3. Fetches attendance reports for past meetings. 4. Generates PDF summaries of attendance using reportlab. 5. Handles authentication errors and invalid meeting parameters. 6. Generates a summary showing: meetings scheduled, reports generated, and PDF distribution status. Cloud Services",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "11.30",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "AWS S3 File Upload and Signed URL Generation",
    "description": "Write a Python script that uploads files to AWS S3 and generates signed URLs for secure sharing. The script should: 1. Accept input files from a specified folder. 2. Upload each file to an S3 bucket with proper permissions. 3. Generate time-limited signed URLs for download access. 4. Save all metadata file name, size, URL, and expiration time into a JSON log. 5. Provide error handling for failed uploads or missing AWS credentials. 6. At the end, generate a summary report showing the number of files uploaded, total storage size, and average upload time.",
    "tags": [
      "python",
      "aws",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "11.31",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Google Drive File Upload and Organization",
    "description": "Write a Python create a Python script that automates file uploads to Google Drive. The program should: 1. Authenticate with Google Drive API. 2. Upload files from a specified folder. 3. Organize them into subfolders based on file type: PDF, images, and docs. 4. Generate shareable links for each file. 5. Save all details into a JSON log with file name, folder path, and URL. 6. At the end, generate a summary showing total files uploaded, total storage used, and the most common file types.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.32",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Dropbox File Upload and Organization",
    "description": "Write a Python automation script that integrates with the Dropbox API. The script should: 1. Upload files from a local folder into Dropbox. 2. Organize files into subfolders by type: images, PDFs, and docs. 3. Generate public share links for uploaded files. 4. Save metadata into a JSON log with file name, size, upload time, and share URL. 5. Provide error handling for failed uploads or missing API tokens. 6. At the end, generate a summary showing the number of files uploaded, total storage consumed, and the most common file type.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.33",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "AWS EC2 Instance Management",
    "description": "Write a Python automation script that integrates with the AWS EC2 API. The workflow should: 1. Start, stop, or reboot EC2 instances based on input from a CSV file. 2. Save instance metadata ID, type, region, and status into a JSON log. 3. Generate a.pdf summary of actions taken. 4. Provide error handling for invalid instance IDs or missing AWS credentials. 5. At the end, generate a report showing total instances managed, average runtime per action, and failed requests.",
    "tags": [
      "python",
      "requests",
      "aws",
      "api",
      "csv"
    ]
  },
  {
    "id": "11.34",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "AWS Lambda Function Deployment and Testing",
    "description": "Write a Python create a Python automation program that connects with the AWS Lambda API. The workflow should: 1. Deploy a Python function from local code into AWS Lambda. 2. Trigger the function with test events. 3. Capture and log execution results. 4. Save deployment details, function name, ARN, runtime, and status into a JSON log. 5. At the end, generate a summary report showing deployment success rate, average execution time, and failed test events.",
    "tags": [
      "python",
      "aws",
      "api",
      "json",
      "automation"
    ]
  },
  {
    "id": "11.35",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "AWS CloudWatch EC2 Metrics Monitoring",
    "description": "Write a Python workflow that integrates with the AWS CloudWatch API. The script should: 1. Fetch system metrics CPU, memory, disk usage for EC2 instances. 2. Store results into a local SQLite database. 3. Generate line charts showing performance trends. 4. Send alerts if thresholds are exceeded. 5. Save logs into a JSON file with instance ID, metric, and status. 6. At the end, generate a report showing the number of metrics fetched, average values, and alert counts.",
    "tags": [
      "python",
      "aws",
      "api",
      "json",
      "sql"
    ]
  },
  {
    "id": "11.36",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "AWS SES Bulk Email Automation",
    "description": "Write a Python develop a Python script that integrates with the AWS SES Simple Email Service API. The program should: 1. Send bulk emails from a CSV list of recipients. 2. Personalize each email with name and custom fields. 3. Track delivery status, bounces, and complaints. 4. Save results into a JSON file with recipient ID, status, and timestamp. 5. At the end, generate a summary showing the number of emails sent, the delivery success rate, and the bounce statistics. APIs & Data Services",
    "tags": [
      "python",
      "aws",
      "api",
      "csv",
      "json"
    ]
  },
  {
    "id": "11.37",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Google Sheets Data and Chart Generator",
    "description": "Write a Python develop a Python script that pulls data from the Google Sheets API and generates charts. The script should: 1. Authenticate with Google Sheets using service account credentials. 2. Fetch data from a given spreadsheet and worksheet. 3. Process data with Pandas. 4. Generate bar charts, pie charts, and line graphs using Matplotlib. 5. Save outputs into a.pdf report. 6. Maintain a JSON log showing sheet name, number of rows processed, and chart types created. 7. At the end, generate a summary report showing the number of charts generated and processing time.",
    "tags": [
      "python",
      "pandas",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "11.38",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "OpenAI API Code Snippet Generation",
    "description": "Write a Python script that integrates with the OpenAI API to generate code snippets from plain English instructions. The program should: 1. Accept user s from a .txt file or command line. 2. Call the OpenAI API and request Python code solutions. 3. Save generated code snippets into.py files. 4. Append metadata , token usage, and response time into a JSON log. 5. Provide error handling for failed API calls or empty s. 6. At the end, generate a summary report showing the number of s processed, average response time, and the top5 most common coding tasks requested.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "11.39",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Stripe API Financial Report Generation",
    "description": "Write a Python build a Python program that connects with the Stripe API to generate automated financial reports. The script should: 1. Fetch payment records for a given period. 2. Process transactions into categories: successful, failed, and refunded. 3. Save results into an Excel file with pivot tables. 4. Email the report to the finance teams. 5. Save metadata in a JSON log with transaction count, total value, and processing duration. 6. At the end, generate a summary showing revenue, refund ratio, and the most common error codes.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.40",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Google Translate API Bulk Translation",
    "description": "Write a Python build a Python script that integrates with the Google Translate API for bulk translations. The script should: 1. Accept a CSV file with a column text_to_translate. 2. Translate content into multiple target languages: English, Spanish, and French. 3. Append results as new columns in the CSV. 4. Save logs into a JSON file with original text, languages translated, and status. 5. Provide error handling for unsupported characters or failed API requests. 6. At the end, generate a summary showing total rows translated, languages processed, and skipped entries.",
    "tags": [
      "python",
      "requests",
      "api",
      "csv",
      "json"
    ]
  },
  {
    "id": "11.41",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Salesforce Lead and Opportunity Reporting",
    "description": "Write a Python program that connects with the Salesforce API. The workflow should: 1. Fetch leads and opportunities from the CRM. 2. Export data into an Excel file with pivot tables. 3. Create charts summarizing lead sources and conversion rates. 4. Email the report to sales managers. 5. Save metadata into a JSON log with record counts, query duration, and API status. 6. At the end, generate a summary showing the number of leads fetched, deals in progress, and top lead sources.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "11.42",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Weather API Forecast Analysis and Reporting",
    "description": "Write a Python develop a Python script that connects with the Weather API OpenWeatherMap. The program should: 1. Fetch weather forecasts for a list of cities from a CSV file. 2. Save results into a JSON file with temperature, humidity, and conditions. 3. Generate bar charts comparing daily temperature trends across cities. 4. Email the report to recipients. 5. Provide error handling for invalid city names or missing API keys. 6. At the end, generate a summary showing the number of cities processed, the highest temperature recorded, and the most common weather condition.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "11.43",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Google BigQuery Data Query and Visualization",
    "description": "Write a Python develop a Python script that integrates with the Google BigQuery API. The program should: 1. Run SQL queries against a dataset. 2. Save query results into a CSV file. 3. Generate charts summarizing the data. 4. Email the output to analysts. 5. Save logs into a JSON file with query text, execution time, and row counts. 6. At the end, generate a summary showing the number of queries executed, average runtime, and the largest dataset queried.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "11.44",
    "category": "DevOps & System Automation",
    "subcategory": "Cloud Services",
    "title": "Google Maps API Address Analysis and Heatmap",
    "description": "Write a Python create a Python program that connects with the Google Maps API. The script should: 1. Read a CSV of addresses. 2. Fetch coordinates, travel times, and distances. 3. Save results into a new CSV file with additional columns. 4. Generate a heatmap visualization of locations using Folium. 5. Save logs into a JSON file with address, coordinates, and status. 6. At the end, generate a summary showing total addresses processed, longest travel time, and average distance. Productivity & Collaboration",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "file processing"
    ]
  },
  {
    "id": "11.45",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Slack/Discord System Alert Notifications",
    "description": "Write a Python create a Python automation program that sends notifications to Slack or Discord when system alerts occur. The workflow should: 1. Monitor a log file for error keywords, CRITICAL, and ERROR. 2. Format alert messages with timestamps and error details. 3. Send messages to a predefined Slack/Discord channel using webhooks. 4. Save logs into a JSON file with alert type, timestamp, and delivery status. 5. At the end, generate a summary report showing the total alerts detected, the number delivered successfully, and the most common error categories.",
    "tags": [
      "python",
      "json",
      "web",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.46",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Microsoft Teams Notification Automation",
    "description": "Write a Python develop a Python workflow that integrates with Microsoft Teams. The script should: 1. Monitor a system for specific triggers, service restarts. 2. Send notifications into a Teams channel using Microsoft Graph API. 3. Include log file snippets or charts as attachments. 4. Save delivery logs into a JSON file. 5. At the end, generate a summary showing the number of notifications sent, average delivery time, and most frequent trigger events.",
    "tags": [
      "python",
      "api",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "11.47",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Trello Task Creation Automation",
    "description": "Write a Python create a Python script that integrates with the Trello API to automate task creation. The program should: 1. Parse a CSV file of task descriptions. 2. Create new Trello cards under a specified board and list. 3. Assign members automatically based on CSV data. 4. Save card URLs and IDs into a JSON log. 5. Provide error handling for invalid board IDs or missing API keys.At the end, generate a report showing the number of cards created, users assigned, and skipped tasks.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "automation"
    ]
  },
  {
    "id": "11.48",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Google Calendar Event Creation",
    "description": "Write a Python develop a Python program that connects with the Google Calendar API. The workflow should: 1. Read upcoming events from a CSV file. 2. Create calendar events with title, description, start time, and end time. 3. Set reminders and send invites to attendees. 4. Save created event IDs and links into a JSON file. 5. Provide error handling for invalid dates or missing attendee emails. 6. At the end, generate a report showing the total number of events created, attendees added, and failed insertions.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "11.49",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Zoom Meeting Scheduling Automation",
    "description": "Write a Python build a Python script that integrates with the Zoom API. The script should: 1. Schedule Zoom meetings from a list of topics and times in a CSV file. 2. Generate meeting links and passwords. 3. Send links via email to participants. 4. Save logs into a JSON file with meeting ID, topic, and creation status. 5. At the end, generate a summary report showing the number of meetings scheduled, failed attempts, and the most common durations.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "11.50",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "GitLab CI/CD Pipeline Monitoring",
    "description": "Write a Python workflow that integrates with the GitLab API for DevOps automation. The program should: 1. Monitor CI/CD pipeline results. 2. Extract build status, errors, and logs. 3. Create a.pdf summary of build performance. 4. Send notifications to Slack or Teams for failed builds. 5. Save metadata into a JSON log with pipeline ID, duration, and status. 6. At the end, generate a summary showing pipelines executed, pass/fail ratio, and average build duration.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.51",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Notion Database Export and Update",
    "description": "Write a Python create a Python script that integrates with the Notion API. The workflow should: 1. Pull data from a Notion database, tasks, notes, or projects. 2. Export results into a CSV file for further analysis. 3. Add a new property âlast_exportedâ to each record. 4. Save logs into a JSON file with record IDs and status. 5. Provide error handling for missing API tokens or inaccessible databases. 6. At the end, generate a summary showing the number of records exported, properties updated, and skipped entries.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "11.52",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Zendesk Ticket Reporting and Categorization",
    "description": "Write a Python develop a Python automation program that connects with the Zendesk API. The script should: 1. Fetch new customer support tickets. 2. Categorize them by priority and type. 3. Generate a.xlsx report for the support team. 4. Send a summary email with ticket statistics. 5. Save logs into a JSON file with ticket ID, status, and category. 6. At the end, generate a report showing the number of tickets fetched, the top3 categories, and the average response time.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "11.53",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "LinkedIn Post Engagement Analysis",
    "description": "Write a Python build a Python workflow that integrates with the LinkedIn API. The program should: 1. Fetch recent posts and engagement metrics, likes, comments, shares. 2. Store results in a local database. 3. Generate a.pdf report with charts showing engagement trends. 4. Email the report to marketing teams. 5. Save logs into a JSON file with post IDs, engagement stats, and timestamps. 6. At the end, generate a summary showing top-performing posts and average engagement rate.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "11.54",
    "category": "DevOps & System Automation",
    "subcategory": "Productivity & Collaboration",
    "title": "Mailchimp Subscriber and Campaign Analysis",
    "description": "Write a Python script that integrates with the Mailchimp API. The script should: 1. Fetch subscriber lists and campaign results. 2. Segment users by engagement: active, inactive. 3. Export results into an Excel file with charts. 4. Save logs into a JSON file with subscriber counts, open rates, and click-through rates. 5. At the end, generate a summary showing total subscribers, campaign performance, and the most active audience segments. Media & Entertainment",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "11.55",
    "category": "DevOps & System Automation",
    "subcategory": "Media & Entertainment",
    "title": "X Brand Monitoring and Sentiment Analysis",
    "description": "Write a Python script that fetches data from the Twitter/X API for brand monitoring. The script should: 1. Track mentions of specific keywords or hashtags. 2. Store tweets in a local SQLite database. 3. Perform sentiment analysis using Hugging Face. 4. Generate a daily.pdf report with top mentions and sentiment trends. 5. Maintain logs in a JSON file with keyword, number of tweets fetched, and average sentiment. 6. At the end, generate a summary showing the top5 most active users and a daily sentiment breakdown.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "11.56",
    "category": "DevOps & System Automation",
    "subcategory": "Media & Entertainment",
    "title": "YouTube Channel Metadata Analysis",
    "description": "Write a Python build a Python automation tool that connects with the YouTube Data API. The script should: 1. Fetch video metadata title, views, likes, and comments from a channel. 2. Save results into an Excel file with charts showing growth trends. 3. Store raw data in JSON format for archival. 4. Provide error handling for invalid channel IDs or rate limits. 5. At the end, generate a summary showing the number of videos analyzed, total views, and average engagement rate.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "12.1",
    "category": "AI & NLP Automation",
    "subcategory": "Natural Language Processing",
    "title": "NLP for Customer Support Email Triage",
    "description": "Write a Python automation project using transformers that: 1. Connects to an email server IMAP to fetch new customer support emails. 2. Uses a pre-trained NLP model, BERT, to categorize email intent: complaint, inquiry, feedback. 3. Automatically assigns emails to the correct department based on predicted intent. 4. Stores the email and its classification in a PostgreSQL database. 5. Sends an automatic acknowledgment reply to the customer. 6. Filters out spam using a simple rule-based or model-based filter. 7. Generates a summary showing: emails processed, classification accuracy if ground truth exists, and routing success.",
    "tags": [
      "python",
      "sql",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "12.2",
    "category": "AI & NLP Automation",
    "subcategory": "Natural Language Processing",
    "title": "Generative Blog Post Drafting",
    "description": "Write a Python create a Python automation tool using OpenAI API or Transformers that: 1. Accepts a topic or outline for a blog post. 2. Uses a generative language model to create a full-length draft in Markdown format. 3. Applies a plagiarism check, using difflib against a web search on the generated content. 4. Saves only original drafts to a database for editorial review. 5. Handles invalid or too-short inputs by rejecting them. 6. Generates a summary showing: drafts created, drafts rejected for plagiarism, and successful saves.",
    "tags": [
      "python",
      "api",
      "sql",
      "web",
      "automation"
    ]
  },
  {
    "id": "12.3",
    "category": "AI & NLP Automation",
    "subcategory": "Natural Language Processing",
    "title": "Audio Transcription & Sentiment Analysis",
    "description": "Write a Python create a Python automation pipeline using speechrecognition and vaderSentiment that: 1. Transcribes audio recordings WAV, MP3 to text using a speech-to-text API or library. 2. Stores transcripts in a SQL database. 3. Applies sentiment analysis to the transcribed text to determine tone: positive, negative, or neutral. 4. Provides options to export transcripts as TXT or PDF reports. 5. Handles noisy audio by attempting transcription but logging quality issues. 6. Generates a summary showing: audio files processed, transcription accuracy WER if possible, and sentiment distribution.",
    "tags": [
      "python",
      "api",
      "sql",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "12.4",
    "category": "AI & NLP Automation",
    "subcategory": "Natural Language Processing",
    "title": "Research Article Summarization",
    "description": "Write a Python develop a Python script using sumy or transformers BART, T5 that: 1. Accepts research articles in PDF or text format. 2. Uses a text summarization model to generate concise abstracts. 3. Saves the summaries as Markdown files. 4. Provides a length parameter to control summary size, word count. 5. Handles scientific jargon by using a model trained on academic text. 6. Generates a summary showing: articles processed, average summary length ratio, and quality score, ROUGE if reference available.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.5",
    "category": "AI & NLP Automation",
    "subcategory": "Natural Language Processing",
    "title": "Automated Document Translation Pipeline",
    "description": "Write a Python create a Python automation pipeline using translate or argostranslate that: 1. Accepts documents TXT, PDF for translation. 2. Translates text content into multiple target languages specified in a config. 3. Preserves original formatting in PDFs where possible. 4. Saves translated versions as separate files. 5. Handles unsupported languages by skipping and logging an error. 6. Generates a summary showing: documents processed, languages translated to, and output files created. Predictive & Detection Systems",
    "tags": [
      "python",
      "automation",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.6",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "Predictive Maintenance for IoT Equipment",
    "description": "Write a Python develop a Python script using scikit-learn or TensorFlow that: 1. Trains a machine learning model, Random Forest, and LSTM on historical IoT sensor data to predict failures. 2. Applies the trained model to a real-time stream of new sensor data. 3. Triggers alerts, email, and SMS for predictions exceeding a failure risk threshold. 4. Provides an option to periodically retrain the model with newly collected data. 5. Handles missing sensor readings via imputation. 6. Generates a summary showing: predictions made, alerts triggered, and model accuracy metrics pre/post retraining.",
    "tags": [
      "python",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "12.7",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "Financial Fraud Detection System",
    "description": "Write a Python program using isolationforest or pyod that: 1. Trains an anomaly detection model on historical financial transaction data. 2. Flags new transactions that are anomalous compared to historical patterns. 3. Logs suspicious transactions to a separate database table for auditor review. 4. Provides an option to send immediate alerts for high- confidence fraud predictions. 5. Handles incomplete transaction records by skipping or imputing. 6. Generates a summary showing: transactions processed, anomalies detected, and alerts sent.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "12.8",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "Visual Quality Control in Manufacturing",
    "description": "Write a Python develop a Python script using TensorFlow or ultralytics YOLO that: 1. Analyzes images of products from a manufacturing line. 2. Uses an object detection model to identify and classify defects, scratches, dents, and misalignment. 3. Stores inspection results image path, defects found in a database. 4. Updates a real-time Grafana dashboard showing defect rates over time. 5. Handles corrupted images by skipping them and triggering a re-capture. 6. Generates a summary showing: products analyzed, defects detected, and dashboard sync status.",
    "tags": [
      "python",
      "sql"
    ]
  },
  {
    "id": "12.9",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "Medical Image Anomaly Detection",
    "description": "Write a Python develop a Python system using TensorFlow and Keras that: 1. Loads medical images, X-rays, and MRIs from a DICOM or directory source. 2. Uses a pre-trained CNN, ResNet, and DenseNet to classify images as normal or anomalous. 3. Stores predictions and confidence scores in a database. 4. Generates a PDF report for doctors summarizing the findings. 5. Handles corrupted DICOM files by skipping them. 6. Generates a summary showing: images processed, anomalies detected, and reports generated.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.10",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "Sales Demand Forecasting",
    "description": "Write a Python system using Prophet or sktime that: 1. Trains a time series forecasting model on historical sales data. 2. Predicts future demand for products for the next N periods. 3. Exports the forecasts with confidence intervals to an Excel file. 4. Handles missing historical data through interpolation. 5. Generates a summary showing: forecasts generated, mean absolute error if history available, and Excel export success.",
    "tags": [
      "python",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.11",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "FAQ Generator from Knowledge Base",
    "description": "Write a Python build a Python script using transformers Q&A generation that: 1. Scans a collection of.txt/docx knowledge base documents. 2. Uses an LLM to generate potential user questions and answers. 3. Saves structured FAQs to a JSON file grouped by topic. 4. Provides an option to export to CSV question, answer. 5. Generates a report showing: total questions, documents processed, and top5 topics. 6. Logs unprocessable files.",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.12",
    "category": "AI & NLP Automation",
    "subcategory": "Predictive & Detection Systems",
    "title": "Fake News Detection System",
    "description": "Write a Python pipeline using transformers that: 1. Trains a text classification model on a dataset of labeled real vs.fake news articles. 2. Applies the model to classify new, incoming articles from a feed. 3. Flags articles predicted as \"fake\" and stores them in a separate database. 4. Handles short or incomplete articles by withholding prediction. 5. Generates a summary showing: articles processed, fake news flagged, and model accuracy. Intelligent Document Processing",
    "tags": [
      "python",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "12.13",
    "category": "AI & NLP Automation",
    "subcategory": "Intelligent Document Processing",
    "title": "Intelligent Document Processing Pipeline",
    "description": "Write a Python create a Python tool using OpenCV, pytesseract, and transformers that: 1. Accepts uploaded PDF or image files, scanned documents. 2. Uses computer vision to detect page boundaries and deskew/crop images. 3. Applies OCR to extract raw text from the processed images. 4. Uses a text classification model to predict document types invoice, contract, and receipt. 5. Stores the extracted text and classification in a structured database. 6. Handles low-resolution images by logging a warning. 7. Generates a summary showing: documents processed, classification results, and OCR word accuracy.",
    "tags": [
      "python",
      "sql",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.14",
    "category": "AI & NLP Automation",
    "subcategory": "Intelligent Document Processing",
    "title": "Intelligent Invoice Data Extraction",
    "description": "Write a Python program using pytesseract and spacy or invoice2data that: 1. Processes invoice PDFs or images. 2. Uses AI-powered OCR and NLP to extract key fields: invoice number, date, vendor, and total amount. 3. Validates extracted data against a schema, date format, and number validation. 4. Stores structured data in a database. 5. Handles unusual layouts with flexible template matching. 6. Generates a summary showing: invoices processed, field extraction accuracy, and successful DB saves.",
    "tags": [
      "python",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "12.15",
    "category": "AI & NLP Automation",
    "subcategory": "Intelligent Document Processing",
    "title": "Automated PowerPoint Generation",
    "description": "Write a Python project using python-pptx and OpenAI that: 1. Accepts structured data, JSON containing key points for a presentation. 2. Uses a generative AI model to expand key points into slide content. 3. Creates a PowerPoint presentation with text and suggests relevant visual layouts. 4. Applies a chosen theme and exports the.pptx file. 5. Handles malformed input data by using defaults. 6. Generates a summary showing: slides created, presentations generated, and export success. Recommender & Clustering Systems",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.16",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "E-commerce Product Recommender System",
    "description": "Write a Python develop a Python project using surprise or TensorFlow Recommenders that: 1. Trains a collaborative filtering model on historical user purchase data. 2. Generates personalized product recommendations for each user. 3. Sends a weekly email to users with their top recommendations. 4. Handles new users' cold start by recommending popular or trending items. 5. Generates a summary showing: users served, recommendation accuracy, click-through rate simulation, and email delivery success rate.",
    "tags": [
      "python",
      "machine learning",
      "cli",
      "email"
    ]
  },
  {
    "id": "12.17",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Social Media Audience Segmentation",
    "description": "Write a Python develop a Python script using scikit-learn that: 1. Fetches social media engagement metrics, likes, shares, and comments via API. 2. Applies clustering K-Means, DBSCAN to segment audiences based on engagement behavior. 3. Generates a dashboard HTML with Plotly showing the different segments and their characteristics. 4. Handles API rate limits and errors gracefully. 5. Generates a summary showing: posts analyzed, clusters identified, and dashboard update status.",
    "tags": [
      "python",
      "api",
      "machine learning"
    ]
  },
  {
    "id": "12.18",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Customer Behavior Clustering",
    "description": "Write a Python develop a Python script using scikit-learn and matplotlib that: 1. Reads customer purchase history from a CSV file. 2. Applies clustering K-Means to segment customers based on purchasing behavior, RFM analysis. 3. Generates a scatter plot visualization of the customer segments. 4. Provides an option to specify the number of clusters. 5. Handles invalid data points by cleaning the input. 6. Generates a summary showing: customers processed, clusters formed, and visualization saved. Advanced AI Applications",
    "tags": [
      "python",
      "csv",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.19",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Facial Recognition Attendance System",
    "description": "Write a Python project using face_recognition or deepface that: 1. Captures video frames from a webcam stream in real- time. 2. Compares detected faces against a database of enrolled employee faces. 3. Logs recognized employees' attendance with a timestamp in a database. 4. Provides a fallback manual entry interface for unrecognized faces. 5. Handles poor lighting conditions by adjusting image preprocessing. 6. Generates a summary showing: attendees processed, recognition accuracy if known, and database update success.",
    "tags": [
      "python",
      "sql",
      "web",
      "machine learning",
      "gui"
    ]
  },
  {
    "id": "12.20",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Machine Learning for WH Optimization",
    "description": "Write a Python create a Python automation pipeline using gym and stable- baselines3 that: 1. Simulates a warehouse environment with robots and packages. 2. Uses a reinforcement learning algorithm, PPO to train robots on efficient pathfinding. 3. Rewards faster delivery and penalizes collisions. 4. Deploys the optimized policy to control simulated robots. 5. Handles sensor noise by adding randomness to the simulation. 6. Generates a summary showing: training episodes run, efficiency improvement over baseline, and successful deployments.",
    "tags": [
      "python",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "12.21",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Synthetic Tabular Data Generation",
    "description": "Write a Python program using synthetic_data or ydata-synthetic that: 1. Accepts a schema definition JSON for the desired tabular data column names, types, and constraints. 2. Uses a generative model, GAN, to create realistic synthetic data that matches the schema. 3. Saves the generated data to a CSV file. 4. Provides options for the dataset size number of rows. 5. Handles schema conflicts, impossible constraints by relaxing them or logging an error. 6. Generates a summary showing: datasets generated, rows created, and constraint compliance.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.22",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "CCTV Motion Event Detection",
    "description": "Write a Python create a Python automation pipeline using OpenCV and imageai that: 1. Processes CCTV video footage. 2. Uses object detection to identify and classify moving objects people, vehicles. 3. Logs events, timestamp, and object type to a database. 4. Provides an option to export short video clips around detected events. 5. Handles poor lighting conditions with image enhancement techniques. 6. Generates a summary showing: videos processed, events detected, and clips exported.",
    "tags": [
      "python",
      "sql",
      "automation",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "12.23",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Multilingual Review Sentiment Analysis",
    "description": "Write a Python create a Python automation project using translators and TextBlob that: 1. Collects product reviews in multiple languages. 2. Translates them to English using a translation API. 3. Performs sentiment analysis on the translated text. 4. Summarizes the overall sentiment per product in a dashboard. 5. Handles unsupported characters or languages by skipping. 6. Generates a summary showing: reviews translated, sentiment accuracy, and dashboard updated.",
    "tags": [
      "python",
      "api",
      "automation"
    ]
  },
  {
    "id": "12.24",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Personalized Marketing Email Generation",
    "description": "Write a Python develop a Python script using openai API that: 1. Reads customer profiles, purchase history, and demographics from a database. 2. Uses a generative language model to create personalized marketing email text for each customer. 3. Sends the emails via SMTP. 4. Has a fallback to a generic template if generation fails. 5. Handles invalid email addresses by skipping. 6. Generates a summary showing: emails generated, emails sent, and generation success rate.",
    "tags": [
      "python",
      "api",
      "sql",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "12.25",
    "category": "AI & NLP Automation",
    "subcategory": "Recommender & Clustering Systems",
    "title": "Multimodal Data Validation",
    "description": "Write a Python create a Python script using transformers and PIL that: 1. Processes datasets containing paired image and text entries, product photo + description. 2. Uses a multimodal AI model to analyze the image and text for consistency. 3. Flags entries where the image content and text description are inconsistent. 4. Stores validated entries in a clean database table. 5. Handles corrupted image or text files by skipping. 6. Generates a summary showing: entries processed, inconsistencies found, and validated dataset size. Summarization & Reporting",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.26",
    "category": "AI & NLP Automation",
    "subcategory": "Summarization & Reporting",
    "title": "Meeting Transcript Summarizer",
    "description": "Write a Python script using transformers and textsum that: 1. Accepts a folder of meeting transcripts in.txt and.docx formats. 2. Automatically detects the language of each transcript using langdetect. 3. Generates a concise summary using a Hugging Face summarization pipeline, with length controlled by a CLI argument length short/medium/long. 4. Saves the output for each file to a JSON file with keys: filename, summary, keywords. 5. Maintains an error log for unreadable or empty files. 6. Generates a final report showing: total transcripts processed, average input word count, and average summary length.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "12.27",
    "category": "AI & NLP Automation",
    "subcategory": "Summarization & Reporting",
    "title": "Meeting Minutes Generator",
    "description": "Write a Python create a Python automation program using Spacy and transformers that: 1. Processes raw meeting transcripts.txt with speaker labels and timestamps. 2. Extracts speaker names, key discussion points, and decisions. 3. Saves structured minutes to a Markdown file. 4. Generates a JSON log with: participant_names, speaking_time, meeting_duration. 5. Handles missing timestamps or inconsistent formatting. 6. Outputs a summary file highlighting the top3 discussion topics.",
    "tags": [
      "python",
      "json",
      "automation",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.28",
    "category": "AI & NLP Automation",
    "subcategory": "Summarization & Reporting",
    "title": "Blog to Tweet Summarizer",
    "description": "Write a Python automation tool using transformers summarization that: 1. Accepts.txt/docx blog posts as input. 2. Generates summaries â¤280 characters using a summarization model. 3. Auto-generates hashtags from keywords using yake or rake-nltk. 4. Saves output to CSV: blog_title, short_summary, hashtags. 5. Generates a JSON log with: posts processed, average summary length, and top hashtags. 6. Logs posts with formatting errors.",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "logging"
    ]
  },
  {
    "id": "12.29",
    "category": "AI & NLP Automation",
    "subcategory": "Summarization & Reporting",
    "title": "Executive Report Summarizer",
    "description": "Write a Python script using transformers and pdfplumber that: 1. Processes.docx/.pdf weekly business reports. 2. Uses an LLM to extract KPIs, sales, and costs and generate a Markdown executive summary. 3. Saves summary to PDF and extracted KPIs to JSON. 4. Generates a report showing: reports processed, average summary length, most frequent KPIs. 5. Handle corrupted files.",
    "tags": [
      "python",
      "json",
      "file processing"
    ]
  },
  {
    "id": "12.30",
    "category": "AI & NLP Automation",
    "subcategory": "Summarization & Reporting",
    "title": "Academic Paper Summarizer",
    "description": "Write a Python program using pdfplumber and transformers that: 1. Processes academic PDFs, extracting abstract, methods, results, and conclusion. 2. Generates structured Markdown summaries with headings. 3. Saves metadata title, authors, and keywords to JSON. 4. Generates a report showing: papers processed, average summary length, and top5 keywords. 5. Handles encrypted/unreadable PDFs. Classification & Extraction",
    "tags": [
      "python",
      "json"
    ]
  },
  {
    "id": "12.31",
    "category": "AI & NLP Automation",
    "subcategory": "Classification & Extraction",
    "title": "Email Action Item Extractor",
    "description": "Write a Python develop a Python automation agent using transformers and email that: 1. Scans a folder of.eml files and parses their content. 2. Uses an instruction-tuned LLM, FLAN-T5 to extract action items: tasks, deadlines, and responsible parties. 3. Saves results to a CSV with columns: subject, task, due_date, and responsible_party. 4. Provides a domain-filter to process only emails from specific sender domains. 5. Handles malformed.eml files gracefully, logging errors. 6. Generates a JSON summary showing: tasks detected, tasks missing deadlines, and the most assigned employees.",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "12.32",
    "category": "AI & NLP Automation",
    "subcategory": "Classification & Extraction",
    "title": "Support Ticket Classifier",
    "description": "Write a Python utility using Spacy or transformers that: 1. Reads customer support tickets from a CSV file with a description column. 2. Classifies each ticket into predefined categories: Billing, Technical, and Inquiry using a text classification pipeline. 3. Appends a category column to the output CSV. 4. Handles empty or unreadable descriptions by skipping and logging. 5. Generates a JSON summary report showing: category counts, percentage distribution, and top3 issue types.",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.33",
    "category": "AI & NLP Automation",
    "subcategory": "Classification & Extraction",
    "title": "Named Entity Recognition NER Extractor",
    "description": "Write a Python utility using Spacy NER pipeline that: 1. Processes.txt files in a folder, extracting entities PERSON, ORG, GPE. 2. Saves results to a CSV with columns: entity, label, context sentence. 3. Generates a JSON summary of the top10 most frequent entities per type. 4. Handles empty files and encoding errors. 5. Final report shows: files processed, total entities, percentage distribution by type.",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing"
    ]
  },
  {
    "id": "12.34",
    "category": "AI & NLP Automation",
    "subcategory": "Classification & Extraction",
    "title": "News Article Keyword Tagger",
    "description": "Write a Python create a Python program using yake or keybert that: 1. Processes.txt news articles in a folder. 2. Extracts keywords/tags for each article. 3. Saves results to a JSON file per article: title, tags, tag_count. 4. Provides an option to export all tags to a master CSV. 5. Generates a report showing: top10 tags, total tags, average tags per article. 6. Logs articles with empty content.",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.35",
    "category": "AI & NLP Automation",
    "subcategory": "Classification & Extraction",
    "title": "Chat Log Intent & Sentiment Analyzer",
    "description": "Write a Python utility using Spacy and transformers that: 1. Processes chat logs from a CSV chat_id, message. 2. Extracts intent, question, complaint, sentiment, and named entities per message. 3. Saves enhanced CSV with: chat_id, intent, sentiment, entities. 4. Handles missing chat IDs. 5. Generates a JSON summary showing: most common intents, sentiment distribution, and top5 entities.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "12.36",
    "category": "AI & NLP Automation",
    "subcategory": "Classification & Extraction",
    "title": "Financial Metric Extractor",
    "description": "Write a Python build a Python utility using regex and Spacy that: 1. Parses unstructured PDF reports for financial metrics, revenue, and profit. 2. Saves results to CSV: report_name, metric, value. 3. Maintains a JSON log of extraction accuracy and missing values. 4. Generates a summary showing: top5 metrics extracted. 5. Handles unreadable PDFs. Chatbots & Assistants",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "12.37",
    "category": "AI & NLP Automation",
    "subcategory": "Chatbots & Assistants",
    "title": "PDF-Based Q&A Chatbot",
    "description": "Write a Python create a Python chatbot using langchain, FAISS, and transformers that: 1. Extracts text from all PDFs in a specified folder. 2. Builds a vector store FAISS from document chunks using sentence-transformers. 3. Accepts natural language questions via the terminal and retrieves relevant document passages. 4. Generates answers with citations to the source PDF and page number. 5. Logs all Q&A sessions to chat_log.json. 6. Generates a summary report showing: questions asked, average response time, and the top5 most cited documents.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "12.38",
    "category": "AI & NLP Automation",
    "subcategory": "Chatbots & Assistants",
    "title": "Customer Complaint Rephraser",
    "description": "Write a Python develop a Python program using transformers instruction-tuning, Tk-Instruct that: 1. Reads a CSV file with a complaint column. 2. Uses an LLM to rephrase each complaint into professional, polite language. 3. Saves a new CSV with original_complaint and standardized_complaint. 4. Handles rows with missing data. 5. Generates a JSON summary showing: complaints rewritten, skipped, and the top5 introduced phrases.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "12.39",
    "category": "AI & NLP Automation",
    "subcategory": "Chatbots & Assistants",
    "title": "HR Policy Chatbot",
    "description": "Write a Python develop a Python chatbot using langchain and transformers that: 1. Loads company policies from JSON files. 2. Builds a vector store for semantic search. 3. Answers employee HR questions via terminal, citing policy sources. 4. Logs queries, responses, and confidence scores to chat_log.json. 5. Generates a summary showing: questions answered, average confidence, and top5 HR topics. 6. Handles missing policy files. Translation & Simplification",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.40",
    "category": "AI & NLP Automation",
    "subcategory": "Translation & Simplification",
    "title": "Multi-Language Document Translator",
    "description": "Write a Python build a Python translation tool using transformers MarianMT or OpenAI that: 1. Processes.txt, .docx, and.pdf files in a folder. 2. Translates content into multiple target languages configurable via a list. 3. Saves each translation as a separate file with a language suffix, _fr. 4. Maintains a JSON log with filename, source_lang, target_langs, and status. 5. Handles encoding errors and unreadable files. 6. Generates a summary showing: total words processed, translations per language, and failed files.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "12.42",
    "category": "AI & NLP Automation",
    "subcategory": "Education & Evaluation",
    "title": "Automated Essay Grader",
    "description": "Write a Python develop a Python script using sentence-transformers and sklearn that: 1. Reads student essays from CSV student_id, essay_text. 2. Compares to model answers using semantic similarity. 3. Assigns a score of 0-10 and generates feedback. 4. Saves results to CSV: student_id, score, feedback. 5. Generates a JSON summary showing: essays graded, average score, and score distribution. 6. Handles missing essays.",
    "tags": [
      "python",
      "csv",
      "json"
    ]
  },
  {
    "id": "12.43",
    "category": "AI & NLP Automation",
    "subcategory": "Education & Evaluation",
    "title": "Study Flashcard Generator",
    "description": "Write a Python create a Python tool using keybert and transformers that: 1. Processes lecture notes.txt/.docx. 2. Extracts key terms and generates Q&A flashcards. 3. Saves to CSV: question, answer. 4. Generates a JSON summary showing: flashcards per file, average answer length, and top10 terms. 5. Handles empty notes.",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing"
    ]
  },
  {
    "id": "12.44",
    "category": "AI & NLP Automation",
    "subcategory": "Education & Evaluation",
    "title": "Q&A Fine-Tuning Dataset Preparation",
    "description": "Write a Python automation tool using transformers and jsonlines that: 1. Reads a CSV of customer Q&A question, answer. 2. Cleans and formats data for LLM fine-tuning. 3. Outputs a JSONL file with and completion fields. 4. Filters inappropriate/incomplete entries. 5. Generates a summary log showing: dataset size, total",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "12.45",
    "category": "AI & NLP Automation",
    "subcategory": "Education & Evaluation",
    "title": "Survey Response Topic Modeler",
    "description": "Write a Python build a Python automation tool using BERTopic that: 1. Accepts survey responses from a CSV response column. 2. Clusters responses into themes using topic modeling. 3. Saves clustered responses to a JSON file. 4. Optionally generates a bar chart of top themes. 5. Generates a summary report showing: clusters identified, average responses per cluster, top words. 6. Handles blank responses by logging. Sentiment & Feedback Analysis",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "12.46",
    "category": "AI & NLP Automation",
    "subcategory": "Sentiment & Feedback Analysis",
    "title": "Customer Emotion Analyzer",
    "description": "Write a Python build a Python program using transformers emotion classification that: 1. Analyzes call transcripts.txt for emotions, anger, joy, etc.. 2. Saves results to JSON: transcript_id, emotions_detected, confidence_scores. 3. Optionally generates emotion trend charts. 4. Generates a report showing: the most common emotions, distribution, and customers with high negative emotions. 5. Handles unreadable files.",
    "tags": [
      "python",
      "json",
      "file processing"
    ]
  },
  {
    "id": "12.47",
    "category": "AI & NLP Automation",
    "subcategory": "Sentiment & Feedback Analysis",
    "title": "Customer Feedback Clustering & Visualization",
    "description": "Write a Python develop a Python script using BERTopic and matplotlib that: 1. Clusters customer feedback CSV feedback_text into themes. 2. Generates word clouds for each cluster. 3. Saves grouped feedback to JSON: theme, examples, keywords. 4. Generates a report showing: top5 clusters, sentiment distribution, and common keywords. 5. Handles corrupted rows.",
    "tags": [
      "python",
      "csv",
      "json"
    ]
  },
  {
    "id": "12.48",
    "category": "AI & NLP Automation",
    "subcategory": "Sentiment & Feedback Analysis",
    "title": "Blog Outline Generator",
    "description": "Write a Python create a Python script using the transformers text generation that: 1. Accepts a topic string via CLI. 2. Uses an LLM to generate a blog outline with sections and subpoints. 3. Saves the outline to a Markdown file. 4. Maintains a JSON log: topic, sections, keywords. 5. Generates a report showing: average outline length, common keywords. 6. Handles inappropriate topics.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "12.49",
    "category": "AI & NLP Automation",
    "subcategory": "Sentiment & Feedback Analysis",
    "title": "Sensitive Information Redactor",
    "description": "Write a Python create a Python tool using Spacy and regex that: 1. Scans.txt files for PII: phones, emails, credit cards. 2. Redacts sensitive info with REDACTED. 3. Saves redacted files with _redacted suffix. 4. Maintains a JSON log of redactions per file: count and type. 5. Generates a summary of the most common PII types detected. 6. Handles encoding errors. Professional Writing Support",
    "tags": [
      "python",
      "json",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "12.50",
    "category": "AI & NLP Automation",
    "subcategory": "Professional Writing Support",
    "title": "Contract Comparison Tool",
    "description": "Write a Python utility using sentence-transformers and python- docx that: 1. Compares two contract versions.docx/.pdf. 2. Identifies semantic differences in clauses beyond text-diff. 3. Saves a.docx with annotated changes. 4. Generates a JSON log of modified clauses with confidence scores. 5. Summary report shows: clauses analyzed, unchanged, significantly altered. 6. Handles unparseable files.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "13.1",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "Telegram Coding Q&A Bot with LLM",
    "description": "Write a Python script that creates a Telegram bot for answering coding questions using a Large Language Model LLM. The workflow should: 1. Connect to Telegram via Bot API. 2. Accept user questions about Python, JavaScript, or SQL. 3. Forward the question to an LLM, OpenAI API or Hugging Face. 4. Return concise, helpful answers in the chat. 5. Save chat logs into JSON with user ID, query, answer, and timestamp. 6. At the end, generate a summary showing the number of questions answered, the most common programming topics, and the average response time.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "13.2",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "Discord Customer Support Chatbot",
    "description": "Write a Python develop a Python chatbot for Discord customer support automation. The script should: 1. Connect to a Discord server via its API. 2. Respond to frequently asked customer questions using an AI FAQ model. 3. Escalate unresolved queries to a human moderator. 4. Log interactions into a JSON file with user ID, query type, and resolution status. 5. At the end, generate a report showing the number of queries handled, the percentage resolved by AI, and the average escalation rate.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "13.3",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "Slack Project Management Bot",
    "description": "Write a Python create a Python Slack bot assistant for project management. The script should: 1. Connect to Slack via API. 2. Monitor project channels for tasks or deadlines. 3. Automatically create Trello/Jira tickets when tasks are mentioned. 4. Save logs into JSON with task description, assigned user, and timestamp. 5. At the end, generate a summary showing the number of tasks captured, boards updated, and common project topics.",
    "tags": [
      "python",
      "api",
      "json",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "13.4",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "WhatsApp Conversational Bot with Twilio",
    "description": "Write a Python create a Python WhatsApp bot using Twilio + AI. The script should: 1. Connect to WhatsApp via Twilio API. 2. Accept incoming messages from users. 3. Use an LLM to provide conversational answers. 4. Send responses back through WhatsApp. 5. Save logs into JSON with user number, message text, response, and timestamp. â¢ At the end, generate a summary showing total messages processed, response rate, and top5 conversation topics.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "13.5",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "Slack Stand-Up Report Bot",
    "description": "Write a Python create a Python Slack stand-up bot with AI summaries. The script should: 1. Ask team members daily questions: \"What did you do yesterday? What will you do today? Any blockers?\" 2. Collect responses in Slack channels. 3. Use an LLM to summarize into a team progress report. 4. Save reports into JSON and post them in a summary channel. 5. At the end, generate a summary showing the number of participants, the average blockers reported, and the progress trends.",
    "tags": [
      "python",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.6",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "Multilingual Chatbot with Language Detection",
    "description": "Write a Python build a Python multilingual chatbot. The script should: 1. Accept user queries in multiple languages. 2. Detect the input language automatically. 3. Use an LLM to translate, process, and respond in the same language. 4. Save interactions into JSON with language, query, response, and confidence score. 5. At the end, generate a summary showing the number of languages detected, the top5 most common languages, and translation accuracy.",
    "tags": [
      "python",
      "json"
    ]
  },
  {
    "id": "13.7",
    "category": "AI Agents & Bots",
    "subcategory": "Messaging & Chatbots",
    "title": "Language Learning Chatbot",
    "description": "Write a Python build a Python language learning chatbot. The script should: 1. Accept conversations in the target language. 2. Provide corrections, translations, and explanations. 3. Track user progress over time, grammar, and vocabulary. 4. Save sessions into JSON with user ID, mistakes, corrections, and progress score. 5. At the end, generate a summary showing total sessions, most improved users, and most common mistakes. Productivity & Assistance",
    "tags": [
      "python",
      "json"
    ]
  },
  {
    "id": "13.8",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "AI-Powered Email Reply Assistant",
    "description": "Write a Python create a Python email assistant that drafts replies with AI. The workflow should: 1. Connect to an IMAP inbox. 2. Fetch unread emails and classify them as support, inquiry, or personal. 3. Use an LLM to draft suggested replies. 4. Save drafts into an email_drafts folder for manual approval. 5. Log results into JSON with subject, category, draft length, and timestamp. 6. At the end, generate a summary showing the number of drafts created, categories covered, and average response time.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "email",
      "logging"
    ]
  },
  {
    "id": "13.9",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "Daily News Summarization Agent",
    "description": "Write a Python daily news agent that scrapes the web, summarizes articles, and emails results. The script should: 1. Scrape news headlines from multiple websites. 2. Extract article text and summarize using an LLM or Hugging Face model. 3. Compile a digest into a.pdf or.html report. 4. Email the digest every morning to subscribers. 5. Save logs into JSON with article count, sources, and summary length. 6. At the end, generate a summary showing total articles processed, average summary size, and most common news sources.",
    "tags": [
      "python",
      "json",
      "web",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "13.10",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "LinkedIn Networking Assistant Bot",
    "description": "Write a Python LinkedIn bot that assists in networking. The script should: 1. Fetch connection requests and messages. 2. Suggest AI-powered personalized responses. 3. Draft connection notes based on shared interests. 4. Save logs into JSON with recipient, suggested reply, and acceptance status. 5. At the end, generate a report showing the number of connections processed, acceptance rates, and the most common AI reply templates.",
    "tags": [
      "python",
      "requests",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "13.11",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "Google Calendar Scheduling Bot",
    "description": "Write a Python create a Python calendar scheduling bot. The script should: 1. Connect to Google Calendar API. 2. Parse meeting requests from email or chat. 3. Use AI to propose optimal meeting times. 4. Automatically create calendar events. 5. Save logs into JSON with meeting title, participants, and status. 6. At the end, generate a summary showing the number of events scheduled, the most common time slots, and the conflicts resolved.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.12",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "AI Travel Itinerary Planning Bot",
    "description": "Write a Python develop a Python AI travel assistant bot. The workflow should: 1. Accept trip preferences, destination, budget, and duration from a chat interface. 2. Query APIs for flights, hotels, and attractions. 3. Use AI to generate optimized itineraries. 4. Save itineraries into.pdf or.html format. 5. Log requests into JSON with user, trip details, and itinerary file path. 6. At the end, generate a summary showing the number of itineraries generated, average trip length, and the most popular destinations.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.13",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "AI Shopping Assistant Bot",
    "description": "Write a Python AI shopping assistant bot. The script should: 1. Accept product requests from users via Telegram or Discord. 2. Scrape e-commerce sites for prices and reviews. 3. Summarize findings and provide recommendations. 4. Save results into JSON with product, price range, rating, and recommendation. 5. At the end, generate a report showing the number of products researched, the best deals found, and the most common categories.",
    "tags": [
      "python",
      "requests",
      "json",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "13.14",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "Healthcare Symptom Assistant Bot",
    "description": "Write a Python create a Python healthcare assistant bot. The workflow should: 1. Accept patient symptoms via chat or form. 2. Use a medical knowledge model to suggest possible conditions, not diagnoses. 3. Recommend whether to seek professional care. 4. Save logs into JSON with symptoms, suggested conditions, and urgency rating. 5. At the end, generate a summary showing the number of symptom reports processed, the most frequent conditions flagged, and referral rates.",
    "tags": [
      "python",
      "json",
      "logging"
    ]
  },
  {
    "id": "13.15",
    "category": "AI Agents & Bots",
    "subcategory": "Productivity & Assistance",
    "title": "E-Learning Assistant Bot",
    "description": "Write a Python build a Python e-learning assistant bot. The script should: 1. Accept user queries about course content. 2. Generate summaries, flashcards, and quiz questions with AI. 3. Track student performance over time. 4. Save interactions into JSON with topic, summary, quiz score, and progress. 5. At the end, generate a summary showing total study sessions, average quiz scores, and the most challenging subjects. Business & Enterprise",
    "tags": [
      "python",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.16",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "Resume Screening and Ranking Agent",
    "description": "Write a Python build a Python resume screening agent that extracts skills and ranks candidates. The script should: 1. Accept resumes in.pdf or.docx. 2. Use NLP to extract skills, experience, and education. 3. Match candidates against job requirements. 4. Rank resumes by suitability score. 5. Save results into JSON with candidate name, score, and extracted skills. 6. At the end, generate a summary showing the number of resumes screened, the top5 ranked candidates, and the most common missing skills.",
    "tags": [
      "python",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.17",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "GitHub Issue & PR Assistant Bot",
    "description": "Write a Python build a Python GitHub assistant bot. The workflow should: 1. Monitor repositories for issues and pull requests. 2. Use AI to draft helpful responses or code review summaries. 3. Automatically assign reviewers for new pull requests. 4. Save actions into JSON with repo name, issue ID, and action taken. 5. At the end, generate a summary showing the number of repos monitored, actions taken, and AI responses accepted by maintainers.",
    "tags": [
      "python",
      "requests",
      "json",
      "machine learning",
      "monitoring"
    ]
  },
  {
    "id": "13.18",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "Research Assistant Bot for Academic Papers",
    "description": "Write a Python build a Python research assistant bot. The workflow should: 1. Accept research queries via CLI or chat. 2. Scrape academic websites or APIs, arXiv, PubMed. 3. Summarize papers using an LLM. 4. Compile results into a.pdf or.md report. 5. Save logs into JSON with query, number of papers fetched, and summary length. 6. At the end, generate a summary showing queries processed, average papers per query, and most frequent research fields.",
    "tags": [
      "python",
      "api",
      "json",
      "web",
      "scraping"
    ]
  },
  {
    "id": "13.19",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "Reddit AI-Powered Comment Bot",
    "description": "Write a Python Reddit bot powered by AI. The script should: 1. Monitor chosen subreddits for questions or discussions. 2. Generate helpful AI-powered responses. 3. Post comments using Reddit API. 4. Save actions into JSON with subreddit, post ID, response text, and upvotes. 5. At the end, generate a summary showing the number of responses posted, upvote averages, and the most discussed topics. Specialized AI Assistants",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "monitoring"
    ]
  },
  {
    "id": "13.20",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "Voice Assistant Bot with Speech-to-Text",
    "description": "Write a Python develop a Python voice assistant bot using Speech-to-Text + AI. The workflow should: 1. Record audio commands from a microphone. 2. Transcribe with a Speech-to-Text API. 3. Send queries to an LLM for answers or task execution. 4. Reply with synthesized speech Text-to-Speech API. 5. Save interaction history into JSON with transcript, response, and audio file path. 6. At the end, generate a summary showing the number of commands processed, average transcription accuracy, and common requests.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.21",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "Twitter/X Engagement Bot with AI",
    "description": "Write a Python Twitter/X bot that engages with users using AI. The script should: 1. Fetch tweets mentioning specific keywords. 2. Use an LLM to generate context-aware replies. 3. Post responses automatically via Twitter API. 4. Save logs into JSON with tweet ID, reply content, and engagement metrics. 5. At the end, generate a summary showing number of tweets processed, replies posted, and most engaged topics.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "13.22",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "AI Tutor Bot for Learning Support",
    "description": "Write a Python develop a Python AI tutor bot for learning support. The script should: 1. Accept student questions from a web form or chat interface. 2. Use an LLM to provide detailed explanations. 3. Generate practice questions and quizzes automatically. 4. Save interactions into JSON with question, explanation, quiz generated, and timestamp. 5. At the end, generate a summary showing number of students helped, topics covered, and quiz accuracy rates.",
    "tags": [
      "python",
      "json",
      "web",
      "machine learning",
      "gui"
    ]
  },
  {
    "id": "13.23",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "AI Fitness Coach Bot",
    "description": "Write a Python fitness coach bot powered by AI. The workflow should: 1. Accept user fitness goals, current stats, and preferences via chat. 2. Generate personalized workout and diet plans with an AI model. 3. Provide daily motivational messages and tips. 4. Save plans into JSON with user ID, workout schedule, and nutrition advice. 5. At the end, generate a summary showing the number of users coached, the most common fitness goals, and plan adherence rates.",
    "tags": [
      "python",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "13.24",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "AI Mental Health Companion Bot",
    "description": "Write a Python develop a Python AI mental health companion bot. The script should: 1. Chat empathetically with users about stress, anxiety, or mood. 2. Use sentiment analysis to detect emotional tone. 3. Provide resources such as relaxation exercises or mental health hotlines. 4. Save logs into JSON with user mood, session length, and resources suggested. 5. At the end, generate a summary showing the number of conversations, the most common moods detected, and the resources most often recommended.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "13.25",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "AI Content Creation Bot",
    "description": "Write a Python create a Python content creation bot. The workflow should: 1. Accept a topic from the user, \"Machine Learning Basics\". 2. Use an LLM to generate structured blog outlines and draft articles. 3. Export results into.md or.docx format. 4. Save content logs into JSON with topic, word count, and quality score. 5. At the end, generate a summary showing the number of articles generated, the average length, and the most popular topics.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "13.26",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "News Verification Bot",
    "description": "Write a Python develop a Python news verification bot. The script should: 1. Accept URLs or text snippets from users. 2. Scrape sources and cross-check facts against trusted APIs. 3. Use AI to detect potential misinformation. 4. Save findings into JSON with claim, verification result, and confidence level. 5. At the end, generate a summary showing the number of claims verified, the accuracy rate, and the most common misinformation topics.",
    "tags": [
      "python",
      "api",
      "json",
      "scraping",
      "machine learning"
    ]
  },
  {
    "id": "13.27",
    "category": "AI Agents & Bots",
    "subcategory": "Business & Enterprise",
    "title": "LinkedIn Post Draft Generator",
    "description": "Write a Python create a Python automation script using transformers that: 1. Reads company reports.docx/.pdf. 2. Uses an LLM to extract highlights and generate engaging LinkedIn post drafts. 3. Saves output to CSV: report_title, post_draft, hashtags. 4. Generates a JSON summary showing: reports processed, average post length, and common hashtags. 5. Handles corrupted files. Advanced & Multi-Agent Systems",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "13.28",
    "category": "AI Agents & Bots",
    "subcategory": "Advanced & Multi-Agent Systems",
    "title": "Multi-Agent Bot Orchestration",
    "description": "Write a Python develop a Python multi-agent system bot. The script should: 1. Orchestrate multiple specialized agents, summarizer, researcher, writer. 2. Assign tasks to each agent sequentially. 3. Combine results into a single coherent output. 4. Save workflow logs into JSON with agent name, task, and output length. 5. At the end, generate a summary showing the number of agents used, the average task completion time, and the quality rating of the combined output.",
    "tags": [
      "python",
      "json",
      "logging"
    ]
  },
  {
    "id": "14.1",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "Port Scanner with Service Fingerprinting",
    "description": "Write a Python script using socket and python-nmap that: 1. Performs a TCP port scan on a target host or network range. 2. Identifies open ports and attempts to fingerprint the running service and version. 3. Stores results IP, port, service, version in a CSV file. 4. Provides CLI options to adjust scanning speed -T timing template and port range -p. 5. Handles unreachable hosts and timeouts gracefully. 6. Generates a summary showing: hosts scanned, open ports detected, and service fingerprint accuracy.",
    "tags": [
      "python",
      "csv",
      "cli",
      "file processing"
    ]
  },
  {
    "id": "14.2",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "Web Vulnerability Header Scanner",
    "description": "Write a Python script using requests and beautifulsoup4 that: 1. Scans a list of URLs for common web vulnerabilities in HTTP headers. 2. Checks for missing security headers X-Content-Type- Options, HSTS, insecure cookies HttpOnly, Secure. 3. Logs potential weaknesses with severity levels. 4. Handles timeouts and SSL errors gracefully. 5. Generates a summary showing: sites scanned, vulnerabilities detected, and report generation status.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "web",
      "logging"
    ]
  },
  {
    "id": "14.3",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "SQL Injection Vulnerability Tester",
    "description": "Write a Python create a Python automation project using requests and bs4 that: 1. Tests web forms and URL parameters for SQL injection vulnerabilities. 2. Sends a series of crafted payloads, ' OR '1'='1 and analyzes responses for errors or anomalies. 3. Logs vulnerable endpoints, payloads used, and response snippets. 4. Provides an option to generate a detailed PDF report for auditors. 5. Handles WAF blocks and unexpected HTTP responses gracefully. 6. Generates a summary showing: forms tested, vulnerabilities found, and report exported.",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "sql",
      "web"
    ]
  },
  {
    "id": "14.4",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "Cross-Site Scripting XSS Scanner",
    "description": "Write a Python develop a Python script using Selenium and requests that: 1. Injects XSS test payloads into all input fields of a web application. 2. Detects successful injections by analyzing responses for reflected payloads. 3. Captures screenshots of the browser DOM when a potential XSS is found. 4. Handles JavaScript-heavy applications and AJAX calls gracefully. 5. Generates a summary showing: inputs tested, XSS vulnerabilities found, and evidence captured.",
    "tags": [
      "python",
      "requests",
      "selenium",
      "web",
      "testing"
    ]
  },
  {
    "id": "14.5",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "SSL/TLS Configuration Scanner",
    "description": "Write a Python develop a Python tool using sslyze or cryptography that: 1. Tests SSL/TLS configurations for a list of domains. 2. Checks for expired certificates, weak ciphers, and insecure protocols SSLv2, SSLv3. 3. Generates remediation suggestions for each finding. 4. Handles self-signed certificates and connection failures gracefully. 5. Generates a summary showing: domains tested, misconfigurations found, and compliance report generated.",
    "tags": [
      "python",
      "machine learning",
      "testing",
      "security"
    ]
  },
  {
    "id": "14.6",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "Web Login Brute-Force Tool",
    "description": "Write a Python develop a Python tool using mechanize or requests with sessions that: 1. Automates brute-force attacks against web login forms. 2. Submits passwords from a wordlist while maintaining session cookies. 3. Captures and stores successful session cookies for further access. 4. Implements rate-limiting and random user-agents. 5. Handles CAPTCHA and IP blocks gracefully. 6. Generates a summary showing: accounts tested, successes found, and cookies stored securely.",
    "tags": [
      "python",
      "requests",
      "web",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.7",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "SQL Injection Log Detector",
    "description": "Write a Python automation system using re and tail that: 1. Monitors web server access logs in real-time. 2. Parses log entries to detect potential SQL injection patterns in query strings or POST data. 3. Sends alerts to administrators upon detection. 4. Handles large and rotating log files efficiently. 5. Generates a summary showing: logs scanned, injection attempts detected, and alerts sent.",
    "tags": [
      "python",
      "sql",
      "web",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.8",
    "category": "Cybersecurity Automation",
    "subcategory": "Scanning & Vulnerability Testing",
    "title": "CSRF Vulnerability Tester",
    "description": "Write a Python develop a Python script using requests and beautifulsoup4 that: 1. Tests endpoints for CSRF vulnerabilities by checking for anti-CSRF tokens. 2. Attempts to submit requests without tokens or with forged tokens. 3. Logs endpoints that are potentially vulnerable. 4. Exports findings to an HTML report. 5. Handles session timeouts and redirects gracefully. 6. Generates a summary showing: endpoints tested, CSRF issues found, and report exported. Authentication & Password Security",
    "tags": [
      "python",
      "requests",
      "beautifulsoup",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "14.9",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Password Security",
    "title": "Password Strength Analyzer",
    "description": "Write a Python develop a Python utility using passlib or custom rules that: 1. Accepts a list of username: password pairs from a file or database. 2. Checks passwords against common weak patterns, dictionary words, sequences, and repetitions. 3. Validates compliance with complexity policies, min length, mixed case, numbers, symbols. 4. Flags risky accounts and outputs results to a secure, encrypted file. 5. Handles malformed input lines gracefully. 6. Generates a summary showing: accounts analyzed, weak passwords found, and results stored securely.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "file processing",
      "security"
    ]
  },
  {
    "id": "14.10",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Password Security",
    "title": "SSH Brute-Force Tool For Authorized Testing",
    "description": "Write a Python create a Python program using Paramiko that: 1. Attempts SSH authentication against a target host using a username and wordlist file. 2. Logs successful credentials to an encrypted file with timestamps. 3. Implements rate-limiting delay between attempts to avoid lockouts or detection. 4. Handles connection errors, timeouts, and host key verification gracefully. 5. Generates a summary showing: attempts made, successful logins found, and secure logging status.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging",
      "security"
    ]
  },
  {
    "id": "14.11",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Password Security",
    "title": "Password Spraying Simulator",
    "description": "Write a Python develop a Python program using requests that: 1. Performs a password spraying attack against a web application or service. 2. Tests a few common passwords, Welcome123, Password1 against many usernames. 3. Uses throttling and random delays to avoid account lockouts. 4. Logs successful authentications to a secure file. 5. Handles CAPTCHA and login failures gracefully. 6. Generates a summary showing: accounts tested, successful logins, and secure logging status.",
    "tags": [
      "python",
      "requests",
      "web",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "14.12",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Password Security",
    "title": "Brute-Force Attempt Detector",
    "description": "Write a Python develop a Python program using re and datetime that: 1. Parses authentication logs /var/log/auth.log, Security.evtx via Winevt. 2. Detects multiple failed login attempts from the same IP address. 3. Automatically blocks the offending IP using iptables or Windows Firewall. 4. Handles log rotation and different log formats gracefully. 5. Generates a summary showing: log entries analyzed, IPs blocked, and processing errors. Reconnaissance Tools",
    "tags": [
      "python",
      "machine learning",
      "logging",
      "security"
    ]
  },
  {
    "id": "14.13",
    "category": "Cybersecurity Automation",
    "subcategory": "Reconnaissance Tools",
    "title": "WHOIS Information Gatherer",
    "description": "Write a Python develop a Python tool using python-whois that: 1. Performs WHOIS lookups for a list of domain names. 2. Extracts key information: registrar, creation/expiration dates, name servers, and registrant info. 3. Stores structured results in a SQLite database for trend analysis. 4. Handles private registrations and rate-limiting by WHOIS servers gracefully. 5. Generates a summary showing: domains queried, records retrieved, and database storage success.",
    "tags": [
      "python",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "14.14",
    "category": "Cybersecurity Automation",
    "subcategory": "Reconnaissance Tools",
    "title": "Web Directory Brute-Forcer",
    "description": "Write a Python project using requests and concurrent.futures that: 1. Tests for common and hidden directories on a web server using a wordlist. 2. Logs discovered paths200,403 status codes and their response sizes. 3. Provides rate-limiting to avoid overwhelming the server. 4. Handles connection resets and timeouts gracefully. 5. Generates a summary showing: paths tested, valid directories found, and results saved.",
    "tags": [
      "python",
      "requests",
      "web",
      "logging",
      "testing"
    ]
  },
  {
    "id": "14.15",
    "category": "Cybersecurity Automation",
    "subcategory": "Reconnaissance Tools",
    "title": "DNS Enumeration Tool",
    "description": "Write a Python script using dnspython that: 1. Performs DNS reconnaissance on a target domain. 2. Enumerates subdomains using a wordlist and DNS queries. 3. Attempts a zone transfer AXFR from the primary DNS server. 4. Exports results A, AAAA, MX, TXT records to JSON or CSV. 5. Handles DNS timeouts and refused zone transfers gracefully. 6. Generates a summary showing: records discovered, zone transfers attempted, and export success.",
    "tags": [
      "python",
      "csv",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "14.16",
    "category": "Cybersecurity Automation",
    "subcategory": "Reconnaissance Tools",
    "title": "Wi-Fi Network Scanner",
    "description": "Write a Python script using Scapy that: 1. Scans for nearby Wi-Fi networks by listening for beacon frames. 2. Collects SSIDs, BSSIDs, signal strength, and encryption types WEP, WPA, WPA2. 3. Exports results to a CSV file. 4. Handles missing monitor mode support gracefully. 5. Generates a summary showing: networks detected, open networks found, and CSV saved.",
    "tags": [
      "python",
      "csv",
      "file processing",
      "monitoring"
    ]
  },
  {
    "id": "14.17",
    "category": "Cybersecurity Automation",
    "subcategory": "Reconnaissance Tools",
    "title": "Dark Web Monitoring Tool",
    "description": "Write a Python script using tor and requests with stems that: 1. Connects to the Tor network to access dark web forums and marketplaces. 2. Searches for specific keywords related to compromised data or threats. 3. Stores findings in an encrypted database. 4. Handles connection failures and CAPTCHA gracefully. 5. Generates a summary showing: searches performed, matches found, and data stored.",
    "tags": [
      "python",
      "requests",
      "sql",
      "web",
      "machine learning"
    ]
  },
  {
    "id": "14.18",
    "category": "Cybersecurity Automation",
    "subcategory": "Reconnaissance Tools",
    "title": "Shodan Internet Device Scanner",
    "description": "Write a Python create a Python automation pipeline using Shodan API that: 1. Queries Shodan for devices with specific filters, port:22, vuln: CVE-2014-0160. 2. Collects IP, port, banner, and vulnerability information for each result. 3. Stores results in a PostgreSQL database for further analysis. 4. Handles API rate limits and quota exceeded errors gracefully. 5. Generates a summary showing: devices found, vulnerabilities identified, and database import success. Monitoring & Detection",
    "tags": [
      "python",
      "api",
      "sql",
      "automation",
      "monitoring"
    ]
  },
  {
    "id": "14.19",
    "category": "Cybersecurity Automation",
    "subcategory": "Monitoring & Detection",
    "title": "File Integrity Monitor FIM",
    "description": "Write a Python program using hashlib and watchdog that: 1. Calculates cryptographic hashes, SHA-256 of critical system files and directories. 2. Monitors files for changes by comparing current hashes to a known baseline. 3. Sends immediate alerts via email, syslog upon detection of unauthorized changes. 4. Handles locked files and permission errors gracefully. 5. Generates a summary showing: files monitored, changes detected, and alerts delivered.",
    "tags": [
      "python",
      "machine learning",
      "email",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "14.20",
    "category": "Cybersecurity Automation",
    "subcategory": "Monitoring & Detection",
    "title": "Network Packet Sniffer",
    "description": "Write a Python create a Python utility using Scapy that: 1. Captures network packets on a specified interface. 2. Filters traffic by protocol, TCP, UDP, ICMP or port. 3. Detects suspicious patterns, port scans, and malformed packets. 4. Saves packets to a PCAP file for offline analysis. 5. Handles permission errors and requires root gracefully. 6. Generates a summary showing: packets captured, alerts flagged, and PCAP saved.",
    "tags": [
      "python",
      "gui",
      "file processing"
    ]
  },
  {
    "id": "14.21",
    "category": "Cybersecurity Automation",
    "subcategory": "Monitoring & Detection",
    "title": "ARP Spoofing Detector",
    "description": "Write a Python utility using Scapy that: 1. Monitors the local network ARP table continuously. 2. Detects ARP spoofing by identifying duplicate IP addresses with different MAC addresses. 3. Triggers an alert print to screen, log file upon detection. 4. Handles false positives in dynamic environments gracefully. 5. Generates a summary showing: hosts monitored, anomalies detected, and alerts triggered.",
    "tags": [
      "python",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "14.22",
    "category": "Cybersecurity Automation",
    "subcategory": "Monitoring & Detection",
    "title": "Leaked Credential Checker",
    "description": "Write a Python create a Python automation system using requests and re that: 1. Monitors public paste sites and repositories for leaked credentials. 2. Searches for patterns like email addresses, API keys, or specific usernames. 3. Sends alerts to a Slack channel or email upon detection. 4. Handles site downtime and access restrictions gracefully. 5. Generates a summary showing: searches performed, leaks detected, and alerts delivered.",
    "tags": [
      "python",
      "requests",
      "api",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.24",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "Malware Hash Lookup Utility",
    "description": "Write a Python create a Python script using Virustotal3 or requests that: 1. Calculates file hashes MD5, SHA-1, SHA-256 of local files. 2. Queries the VirusTotal API to check for known malware signatures. 3. Caches results to avoid re-checking the same hash. 4. Handles API rate limits4 requests/min gracefully. 5. Generates a summary showing: files scanned, malicious hits, and results reported.",
    "tags": [
      "python",
      "requests",
      "api",
      "file processing"
    ]
  },
  {
    "id": "14.25",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "Email Header Analyzer",
    "description": "Write a Python create a Python pipeline using the email library that: 1. Parses email files EML or raw headers. 2. Extracts and analyzes headers for signs of spoofing Received-SPF, DKIM. 3. Flags anomalies such as mismatched From domains or suspicious routing. 4. Stores results in a SQL database for forensic analysis. 5. Handles malformed headers gracefully. 6. Generates a summary showing: emails analyzed, anomalies found, and database storage success.",
    "tags": [
      "python",
      "sql",
      "machine learning",
      "email",
      "file processing"
    ]
  },
  {
    "id": "14.26",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "Phishing URL Detector",
    "description": "Write a Python create a Python tool using requests and tldextract that: 1. Analyzes URLs for phishing indicators: suspicious domains, IP addresses, known bad patterns. 2. Uses a machine learning model, from phishing emails to classify URLs. 3. Stores flagged URLs in a SQLite database for review. 4. Handles invalid URLs and connection timeouts gracefully. 5. Generates a summary showing: URLs analyzed, phishing detected, and database updated.",
    "tags": [
      "python",
      "requests",
      "sql",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "14.27",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "Security Baseline Auditor",
    "description": "Write a Python script using subprocess and platform that: 1. Audits a system against CIS security benchmarks. 2. Checks firewall rules, password policies, software versions, and unnecessary services. 3. Generates a compliance report with pass/fail status and remediation steps. 4. Handles permission errors and OS differences gracefully. 5. Generates a summary showing: checks performed, compliance percentage, and report generated.",
    "tags": [
      "python",
      "machine learning",
      "security"
    ]
  },
  {
    "id": "14.28",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "Ransomware Recovery Simulator",
    "description": "Write a Python develop a Python utility using cryptography that: 1. Simulates a ransomware attack in an isolated test directory by encrypting files. 2. Tests the backup restoration process from a backup location. 3. Logs the success or failure of file recovery. 4. Handles file permission errors during the simulation gracefully. 5. Generates a summary showing: files encrypted, recovery success rate, and backup validation.",
    "tags": [
      "python",
      "machine learning",
      "file processing",
      "logging",
      "testing"
    ]
  },
  {
    "id": "14.29",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "IDS/IPS Testing via Packet Injection",
    "description": "Write a Python create a Python program using Scapy that: 1. Crafts and injects malicious packets, SQL injection in payload, port scan patterns. 2. Monitors the network for IDS/IPS alerts triggered by the injected traffic. 3. Generates a report of which attacks were detected and which evaded detection. 4. Handles network interface errors gracefully. 5. Generates a summary showing: packets injected, alerts triggered, and report generated.",
    "tags": [
      "python",
      "sql",
      "gui",
      "monitoring"
    ]
  },
  {
    "id": "14.30",
    "category": "Cybersecurity Automation",
    "subcategory": "Security Testing & Response",
    "title": "Penetration Testing Report Generator",
    "description": "Write a Python develop a Python program using Jinja2 and WeasyPrint that: 1. Collects findings from various security tools, CSV, and JSON outputs. 2. Structures vulnerabilities by severity: Critical, High, Medium, Low. 3. Generates a professional PDF penetration test report with an executive summary and remediation steps. 4. Handles missing data and formatting errors gracefully. 5. Generates a summary showing: findings documented, reports generated, and PDF output success. Vulnerability & Security Monitoring",
    "tags": [
      "python",
      "csv",
      "json",
      "monitoring",
      "testing"
    ]
  },
  {
    "id": "14.31",
    "category": "Cybersecurity Automation",
    "subcategory": "Vulnerability & Security Monitoring",
    "title": "Web Application Vulnerability Scanning",
    "description": "Write a Python script that automates vulnerability scanning of web applications. The workflow should: 1. Accept a list of URLs from a CSV file. 2. Use an open-source scanner, subprocess call to OWASP ZAP or Nmap to detect common vulnerabilities. 3. Parse results into structured JSON format with keys: url, vulnerability_type, severity, timestamp. 4. Save scan results into a database for historical tracking. 5. At the end, generate a summary report showing total URLs scanned, the number of vulnerabilities found, and the severity distribution.",
    "tags": [
      "python",
      "csv",
      "json",
      "sql",
      "web"
    ]
  },
  {
    "id": "14.32",
    "category": "Cybersecurity Automation",
    "subcategory": "Vulnerability & Security Monitoring",
    "title": "Container Vulnerability Scanning",
    "description": "Write a Python create a Python script that automates container vulnerability scanning. The workflow should: 1. List all Docker images running on a host. 2. Scan them for vulnerabilities using tools like Trivy or Clair. 3. Save scan results into a JSON file with image name, CVE identifiers, and severity. 4. At the end, generate a report showing the number of images scanned, total vulnerabilities detected, and severity distribution.",
    "tags": [
      "python",
      "docker",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.33",
    "category": "Cybersecurity Automation",
    "subcategory": "Vulnerability & Security Monitoring",
    "title": "API Security Testing",
    "description": "Write a Python program that automates API security testing. The workflow should: 1. Accept API endpoints from a CSV file. 2. Send test requests with fuzzed or invalid inputs. 3. Detect vulnerabilities such as missing authentication or improper error handling. 4. Save test results into JSON with endpoint, issue type, and response code. 5. At the end, generate a summary showing number of APIs tested, vulnerabilities detected, and most common issue types. Authentication & Access Control",
    "tags": [
      "python",
      "requests",
      "api",
      "csv",
      "json"
    ]
  },
  {
    "id": "14.34",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "SSH Login Attempt Monitoring and IP Blocking",
    "description": "Write a Python develop a Python script that monitors SSH login attempts and blocks suspicious IPs. The program should: 1. Parse system logs /var/log/auth.log in real time. 2. Identify repeated failed login attempts within a threshold,5 attempts in10 minutes. 3. Automatically add offending IPs to a firewall blocklist, using iptables or ufw. 4. Save incidents into a JSON log with IP address, number of attempts, and timestamp. 5. At the end, generate a summary showing total attempts detected, IPs blocked, and the most common attack origins.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "14.35",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Automated API Key and Password Rotation",
    "description": "Write a Python script that rotates API keys and passwords automatically. The workflow should: 1. Fetch a list of credentials from a secure vault, AWS Secrets Manager or HashiCorp Vault. 2. Generate new API keys or passwords. 3. Update them in applications and services. 4. Log all changes into a JSON file with service name, old key expiration date, and new key details masked. 5. At the end, generate a summary showing the number of credentials rotated, average key age, and the failures encountered.",
    "tags": [
      "python",
      "aws",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "14.36",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Cloud IAM Auditing",
    "description": "Write a Python develop a Python automation tool for cloud IAM auditing. The script should: 1. Connect to AWS or GCP IAM services. 2. List all users, roles, and permissions. 3. Detect over-privileged accounts. 4. Save findings into a JSON file with user ID, role, risky permissions, and timestamp. 5. At the end, generate a summary showing the number of accounts scanned, excessive privileges found, and recommended actions.",
    "tags": [
      "python",
      "aws",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "14.37",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Kubernetes Secrets Rotation",
    "description": "Write a Python develop a Python automation tool for DevOps secrets rotation in Kubernetes. The script should: 1. Connect to a Kubernetes cluster via Python client. 2. Identify secrets DB passwords, API keys. 3. Rotate them periodically with new values. 4. Update dependent pods to use the new secrets. 5. Save rotation logs into JSON with secret name, old expiry, new expiry, and status. 6. At the end, generate a report showing the number of secrets rotated, pods updated, and failures. DevOps & Infrastructure Monitoring",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.38",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "CI/CD Failure Notifications to Slack/Discord",
    "description": "Write a Python create a Python automation program that sends CI/CD notifications to Slack or Discord when builds fail. The script should: 1. Connect to Jenkins/GitHub Actions API. 2. Fetch recent build statuses. 3. Detect failed builds and format notification messages. 4. Send alerts into a Slack/Discord channel via webhooks. 5. Save notifications into a JSON log with build ID, project name, status, and alert timestamp. 6. At the end, generate a report showing total builds monitored, failures detected, and notifications sent.",
    "tags": [
      "python",
      "api",
      "json",
      "web",
      "automation"
    ]
  },
  {
    "id": "14.39",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Docker Container Health Monitoring",
    "description": "Write a Python build a Python script that monitors Docker containers and restarts unhealthy ones. The script should: 1. Connect to Docker Engine via Python SDK docker-py. 2. Check container health statuses periodically. 3. Restart containers marked as unhealthy. 4. Save logs into JSON with container ID, name, health check result, and restart timestamp. 5. At the end, generate a summary showing the number of containers monitored, restarts performed, and the most common failure types.",
    "tags": [
      "python",
      "docker",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "14.40",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Kubernetes Cluster Health Monitoring",
    "description": "Write a Python build a Python program that monitors Kubernetes cluster health. The script should: 1. Connect to a cluster using the Kubernetes Python client. 2. Fetch pod, node, and service statuses. 3. Detect failed pods or nodes under high resource usage. 4. Save health checks into a JSON log with resource name, status, and timestamp. 5. At the end, generate a summary showing cluster uptime, number of failed pods detected, and the top3 nodes with the highest CPU usage.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "cli",
      "logging"
    ]
  },
  {
    "id": "14.41",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Deployment Verification with Smoke Tests",
    "description": "Write a Python build a Python workflow that performs deployment verification. The script should: 1. Run smoke tests after each deployment: API availability, login, and sample queries. 2. Validate response times and status codes. 3. Save results into a JSON file with endpoint, status, latency, and pass/fail status. 4. At the end, generate a report showing the number of tests executed, success rate, and the slowest endpoints.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "14.42",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "Infrastructure Monitoring via Cloud APIs",
    "description": "Write a Python develop a Python script that performs infrastructure monitoring via APIs. The workflow should: 1. Query cloud APIs AWS CloudWatch, Azure Monitor, GCP Stackdriver. 2. Fetch metrics like CPU, memory, and disk utilization. 3. Detect threshold violations and trigger alerts. 4. Save monitoring data into a JSON file with instance ID, metric name, value, and timestamp. 5. At the end, generate a summary showing number of instances monitored, alerts triggered, and top3 most common issues.",
    "tags": [
      "python",
      "aws",
      "api",
      "json",
      "file processing"
    ]
  },
  {
    "id": "14.43",
    "category": "Cybersecurity Automation",
    "subcategory": "Authentication & Access Control",
    "title": "CI/CD Pipeline Auditing",
    "description": "Write a Python build a Python program that performs CI/CD pipeline auditing. The script should: 1. Connect to Jenkins/GitLab pipelines. 2. Check configurations for insecure practices, plain-text secrets, and missing tests. 3. Export findings into JSON with pipeline ID, issue type, and severity. 4. At the end, generate a report showing the number of pipelines audited, insecure configurations found, and the compliance score. Logging & Integrity",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging",
      "testing"
    ]
  },
  {
    "id": "14.44",
    "category": "Cybersecurity Automation",
    "subcategory": "Logging & Integrity",
    "title": "Log Integrity Checking",
    "description": "Write a Python automation tool that performs log integrity checks. The program should: 1. Monitor log files, /var/log/syslog. 2. Generate cryptographic hashes SHA-256 for log segments. 3. Store hash values in a secure JSON ledger. 4. Detect unauthorized changes to logs by comparing new hashes with stored ones. 5. At the end, generate a summary showing the number of log files tracked, the integrity violations detected, and the timestamps of anomalies.",
    "tags": [
      "python",
      "json",
      "automation",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "14.45",
    "category": "Cybersecurity Automation",
    "subcategory": "Logging & Integrity",
    "title": "Log-Based Intrusion Detection",
    "description": "Write a Python develop a Python script that automates log-based intrusion detection. The program should: 1. Monitor system logs for suspicious patterns, SQL injection attempts, and brute force attacks. 2. Use regex rules to detect anomalies. 3. Trigger alerts via email or Slack when matches are found. 4. Save detections into a JSON file with log line, rule matched, and timestamp. 5. At the end, generate a report showing the number of alerts triggered, most common attack patterns, and the top offending IPs.",
    "tags": [
      "python",
      "json",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.46",
    "category": "Cybersecurity Automation",
    "subcategory": "Logging & Integrity",
    "title": "Configuration Drift Detection",
    "description": "Write a Python create a Python script that automates configuration drift detection. The script should: 1. Take baseline configuration files, Nginx, Apache, and SSH. 2. Compare with current system configurations. 3. Highlight differences in settings, port changes, and disabled security options. 4. Save drift results into a JSON log with file name, baseline value, and current value. 5. At the end, generate a summary showing the number of files checked, drifts detected, and severity levels.",
    "tags": [
      "python",
      "json",
      "automation",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "14.47",
    "category": "Cybersecurity Automation",
    "subcategory": "Logging & Integrity",
    "title": "Log Aggregation for DevOps Monitoring",
    "description": "Write a Python create a Python script that performs log aggregation for DevOps monitoring. The workflow should: 1. Collect logs from multiple servers via SSH. 2. Store logs in a centralized database or ElasticSearch. 3. Tag logs by server and service name. 4. Save aggregation metadata into JSON with file sizes, server IDs, and collection time. 5. At the end, generate a summary showing total logs ingested, top error sources, and storage volume.",
    "tags": [
      "python",
      "json",
      "sql",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "14.48",
    "category": "Cybersecurity Automation",
    "subcategory": "Logging & Integrity",
    "title": "SIEM Data Enrichment",
    "description": "Write a Python create a Python script that automates SIEM data enrichment. The workflow should: 1. Fetch raw security logs from ElasticSearch or Splunk. 2. Enrich with geo-IP and WHOIS data. 3. Append enrichment details back into the dataset. 4. Save enriched logs into JSON with IP, geo-location, and organization. 5. At the end, generate a summary showing the number of logs enriched, the top geographic sources, and the most frequent organizations. Backup & Compliance",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "14.50",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "Automated Patch Management",
    "description": "Write a Python automation tool for patch management. The script should: 1. Check installed software versions on a server. 2. Compare with the latest available versions via vendor APIs. 3. Generate a patching plan. 4. Optionally trigger updates for outdated packages. 5. Save results into a JSON file with package name, current version, latest version, and update status. 6. At the end, generate a summary showing the number of packages checked, updates applied, and pending patches.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.51",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "GitHub Repository Security Checks",
    "description": "Write a Python automation tool for GitHub repository security checks. The script should: 1. Scan repositories for secrets API keys, tokens, and credentials. 2. Detect usage of insecure dependencies in requirements.txt or package.json. 3. Save findings into a JSON file with repo name, issue type, and severity. 4. At the end, generate a summary showing the number of repos scanned, secrets detected, and dependency risks found.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "14.52",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "Threat Intelligence Feed Integration",
    "description": "Write a Python script that automates threat intelligence feeds integration. The workflow should: 1. Fetch IPs, domains, and file hashes from threat intelligence APIs, AlienVault OTX. 2. Compare them against local firewall or IDS logs. 3. Flag matches as potential threats. 4. Save flagged queries into JSON with threat type, matched log entry, and risk level. 5. At the end, generate a report showing total indicators checked, matches found, and severity breakdown.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.53",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "Automated Incident Ticketing",
    "description": "Write a Python develop a Python automation tool for automated incident ticketing. The script should: 1. Monitor logs for security incidents failed logins, anomalies. 2. Create incident tickets in Jira or ServiceNow via API. 3. Include evidence such as log snippets or screenshots. 4. Save ticket metadata into JSON with incident type, ticket ID, and status. 5. At the end, generate a summary showing number of incidents detected, tickets created, and resolution rates.",
    "tags": [
      "python",
      "api",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "14.54",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "DNS Anomaly Monitoring",
    "description": "Write a Python create a Python script that performs DNS monitoring for anomalies. The script should: 1. Capture DNS queries from logs. 2. Detect suspicious domains typosquatting, newly registered. 3. Cross-check domains against blocklists. 4. Save flagged queries into JSON with domain, query source, and risk level. 5. At the end, generate a report showing number of queries analyzed, anomalies detected, and most common risky domains.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "14.55",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "System Process Malware Monitoring",
    "description": "Write a Python create a Python script that monitors system processes for malware indicators. The program should: 1. Periodically list running processes with psutil. 2. Compare against a blocklist of known malicious process names or hashes. 3. Kill suspicious processes automatically. 4. Save findings into JSON with process ID, name, action taken, and timestamp. 5. At the end, generate a summary showing number of processes scanned, suspicious detections, and actions executed.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "monitoring"
    ]
  },
  {
    "id": "14.56",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "System Resource Hardening Checks",
    "description": "Write a Python script that automates system resource hardening checks. The workflow should: 1. Verify kernel parameters, disable IP forwarding. 2. Check file permissions on sensitive files /etc/passwd, /etc/shadow. 3. Detect world-writable files. 4. Save results into a JSON log with check name, pass/fail status, and timestamp. 5. At the end, generate a summary showing total checks performed, pass rate, and top failed checks.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "14.57",
    "category": "Cybersecurity Automation",
    "subcategory": "Backup & Compliance",
    "title": "Continuous Compliance Monitoring",
    "description": "Write a Python develop a Python automation system for continuous compliance monitoring. The script should: 1. Periodically check infrastructure against compliance frameworks CIS, ISO27001. 2. Validate password policies, encryption standards, and access controls. 3. Save compliance results into JSON with control ID, status, and severity. 4. At the end, generate a report showing compliance percentage, top failing controls, and recommended remediations.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "monitoring"
    ]
  },
  {
    "id": "15.1",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Web & Form Automation",
    "title": "Government Form Automation with Playwright",
    "description": "Write a Python automation script that fills out government forms in a browser using Playwright. The program should: 1. Accept user data from a CSV file: name, address, and ID number. 2. Automatically open the government website, navigate to the form, and fill in fields. 3. Upload required documents from a local folder. 4. Submit the form and download confirmation receipts. 5. Save logs in a JSON file showing which records were processed successfully and which failed. 6. At the end, generate a summary report with total forms submitted, success rate, and reasons for failed submissions.",
    "tags": [
      "python",
      "csv",
      "json",
      "web",
      "automation"
    ]
  },
  {
    "id": "15.2",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Web & Form Automation",
    "title": "Web Scraping and Supplier Form Submission",
    "description": "Write a Python create a Python script that automates web scraping + form submission in a single workflow. The script should: 1. Scrape product details from an e-commerce website. 2. Fill in a supplier order form automatically using Playwright. 3. Upload scraped data as an Excel sheet into the form. 4. Submit the order and save confirmation receipts. 5. Provide logs in a JSON file with product counts, success rates, and error messages.At the end, generate a report showing total items ordered, suppliers contacted, and the number of failed submissions.",
    "tags": [
      "python",
      "api",
      "json",
      "web",
      "scraping"
    ]
  },
  {
    "id": "15.3",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Web & Form Automation",
    "title": "Web Form Testing for QA Teams",
    "description": "Write a Python script that automates web form testing for QA teams. The workflow should: 1. Accept a list of test input values from a CSV file. 2. Use Playwright or Selenium to submit them to a web form. 3. Record whether submission succeeded, failed, or triggered validation errors. 4. Save results into a CSV with input_value, status, and screenshot_path. 5. Provide a JSON log summarizing the number of tests executed, pass/fail counts, and the most common errors encountered. Desktop & System Automation",
    "tags": [
      "python",
      "selenium",
      "csv",
      "json",
      "web"
    ]
  },
  {
    "id": "15.4",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Desktop & System Automation",
    "title": "Desktop App Control with Pywinauto/PyAutoGUI",
    "description": "Write a Python create a Python script that controls desktop applications using Pywinauto or PyAutoGUI. The script should: 1. Launch a Windows desktop application, Notepad, Calculator, or a custom business app. 2. Perform keystrokes, mouse clicks, and menu navigation automatically. 3. Save screenshots after completing each step. 4. Log every action into a desktop_automation_log.json. 5. Provide error handling for applications not found or windows that fail to load. 6. At the end, generate a summary showing runtime, number of actions performed, and failures encountered.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "cli"
    ]
  },
  {
    "id": "15.5",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Desktop & System Automation",
    "title": "Legacy ERP Data Entry Automation",
    "description": "Write a Python tool that automates repetitive data entry tasks into a legacy ERP desktop system.Use Pywinauto to: 1. Read structured input data from a CSV file. 2. Open the ERP system and navigate through windows. 3. Enter customer information, orders, or invoices. 4. Save a screenshot of each completed record. 5. Log all activities into erp_automation_log.json.Provide error handling for failed entries, and generate a summary showing how many records were successfully added, skipped, or retried.",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "15.6",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Desktop & System Automation",
    "title": "File Synchronization Between Systems",
    "description": "Write a Python create a Python automation script that handles file synchronization between systems. The script should: 1. Monitor a source folder for new files. 2. Copy new or updated files to a target location, local or cloud storage. 3. Keep a JSON log of changes, including file name, size, timestamp, and action taken. 4. At the end, generate a report showing total files synchronized, skipped, or failed, along with transfer success rates.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "15.7",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Desktop & System Automation",
    "title": "Database Backup Automation",
    "description": "Write a Python workflow automation script that performs database backups. The script should: 1. Connect to MySQL or PostgreSQL. 2. Export backups in.sql format. 3. Compress and store them in a backup folder with timestamped filenames. 4. Optionally upload backups to cloud storage AWS S3, Google Cloud. 5. Save logs into db_backup_log.json with backup size, duration, and location. 6. At the end, generate a summary showing total backups created, average size, and any failed attempts.",
    "tags": [
      "python",
      "aws",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "15.8",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Desktop & System Automation",
    "title": "Data Migration Between Systems",
    "description": "Write a Python develop a Python RPA script that automates data migration between systems. The workflow should: 1. Extract data from a legacy system CSV/SQL dump. 2. Transform it into the schema of the new system. 3. Load it into the target database via API or direct connection. 4. Validate the migrated data for completeness. 5. Maintain a JSON log showing total records processed, transformation rules applied, and validation errors. 6. At the end, generate a report showing migration success rate, skipped records, and overall runtime. Workflow Orchestration",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "15.9",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Workflow Orchestration",
    "title": "Daily API Data Workflow with Apache Airflow",
    "description": "Write a Python develop a Python automation workflow integrated with Apache Airflow. The DAG should: 1. Run a daily task that extracts CSV data from an API. 2. Transform the data, clean missing values, and normalize columns. 3. Load results into a PostgreSQL database. 4. Trigger a notification email once the workflow completes. 5. Provide an option to retry failed steps automatically.Log metadata into Airflowâs task logs, and also generate a separate JSON summary with task durations and failure counts.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "15.10",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Workflow Orchestration",
    "title": "Multi-Step Data Pipeline with Prefect",
    "description": "Write a Python script that uses Prefect to orchestrate a multi-step pipeline. The workflow should: 1. Step1: Download raw data files from an FTP server. 2. Step2: Clean and validate data with Pandas. 3. Step3: Store validated data into a local SQLite database. 4. Step4: Generate a summary report and email it to stakeholders. 5. The script should log each stepâs status into Prefectâs UI, and also generate a JSON summary file for archival.Provide error handling for failed FTP downloads or corrupted data files.",
    "tags": [
      "python",
      "pandas",
      "json",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "15.11",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Workflow Orchestration",
    "title": "Multi-API Integration Workflow",
    "description": "Write a Python develop a Python script that orchestrates multi-API integration workflows. The workflow should: 1. Fetch customer orders from an e-commerce API. 2. Validate and enrich orders with shipping rates from a logistics API. 3. Push updated records into a CRM system via REST API. 4. Notify stakeholders with an email once all records are processed. 5. Generate logs in a JSON file showing the number of orders processed, average processing time, and failed API calls. 6. Provide retries for failed API requests and a summary report of all integrations.",
    "tags": [
      "python",
      "requests",
      "api",
      "json",
      "machine learning"
    ]
  },
  {
    "id": "15.12",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Workflow Orchestration",
    "title": "Jira/Trello Task Automation and Reporting",
    "description": "Write a Python build a Python automation workflow that integrates with Jira or Trello APIs. The script should: 1. Pull all open tasks from a project board. 2. Sort them by priority and due date. 3. Export results into a CSV report with task details. 4. Send the report via email to stakeholders. 5. Generate logs in a JSON file with the number of tasks fetched, completed, overdue, and pending.Provide retries for failed API calls and summarize the most common assignees.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "automation"
    ]
  },
  {
    "id": "15.13",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Workflow Orchestration",
    "title": "Browser-Based Dashboard Report Automation",
    "description": "Write a Python develop a Python script that automates browser-based reporting dashboards. The workflow should: 1. Log into a reporting platform with secure credentials. 2. Navigate to specific dashboards. 3. Export reports as.csv or.pdf. 4. Email the reports to stakeholders daily. 5. Save logs into a JSON file with the number of dashboards exported, runtime per dashboard, and any errors encountered.Provide error handling for timeouts or login failures. Document & File Automation",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "15.14",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Document & File Automation",
    "title": "Weekly Excel Report and Email Automation",
    "description": "Write a Python build a Python automation program that generates weekly reports by combining Excel data and email automation. The script should: 1. Open multiple Excel files from a folder and merge sheets into a single Pandas DataFrame. 2. Generate pivot tables and charts summarizing key metrics. 3. Save the report as a.xlsx and.pdf. 4. Automatically email the report to a predefined distribution list using SMTP. 5. Provide error handling for missing Excel files or invalid formats. 6. At the end, create a JSON summary showing total reports sent, recipients, and report generation time.",
    "tags": [
      "python",
      "pandas",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "15.15",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Document & File Automation",
    "title": "Invoice Processing with Robocorp and OCR",
    "description": "Write a Python script that uses Robocorp or RPA Framework to handle invoice processing. The workflow should: 1. Download invoices from a company email inbox. 2. Extract invoice data using OCR, Tesseract. 3. Validate totals and due dates. 4. Insert results into an Excel sheet or a database. 5. Notify the accounting team once processing is complete. 6. Save logs into a JSON file showing invoice counts, validation errors, and extracted totals. 7. Generate a report with total invoices processed, success rate, and exceptions encountered.",
    "tags": [
      "python",
      "json",
      "sql",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "15.16",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Document & File Automation",
    "title": "Batch PDF Watermarking and Archiving",
    "description": "Write a Python automation script that automates batch PDF watermarking and archiving. The script should: 1. Accept a folder of PDF files. 2. Apply a watermark, âConfidentialâ or a company logo to each page. 3. Move processed files into a separate âArchivedâ folder. 4. Generate a log file showing which PDFs were watermarked and archived. 5. Save logs into pdf_watermark_log.json. 6. At the end, generate a summary report showing total PDFs processed, the number skipped due to encryption or corruption, and the average processing time.",
    "tags": [
      "python",
      "json",
      "automation",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "15.17",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Document & File Automation",
    "title": "Daily Excel File Cleanup for Financial Teams",
    "description": "Write a Python develop a Python program that automates daily Excel file cleanup for financial teams. The script should: 1. Open Excel workbooks from a specified folder. 2. Remove duplicate rows, fill missing values with placeholders, and standardize column names. 3. Save cleaned versions in a new folder with filenames suffixed cleaned.xlsx. 4. Log all actions: duplicates removed, missing values filled, errors. 5. Provide a JSON summary showing the number of Excel files processed, rows cleaned, and total time taken.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "15.18",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Document & File Automation",
    "title": "Document Approval Management",
    "description": "Write a Python automation script that manages document approvals. The script should: 1. Monitor a shared folder for new Word/PDF files. 2. Send them to designated reviewers via email. 3. Track approval responses in a CSV file. 4. Notify stakeholders once approvals are complete. 5. Maintain a JSON log with document name, reviewer, approval status, and timestamps. 6. At the end, generate a summary showing total documents processed, approval rates, and average turnaround time.",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "15.19",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Document & File Automation",
    "title": "Expense Report Validation with OCR",
    "description": "Write a Python create a Python tool that automates expense report validation. The script should: 1. Read receipts from a folder of scanned PDFs/images. 2. Extract values using OCR. 3. Match receipts against submitted claims in an Excel file. 4. Flag mismatches and generate a validation report. 5. Log activity into expense_validation_log.json with receipt ID, extracted amount, claimed amount, and match status. 6. At the end, generate a summary showing total receipts processed, mismatches found, and average OCR accuracy. Monitoring & Alerts",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "15.20",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Monitoring & Alerts",
    "title": "Daily Website Monitoring and Alerts",
    "description": "Write a Python build a Python RPA program that schedules daily website monitoring tasks. The script should: 1. Open a list of websites from a CSV file. 2. Take screenshots of each homepage at specific times. 3. Log response times and HTTP status codes. 4. Send alerts via Slack/Discord if a site is down. 5. Save all results into a JSON file with timestamps, response times, and site statuses. 6. At the end, generate a report showing uptime percentages for each website monitored during the day.",
    "tags": [
      "python",
      "csv",
      "json",
      "web",
      "machine learning"
    ]
  },
  {
    "id": "15.21",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Monitoring & Alerts",
    "title": "Shared Folder Invoice Monitoring",
    "description": "Write a Python create a Python RPA script that monitors a shared network folder for invoices. The workflow should: 1. Detect new PDF invoices placed in the folder. 2. Extract invoice numbers, totals, and due dates using OCR. 3. Enter details into an Excel sheet or database. 4. Send alerts for overdue invoices via email or Slack. 5. Maintain a JSON log showing invoices processed, validation errors, and extraction accuracy. 6. At the end, generate a report showing the total invoices handled, overdue amounts, and exceptions.",
    "tags": [
      "python",
      "json",
      "sql",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "15.22",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Monitoring & Alerts",
    "title": "Stock Market Data Monitoring and Alerts",
    "description": "Write a Python automation script that monitors stock market data and generates alerts. The script should: 1. Pull live stock prices from a financial API. 2. Compare them against predefined thresholds from a CSV file. 3. Send alerts via email, Slack, or Telegram when thresholds are crossed. 4. Save historical data into a database for trend analysis. 5. Log all activity into a JSON file with stock symbol, trigger condition, and alert status. 6. At the end, generate a report showing the number of alerts triggered, stocks monitored, and average response time.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "15.23",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Monitoring & Alerts",
    "title": "Cloud Resource Cleanup Automation",
    "description": "Write a Python develop a Python workflow that automates cloud resource cleanup. The script should: 1. Connect to AWS/GCP/Azure using SDKs. 2. Identify unused resources, stopped VMs, orphaned volumes, and idle load balancers. 3. Generate a cost savings report in Excel. 4. Optionally delete unused resources after confirmation. 5. Save logs into cloud_cleanup_log.json with resource type, status, and estimated savings. 6. At the end, generate a summary showing total resources scanned, cleaned, and the cost saved.",
    "tags": [
      "python",
      "aws",
      "json",
      "automation",
      "logging"
    ]
  },
  {
    "id": "15.24",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Monitoring & Alerts",
    "title": "Real-Time Manufacturing Equipment Monitoring",
    "description": "Write a Python develop a Python automation program that performs real-time monitoring of manufacturing equipment. The script should: 1. Connect to IoT sensors via APIs or MQTT. 2. Collect metrics such as temperature, vibration, and uptime. 3. Trigger alerts if thresholds are exceeded. 4. Store data into a time-series database. 5. Save logs into a JSON file with sensor ID, metric, and status. 6. At the end, generate a summary report showing equipment uptime, average values, and the number of alerts triggered.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "15.25",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Monitoring & Alerts",
    "title": "Compliance Audit Automation",
    "description": "Write a Python create a Python workflow that automates compliance audits. The script should: 1. Scan system logs for policy violations. 2. Generate structured findings in a CSV file with columns system, violation_type, and timestamp. 3. Email the audit report to compliance officers. 4. Save logs into compliance_audit_log.json with the number of violations detected and severity levels. 5. At the end, generate a summary showing systems scanned, violations detected, and the top3 violation categories. Communication & Social Media",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "15.26",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Communication & Social Media",
    "title": "Social Media Posting Automation",
    "description": "Write a Python build a Python RPA script that automates social media posting. The program should: 1. Read post drafts and scheduled times from a CSV file. 2. Log into social platforms Twitter/X, LinkedIn, Facebook via APIs. 3. Publish posts at scheduled times. 4. Log results into a JSON file showing post ID, platform, timestamp, and status. 5. At the end, generate a summary showing total posts published, failures, and the most common posting times.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "automation"
    ]
  },
  {
    "id": "15.27",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Communication & Social Media",
    "title": "Bulk Email Response Automation",
    "description": "Write a Python develop a Python automation program that handles bulk email responses. The script should: 1. Fetch unread emails from an IMAP inbox. 2. Categorize them into topics, support, billing, and sales using NLP. 3. Send auto-replies from pre-written templates. 4. Move processed emails into respective folders. 5. Log actions into a JSON file with email ID, category, response status, and timestamp.Provide error handling for invalid or blocked addresses. 6. At the end, generate a report showing the number of auto- replies sent, emails skipped, and the most common categories.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "15.28",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Communication & Social Media",
    "title": "HR Onboarding Task Automation",
    "description": "Write a Python build a Python RPA tool that automates HR onboarding tasks. The workflow should: 1. Read new employee details from a CSV file. 2. Create accounts in Active Directory or company systems via APIs. 3. Generate a welcome email and send credentials. 4. Assign onboarding tasks to Jira/Trello automatically. 5. Log actions into a JSON file showing employee ID, systems updated, and status. 6. Provide error handling for failed account creations. 7. At the end, generate a summary report with the number of employees onboarded and systems updated.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "automation"
    ]
  },
  {
    "id": "15.29",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Communication & Social Media",
    "title": "Customer Subscription Renewal Automation",
    "description": "Write a Python develop a Python workflow that automates customer subscription renewals. The script should: 1. Fetch expiring subscriptions from a database. 2. Generate renewal invoices in PDF format. 3. Send them via email with payment links. 4. Update the database once payment confirmation is received via API. 5. Maintain a JSON log with subscription ID, invoice number, email status, and payment confirmation. 6. At the end, generate a report showing total renewals processed, payments received, and outstanding invoices.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "15.30",
    "category": "RPA & Workflow Orchestration",
    "subcategory": "Communication & Social Media",
    "title": "Multi-Language Website Testing",
    "description": "Write a Python build a Python RPA script that automates multi-language website testing. The workflow should: 1. Use Playwright to navigate a multilingual website. 2. Capture screenshots of each page in different language settings. 3. Verify that translations are loaded correctly. 4. Log issues such as missing or untranslated text. 5. Save logs into a JSON file with page URL, language code, and error details. 6. At the end, generate a report showing pages tested, languages covered, and error counts.",
    "tags": [
      "python",
      "json",
      "web",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.1",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "API ETL Pipeline with PostgreSQL",
    "description": "Write a Python ETL automation script that extracts data from APIs in both CSV and JSON formats, transforms it with Pandas, and loads it into a PostgreSQL database. The script should: 1. Fetch multiple datasets via REST API. 2. Normalize column names and handle missing values. 3. Load results into database tables with proper schema. 4. Log every dataset processed into a JSON file with record counts and status. 5. At the end, generate a report showing the number of datasets ingested, rows processed, and failed API calls.",
    "tags": [
      "python",
      "pandas",
      "api",
      "csv",
      "json"
    ]
  },
  {
    "id": "16.2",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "Multi-Format Data Ingestion",
    "description": "Write a Python develop a Python script that automates data ingestion from multiple file formats. The workflow should: 1. Accept CSV, JSON, and XML files from an input folder. 2. Convert all into a unified schema. 3. Load cleaned data into a SQLite database. 4. Save ingestion logs into a JSON file with file type, rows inserted, and errors. 5. At the end, generate a summary showing the number of files processed by type, total rows ingested, and failure rates.",
    "tags": [
      "python",
      "csv",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "16.3",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "Real-Time Streaming Data Pipeline",
    "description": "Write a Python pipeline that processes real-time streaming data. The script should: 1. Connect to a Kafka or RabbitMQ stream. 2. Parse incoming JSON messages. 3. Transform and store results into a database. 4. Save raw and processed messages into log files. 5. Maintain a JSON summary with message counts, processing speed, and error counts.At the end, generate a report showing throughput, most common error types, and lag times.",
    "tags": [
      "python",
      "json",
      "sql",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "16.4",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "Batch ETL Jobs with Airflow",
    "description": "Write a Python script that schedules batch ETL jobs with Airflow. The workflow should: 1. Extract sales data from an API. 2. Transform the data, clean currency formats, and convert dates. 3. Load results into a MySQL database. 4. Retry failed jobs automatically. 5. Save DAG run logs into JSON with task IDs, duration, and retries. 6. At the end, generate a summary showing job success rate, average runtime, and the most common failure step.",
    "tags": [
      "python",
      "api",
      "json",
      "sql",
      "machine learning"
    ]
  },
  {
    "id": "16.5",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "Multi-Source Data Integration",
    "description": "Write a Python build a Python automation workflow for multi-source data integration. The script should: 1. Fetch data from APIs, CSVs, and databases. 2. Normalize schemas into a unified data model. 3. Merge datasets with conflict resolution rules. 4. Save integrated results into a warehouse table. 5. Log integration steps into JSON with source, row counts, and conflicts resolved. 6. At the end, generate a summary showing the number of sources merged, conflicts detected, and the final dataset size.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "16.6",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "Multi-Step ETL with Prefect",
    "description": "Write a Python create a Python script that orchestrates multi-step ETL pipelines with Prefect. The workflow should: 1. Step1: Extract sales data from an API. 2. Step2: Transform into a standardized schema. 3. Step3: Load results into Snowflake. 4. Step4: Generate a report and notify stakeholders. 5. Track all steps in Prefect UI and also log execution details into JSON. 6. At the end, generate a summary showing total pipelines run, success/failure counts, and average runtime.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "16.7",
    "category": "Data Pipelines & Big Data",
    "subcategory": "ETL & Data Pipelines",
    "title": "End-to-End Spark Batch Processing",
    "description": "Write a Python develop a Python automation program that executes end-to-end batch processing with Spark. The script should: 1. Load raw data from S3. 2. Run transformations and aggregations on Spark clusters. 3. Save outputs into a Hive or Delta Lake table. 4. Generate quality checks on the results. 5. Save job logs into JSON with cluster ID, runtime, and error details.At the end, generate a summary showing rows processed, partitions used, job duration, and pass/fail status of quality checks. Data Cleaning & Processing",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "16.8",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Cleaning & Processing",
    "title": "Large CSV Batch Processing",
    "description": "Write a Python develop a Python program that processes large CSV files in batches for scalability. The script should: 1. Accept CSV files larger than1 GB. 2. Split them into smaller chunks,100k rows each. 3. Process each chunk independently with Pandas. 4. Append results into a consolidated database table. 5. Log every chunk into a JSON file with row count, chunk index, and runtime. 6. At the end, generate a summary showing total rows processed, average chunk size, and number of failed chunks.",
    "tags": [
      "python",
      "pandas",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "16.9",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Cleaning & Processing",
    "title": "Inconsistent Excel Sheet Cleanup",
    "description": "Write a Python create a Python automation workflow that cleans inconsistent Excel sheets. The script should: 1. Read all Excel files from a folder. 2. Standardize headers, formats, and data types. 3. Handle duplicates and missing values. 4. Save cleaned versions into a new folder with cleaned.xlsx suffix. 5. Log every file processed into a JSON file with the number of rows cleaned, duplicates removed, and errors fixed. 6. At the end, generate a summary showing total Excel files processed, average cleanup time, and most common issues.",
    "tags": [
      "python",
      "json",
      "automation",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "16.10",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Cleaning & Processing",
    "title": "Incremental Database Ingestion",
    "description": "Write a Python pipeline that performs incremental database ingestion. The program should: 1. Connect to a source database. 2. Extract only new or updated rows using timestamp columns. 3. Append changes into a target database table. 4. Maintain a change-tracking JSON log with row counts and timestamps. 5. At the end, generate a summary showing total rows inserted, skipped duplicates, and average ingestion time.",
    "tags": [
      "python",
      "json",
      "sql",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "16.11",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Cleaning & Processing",
    "title": "Data Deduplication for Large CSVs",
    "description": "Write a Python build a Python automation tool for data deduplication. The script should: 1. Accept large CSV datasets. 2. Detect and remove duplicate rows using composite keys. 3. Save deduplicated files into a new folder. 4. Log deduplication stats into a JSON file, duplicates removed, and unique count. 5. At the end, generate a summary showing the percentage of duplicates removed and the most common duplicate patterns. Big Data & Warehousing",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "file processing"
    ]
  },
  {
    "id": "16.12",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Big Data & Warehousing",
    "title": "Apache Spark Job Scheduling",
    "description": "Write a Python script that schedules Apache Spark jobs to process millions of rows. The program should: 1. Use PySpark to read large datasets from HDFS or S3. 2. Apply transformations, filtering, joins, and aggregations. 3. Save results into a Parquet dataset. 4. Schedule the job to run daily via cron or Airflow. 5. Maintain a JSON log with dataset size, job duration, and errors.At the end, generate a summary showing the number of rows processed, partitions used, and runtime trends.",
    "tags": [
      "python",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "16.13",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Big Data & Warehousing",
    "title": "Data Warehouse Loading with Snowflake/Redshift",
    "description": "Write a Python create a Python program that automates data warehouse loading. The script should: 1. Extract data from transactional databases. 2. Apply transformations with Pandas or Dask. 3. Load results into a Snowflake or Redshift data warehouse. 4. Log load history into a JSON file with timestamps, row counts, and status. 5. At the end, generate a summary showing the number of tables loaded, rows ingested, and runtime efficiency.",
    "tags": [
      "python",
      "pandas",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "16.14",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Big Data & Warehousing",
    "title": "Data Lake Ingestion with S3",
    "description": "Write a Python create a Python workflow for data lake ingestion. The script should: 1. Accept raw files from multiple sources: CSV, Parquet, JSON. 2. Store them in an S3-based data lake under partitioned folders by date/source. 3. Generate metadata for each file size, schema, and ingestion time. 4. Save metadata into a JSON catalog. 5. At the end, generate a summary showing the number of files ingested, partition sizes, and the top3 largest files. Reporting & Analytics",
    "tags": [
      "python",
      "csv",
      "json",
      "file processing",
      "logging"
    ]
  },
  {
    "id": "16.15",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Reporting & Analytics",
    "title": "Daily PostgreSQL Report Generation",
    "description": "Write a Python build a Python automation program that generates daily reports from PostgreSQL. The script should: 1. Run SQL queries every morning. 2. Export results into Excel and PDF reports. 3. Email reports to stakeholders with SMTP. 4. Archive old reports into a dated folder. 5. Log each execution into a JSON file with query time, row counts, and delivery status. 6. At the end, generate a summary showing the number of reports generated, recipients notified, and failed deliveries.",
    "tags": [
      "python",
      "json",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.16",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Reporting & Analytics",
    "title": "Multi-API Aggregated Reporting",
    "description": "Write a Python workflow that generates aggregated reports from multiple APIs. The script should: 1. Pull financial, marketing, and sales data from different APIs. 2. Normalize and merge into a single dataset. 3. Generate a dashboard report with charts. 4. Export to.pdf and.html. 5. Maintain a JSON log with API name, rows ingested, and merge status. 6. At the end, generate a summary showing the number of APIs integrated, the total rows processed, and the most common data fields.",
    "tags": [
      "python",
      "api",
      "json",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "16.17",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Reporting & Analytics",
    "title": "Daily Log Ingestion and Analysis",
    "description": "Write a Python develop a Python script that automates daily log ingestion and analysis. The script should: 1. Read system/application logs from a folder. 2. Parse logs into structured JSON format: timestamp, level, message. 3. Store parsed logs into a database for querying. 4. Generate daily error trend charts. 5. Save logs into a JSON file with file count, error distribution, and runtime. 6. At the end, generate a report showing total logs ingested, the top5 most frequent error types, and peak error times.",
    "tags": [
      "python",
      "json",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.18",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Reporting & Analytics",
    "title": "Machine Learning Feature Engineering",
    "description": "Write a Python develop a Python pipeline for machine learning feature engineering. The workflow should: 1. Ingest raw transaction data from a database. 2. Generate derived features: rolling averages, ratios, and categorical encodings. 3. Save processed data into a feature store, Feast. 4. Log feature creation into JSON with feature names, transformations applied, and record counts. 5. At the end, generate a summary showing the number of features created, the dataset size, and the average runtime per feature. Data Security & Compliance",
    "tags": [
      "python",
      "json",
      "sql",
      "machine learning",
      "logging"
    ]
  },
  {
    "id": "16.19",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Security & Compliance",
    "title": "Scheduled Data Validation",
    "description": "Write a Python build a Python automation tool that performs scheduled data validation. The script should: 1. Connect to a production database. 2. Run validation rules for missing values, invalid ranges, and duplicates. 3. Generate a data quality report in Excel. 4. Notify stakeholders with an email if thresholds are violated. 5. Save quality check logs into a JSON file with rules applied, error counts, and pass/fail status. 6. At the end, generate a summary showing total checks run, percentage passed, and the top5 recurring issues.",
    "tags": [
      "python",
      "json",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.20",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Security & Compliance",
    "title": "Data Anonymization for Privacy",
    "description": "Write a Python develop a Python automation tool for data anonymization for privacy. The script should: 1. Read sensitive customer data from a database. 2. Mask or anonymize fields like name, email, and phone number. 3. Save anonymized data into a new secure database. 4. Log field-level transformations into JSON for audit purposes. 5. At the end, generate a summary showing the number of records anonymized, fields affected, and runtime.",
    "tags": [
      "python",
      "json",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.21",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Security & Compliance",
    "title": "Data Archival for Compliance",
    "description": "Write a Python build a Python script that automates data archival for compliance. The workflow should: 1. Identify records older than a given date in a database. 2. Export them into compressed CSV or Parquet files. 3. Store them in an archive folder with timestamped filenames. 4. Save archival metadata into JSON with row count, file size, and status. 5. At the end, generate a summary showing the number of records archived, average file size, and the most common archival period.",
    "tags": [
      "python",
      "csv",
      "json",
      "sql",
      "automation"
    ]
  },
  {
    "id": "16.22",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Security & Compliance",
    "title": "Scheduled Data Quality Checks",
    "description": "Write a Python develop a Python automation program that performs scheduled data quality checks. The script should: 1. Connect to a data warehouse. 2. Validate column formats, ranges, and uniqueness. 3. Generate a data quality report in Excel. 4. Notify stakeholders with an email if thresholds are violated. 5. Save quality check logs into a JSON file with rules applied, error counts, and pass/fail status.At the end, generate a summary showing total checks run, percentage passed, and the top5 recurring issues.",
    "tags": [
      "python",
      "json",
      "automation",
      "machine learning",
      "email"
    ]
  },
  {
    "id": "16.23",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Data Security & Compliance",
    "title": "Data Lineage Tracking",
    "description": "Write a Python create a Python script for data lineage tracking. The workflow should: 1. Tag datasets at each pipeline stage with metadata source, transformation, and destination. 2. Store lineage information into a JSON-based catalog. 3. Visualize lineage with a graph library, NetworkX. 4. At the end, generate a summary showing the number of datasets tracked, the most common transformations, and lineage depth. Advanced Applications",
    "tags": [
      "python",
      "json",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "16.24",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Advanced Applications",
    "title": "Real-Time Financial Transaction Monitoring",
    "description": "Write a Python create a Python pipeline for real-time financial transaction monitoring. The script should: 1. Stream incoming transactions from Kafka. 2. Apply fraud detection rules amount thresholds, location anomalies. 3. Flag suspicious transactions in an alerts table. 4. Save logs into a JSON file with transaction ID, flagged reason, and timestamp. 5. At the end, generate a report showing the number of transactions processed, alerts triggered, and false positives detected.",
    "tags": [
      "python",
      "json",
      "file processing",
      "logging",
      "monitoring"
    ]
  },
  {
    "id": "16.25",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Advanced Applications",
    "title": "Automated Data Enrichment",
    "description": "Write a Python workflow that performs automated data enrichment. The script should: 1. Read customer records from a CSV file. 2. Enrich with external APIs, geolocation, social media, and company info. 3. Merge enriched data back into a master dataset. 4. Save enriched results into a JSON file and database table. 5. At the end, generate a summary showing the number of records enriched, enrichment sources used, and average API latency.",
    "tags": [
      "python",
      "api",
      "csv",
      "json",
      "sql"
    ]
  },
  {
    "id": "16.26",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Advanced Applications",
    "title": "Data Format Conversion at Scale",
    "description": "Write a Python script that automates data format conversion at scale. The script should: 1. Read raw data files in CSV, JSON, and XML formats. 2. Convert them into Parquet for optimized big data storage. 3. Store converted files into a designated folder or S3 bucket. 4. Save metadata into a JSON log with file name, input format, output format, and row count. 5. At the end, generate a summary showing the number of files converted by format, total rows processed, and storage reduction achieved.",
    "tags": [
      "python",
      "csv",
      "json",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.27",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Advanced Applications",
    "title": "Real-Time Anomaly Detection",
    "description": "Write a Python develop a Python automation tool for real-time anomaly detection. The script should: 1. Stream data from Kafka. 2. Apply statistical rules or ML models for anomaly detection. 3. Store anomalies into a database table. 4. Generate an alert log in JSON with anomaly type, severity, and timestamp. 5. At the end, generate a report showing the number of anomalies detected, the severity distribution, and the detection accuracy.",
    "tags": [
      "python",
      "json",
      "sql",
      "automation",
      "machine learning"
    ]
  },
  {
    "id": "16.28",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Advanced Applications",
    "title": "Large-Scale Geospatial Data Processing",
    "description": "Write a Python workflow that performs large-scale geospatial data processing. The script should: 1. Read shapefiles and geo-coordinates from a data lake. 2. Transform them with GeoPandas spatial joins, distance calculations. 3. Save outputs into GeoJSON and PostGIS. 4. Maintain logs in JSON with file size, operations applied, and runtime. 5. At the end, generate a summary showing datasets processed, operations executed, and the largest spatial region analyzed.",
    "tags": [
      "python",
      "pandas",
      "json",
      "machine learning",
      "file processing"
    ]
  },
  {
    "id": "16.29",
    "category": "Data Pipelines & Big Data",
    "subcategory": "Advanced Applications",
    "title": "Time-Series Data Pipeline Automation",
    "description": "Write a Python build a Python program for time-series pipeline automation. The script should: 1. Ingest IoT sensor data. 2. Resample into fixed intervals hourly/daily. 3. Fill gaps with interpolation. 4. Store cleaned results into a database. 5. Save logs into a JSON file with series length, gaps filled, and transformations applied. 6. At the end, generate a summary showing total sensors processed, average completeness, and data volume per interval.",
    "tags": [
      "python",
      "csv",
      "json",
      "sql",
      "automation"
    ]
  }
]
